#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
URL ì¶”ì¶œ ë° í™ˆí˜ì´ì§€ ì°¾ê¸° ë„êµ¬ (Selenium ê¸°ë°˜ + VPN ìš°íšŒ)
"""

import json
import os
import sys
import time
import random
import re
import logging
import requests
import base64
import tempfile
import subprocess
import threading
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import quote_plus, urljoin, urlparse, unquote
from datetime import datetime

# Selenium imports
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# ê°•í™”ëœ ìƒìˆ˜ ì„¤ì •
EXCLUDE_DOMAINS = [
    'youtube.com', 'facebook.com', 'instagram.com', 'twitter.com',
    'naver.com', 'daum.net', 'google.com', 'yahoo.com',
    'blog.naver.com', 'cafe.naver.com', 'tistory.com',
    'wikipedia.org', 'namu.wiki', 'blogspot.com', 'wordpress.com'
]

# VPN ê´€ë ¨ ìƒìˆ˜
VPN_COUNTRIES = ['japan', 'korea', 'singapore', 'thailand', 'malaysia']
VPN_PROCESS = None
VPN_CONFIG_PATH = None

class VPNManager:
    """VPN ì—°ê²° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.current_process = None
        self.current_config = None
        self.logger = logging.getLogger('VPNManager')
        self.available_servers = []
        self.current_server_index = 0
    
    def get_vpn_servers(self, country: str) -> List[Dict]:
        """VPN Gateì—ì„œ ì„œë²„ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            self.logger.info(f"VPN ì„œë²„ ê²€ìƒ‰ ì¤‘: {country}")
            vpn_data = requests.get("http://www.vpngate.net/api/iphone/", timeout=10).text.replace("\r", "")
            servers = [line.split(",") for line in vpn_data.split("\n")]
            labels = servers[1]
            labels[0] = labels[0][1:]
            servers = [s for s in servers[2:] if len(s) > 1]
            
            # êµ­ê°€ ë§¤ì¹­
            if len(country) == 2:
                i = 6  # short name for country
            elif len(country) > 2:
                i = 5  # long name for country
            else:
                return []
            
            desired = [s for s in servers if country.lower() in s[i].lower()]
            supported = [s for s in desired if len(s[-1]) > 0]
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_servers = sorted(supported, key=lambda s: float(s[2].replace(",", ".")), reverse=True)
            
            server_list = []
            for server in sorted_servers[:3]:  # ìƒìœ„ 3ê°œë§Œ
                server_info = {
                    'config': server[-1],
                    'score': float(server[2].replace(",", ".")),
                    'country': server[5] if len(server) > 5 else country,
                    'speed': float(server[4]) / 10**6 if len(server) > 4 else 0
                }
                server_list.append(server_info)
            
            self.logger.info(f"{country}ì—ì„œ {len(server_list)}ê°œ VPN ì„œë²„ ë°œê²¬")
            return server_list
            
        except Exception as e:
            self.logger.error(f"VPN ì„œë²„ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    def connect_vpn(self, country: str) -> bool:
        """VPN ì—°ê²°"""
        try:
            servers = self.get_vpn_servers(country)
            if not servers:
                return False
            
            # ê¸°ì¡´ ì—°ê²° ì¢…ë£Œ
            self.disconnect_vpn()
            
            # ê°€ì¥ ì¢‹ì€ ì„œë²„ ì„ íƒ
            server = servers[0]
            self.logger.info(f"VPN ì—°ê²° ì‹œë„: {server['country']} (ì ìˆ˜: {server['score']})")
            
            # ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±
            _, config_path = tempfile.mkstemp(suffix='.ovpn')
            self.current_config = config_path
            
            with open(config_path, 'w') as f:
                config_content = base64.b64decode(server['config']).decode()
                f.write(config_content)
                # Windowsìš© ì¶”ê°€ ì„¤ì •
                f.write("\nscript-security 2\n")
            
            # OpenVPN ì—°ê²° (ë°±ê·¸ë¼ìš´ë“œ)
            try:
                # Windowsì—ì„œëŠ” ê´€ë¦¬ì ê¶Œí•œ í•„ìš”í•  ìˆ˜ ìˆìŒ
                self.current_process = subprocess.Popen(
                    ["openvpn", "--config", config_path],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                )
                
                # ì—°ê²° ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
                for i in range(30):
                    if self.current_process.poll() is None:
                        time.sleep(1)
                        if i > 10:  # 10ì´ˆ í›„ë¶€í„°ëŠ” ì—°ê²°ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                            self.logger.info(f"VPN ì—°ê²° ì„±ê³µ: {country}")
                            return True
                    else:
                        break
                
                self.logger.warning(f"VPN ì—°ê²° ì‹¤íŒ¨ ë˜ëŠ” ì‹œê°„ ì´ˆê³¼: {country}")
                return False
                
            except FileNotFoundError:
                self.logger.error("OpenVPNì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. VPN ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            except Exception as e:
                self.logger.error(f"VPN ì—°ê²° ì¤‘ ì˜¤ë¥˜: {e}")
                return False
                
        except Exception as e:
            self.logger.error(f"VPN ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def disconnect_vpn(self):
        """VPN ì—°ê²° ì¢…ë£Œ"""
        try:
            if self.current_process:
                self.current_process.terminate()
                # ê°•ì œ ì¢…ë£Œ ëŒ€ê¸°
                for _ in range(5):
                    if self.current_process.poll() is not None:
                        break
                    time.sleep(1)
                else:
                    self.current_process.kill()
                
                self.current_process = None
                self.logger.info("VPN ì—°ê²° ì¢…ë£Œ")
            
            if self.current_config and os.path.exists(self.current_config):
                os.unlink(self.current_config)
                self.current_config = None
                
        except Exception as e:
            self.logger.error(f"VPN ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def __del__(self):
        """ì†Œë©¸ìì—ì„œ VPN ì—°ê²° ì •ë¦¬"""
        self.disconnect_vpn()

# ê¸°ì¡´ ê²½ë¡œ ì„¤ì • ë’¤ì— ì¶”ê°€
# ìƒìœ„ ë””ë ‰í† ë¦¬ ëª¨ë“ˆ import (ê²½ë¡œ ì„¤ì • í›„)
try:
    from validator import AIValidator
    print("âœ… AIValidator import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ AIValidator import ì‹¤íŒ¨: {e}")
    print("ğŸ”§ AIValidator ì—†ì´ ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    AIValidator = None

class URLExtractor:
    """URL ì¶”ì¶œ ë° í™ˆí˜ì´ì§€ ê²€ìƒ‰ í´ë˜ìŠ¤ (Selenium ê¸°ë°˜ + VPN ìš°íšŒ)"""
    
    def __init__(self, headless: bool = False):
        """ì´ˆê¸°í™”"""
        self.headless = headless
        self.driver = None
        self.logger = self.setup_logger()
        self.vpn_manager = VPNManager()
        
        # AI ê²€ì¦ê¸° ì´ˆê¸°í™” (ìƒˆë¡œ ì¶”ê°€)
        self.ai_validator = None
        if AIValidator:
            try:
                self.ai_validator = AIValidator()
                self.use_ai_validation = True
                print("ğŸ¤– AI ê²€ì¦ ê¸°ëŠ¥ í™œì„±í™”")
            except Exception as e:
                print(f"âŒ AI ê²€ì¦ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_ai_validation = False
        else:
            self.use_ai_validation = False
            print("ğŸ”§ AI ê²€ì¦ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
        
        # ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
        self.delay_range = (3, 6)
        
        # ê²€ìƒ‰ ê²°ê³¼ ì œí•œ
        self.max_search_results = 5
        self.max_retries = 3
        
        # VPN ìš°íšŒ ê´€ë ¨
        self.google_blocked = False
        self.vpn_countries = ['japan', 'korea', 'singapore', 'thailand', 'malaysia']
        self.current_vpn_index = 0
    
    def setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('URLExtractor')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def setup_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        try:
            chrome_options = Options()
            
            # headless ëª¨ë“œ ì„¤ì •
            if self.headless:
                chrome_options.add_argument('--headless')
            
            # ê¸°ë³¸ ì˜µì…˜ë“¤
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # ìë™í™” íƒì§€ ìš°íšŒ
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # VPN ì‚¬ìš© ì‹œ í”„ë¡ì‹œ ì„¤ì • (í•„ìš”ì‹œ)
            # chrome_options.add_argument('--proxy-server=socks5://127.0.0.1:1080')
            
            # ë“œë¼ì´ë²„ ìƒì„±
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.logger.info(f"Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ (headless: {self.headless})")
            
        except Exception as e:
            self.logger.error(f"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    def close_driver(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            self.logger.info("ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")
        
        # VPN ì—°ê²°ë„ ì¢…ë£Œ
        self.vpn_manager.disconnect_vpn()
    
    def is_google_blocked(self) -> bool:
        """êµ¬ê¸€ì—ì„œ ì°¨ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        try:
            page_source = self.driver.page_source.lower()
            title = self.driver.title.lower()
            
            # ì°¨ë‹¨ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
            block_keywords = [
                'unusual traffic', 'captcha', 'blocked', 'forbidden',
                'access denied', 'suspicious activity', 'automated queries',
                'robot', 'bot detected', 'ë¹„ì •ìƒì ì¸ íŠ¸ë˜í”½'
            ]
            
            for keyword in block_keywords:
                if keyword in page_source or keyword in title:
                    self.logger.warning(f"êµ¬ê¸€ ì°¨ë‹¨ ê°ì§€: {keyword}")
                    return True
            
            # CAPTCHA ìš”ì†Œ í™•ì¸
            captcha_selectors = [
                '#captcha', '.captcha', '[data-recaptcha]',
                '.g-recaptcha', '#recaptcha'
            ]
            
            for selector in captcha_selectors:
                try:
                    if self.driver.find_elements(By.CSS_SELECTOR, selector):
                        self.logger.warning("CAPTCHA ê°ì§€ë¨")
                        return True
                except:
                    continue
            
            return False
            
        except Exception as e:
            self.logger.error(f"ì°¨ë‹¨ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def try_vpn_bypass(self) -> bool:
        """VPNì„ í†µí•œ ìš°íšŒ ì‹œë„"""
        try:
            self.logger.info("ğŸ”§ VPN ìš°íšŒ ì‹œë„ ì¤‘...")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ VPN êµ­ê°€ ìˆœì°¨ ì‹œë„
            for country in self.vpn_countries[self.current_vpn_index:]:
                self.logger.info(f"ğŸŒ VPN ì—°ê²° ì‹œë„: {country}")
                
                if self.vpn_manager.connect_vpn(country):
                    # VPN ì—°ê²° í›„ ì ì‹œ ëŒ€ê¸°
                    time.sleep(5)
                    
                    # ë“œë¼ì´ë²„ ì¬ì‹œì‘ (ìƒˆë¡œìš´ IPë¡œ)
                    self.close_driver()
                    time.sleep(2)
                    self.setup_driver()
                    
                    # êµ¬ê¸€ ì ‘ì† í…ŒìŠ¤íŠ¸
                    try:
                        self.driver.get("https://www.google.com")
                        time.sleep(3)
                        
                        if not self.is_google_blocked():
                            self.logger.info(f"âœ… VPN ìš°íšŒ ì„±ê³µ: {country}")
                            self.google_blocked = False
                            return True
                        else:
                            self.logger.warning(f"âŒ {country} VPNìœ¼ë¡œë„ ì°¨ë‹¨ë¨")
                            
                    except Exception as e:
                        self.logger.warning(f"VPN ì—°ê²° í›„ êµ¬ê¸€ ì ‘ì† ì‹¤íŒ¨: {e}")
                
                # ë‹¤ìŒ êµ­ê°€ë¡œ
                self.current_vpn_index += 1
                if self.current_vpn_index >= len(self.vpn_countries):
                    self.current_vpn_index = 0
                    break
            
            self.logger.error("ëª¨ë“  VPN ì‹œë„ ì‹¤íŒ¨")
            return False
            
        except Exception as e:
            self.logger.error(f"VPN ìš°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def add_delay(self):
        """ìš”ì²­ ê°„ ì§€ì—°"""
        delay = random.uniform(*self.delay_range)
        self.logger.info(f"ì§€ì—° ì‹œê°„: {delay:.1f}ì´ˆ")
        time.sleep(delay)
    
    def clean_search_query(self, organization_name: str, location: str = "") -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ì •ì œ (ì§€ì—­ ì •ë³´ ì œê±°)"""
        # ê¸°ë³¸ ì •ì œ
        cleaned = re.sub(r'[^\w\sê°€-í£]', ' ', organization_name)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # ì§€ì—­ ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ê²€ìƒ‰ ê²°ê³¼ ì œí•œ ë°©ì§€)
        
        # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ì „ëµ
        if any(keyword in cleaned for keyword in ['êµíšŒ', 'ì„±ë‹¹', 'ì ˆ']):
            return f'{cleaned} í™ˆí˜ì´ì§€'
        elif any(keyword in cleaned for keyword in ['ì„¼í„°', 'ê´€', 'ì›']):
            return f'{cleaned} ê³µì‹í™ˆí˜ì´ì§€'
        else:
            return f'{cleaned} í™ˆí˜ì´ì§€'
    
    def search_organization_homepage(self, organization_name: str, location: str = "") -> Optional[str]:
        """ê¸°ê´€ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (AI ê²€ì¦ í¬í•¨)"""
        try:
            if not self.driver:
                self.setup_driver()
            
            candidate_urls = []
            
            # ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤í–‰
            self.logger.info(f"ğŸ” ë„¤ì´ë²„ ê²€ìƒ‰ ì‹œì‘: {organization_name}")
            naver_urls = self._search_with_naver(organization_name)
            if naver_urls:
                self.logger.info(f"ë„¤ì´ë²„ì—ì„œ {len(naver_urls)}ê°œ URL ë°œê²¬")
                candidate_urls.extend([(url, 'naver') for url in naver_urls])
            
            # ê²€ìƒ‰ ê°„ ì§€ì—°
            self.add_delay()
            
            # êµ¬ê¸€ ê²€ìƒ‰ ì‹¤í–‰
            self.logger.info(f"ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ì‹œì‘: {organization_name}")
            google_urls = self._perform_google_search(organization_name, organization_name)
            if google_urls:
                self.logger.info(f"êµ¬ê¸€ì—ì„œ {len(google_urls)}ê°œ URL ë°œê²¬")
                candidate_urls.extend([(url, 'google') for url in google_urls])
            
            # AI ê²€ì¦ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
            if self.use_ai_validation and candidate_urls:
                return self._select_url_with_ai_validation(candidate_urls, organization_name)
            
            # AI ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¤‘ë³µ ì œê±° ë° ìµœì  URL ì„ íƒ
            if candidate_urls:
                unique_urls = list(set([url for url, source in candidate_urls]))
                best_url = self.select_best_homepage(unique_urls, organization_name)
                
                # ì†ŒìŠ¤ ì •ë³´ ì¶œë ¥
                sources = [source for url, source in candidate_urls if url == best_url]
                source_info = f" (ì¶œì²˜: {', '.join(set(sources))})" if sources else ""
                self.logger.info(f"âœ… ìµœì¢… ì„ íƒ URL: {best_url}{source_info}")
                
                return best_url
            
            self.logger.warning(f"âŒ {organization_name}ì˜ í™ˆí˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            return None
            
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
        
    def _select_url_with_ai_validation(self, candidate_urls: List[Tuple[str, str]], 
                                     organization_name: str) -> Optional[str]:
        """AI ê²€ì¦ìœ¼ë¡œ ìµœì  URL ì„ íƒ"""
        import asyncio
        
        async def validate_urls():
            validated_results = []
            
            for url, source in candidate_urls[:5]:  # ìµœëŒ€ 5ê°œë§Œ ê²€ì¦
                try:
                    # í˜ì´ì§€ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                    page_content = self._get_page_preview(url)
                    
                    # AI ê²€ì¦
                    validation = await self.ai_validator.validate_homepage_url_relevance(
                        organization_name, url, page_content, source
                    )
                    
                    if validation.get("is_relevant", False) and validation.get("confidence", 0) > 0.6:
                        validated_results.append({
                            'url': url,
                            'confidence': validation.get("confidence", 0),
                            'source': source,
                            'reasoning': validation.get("reasoning", "")
                        })
                        self.logger.info(f"âœ… AI ê²€ì¦ í†µê³¼: {url} (ì‹ ë¢°ë„: {validation.get('confidence', 0):.2f})")
                    else:
                        self.logger.warning(f"âŒ AI ê²€ì¦ ì‹¤íŒ¨: {url} (ì‹ ë¢°ë„: {validation.get('confidence', 0):.2f})")
                
                except Exception as e:
                    self.logger.warning(f"URL ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {url} - {e}")
                    continue
            
            return validated_results
        
        try:
            # ë¹„ë™ê¸° ê²€ì¦ ì‹¤í–‰
            validated_results = asyncio.run(validate_urls())
            
            if validated_results:
                # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                validated_results.sort(key=lambda x: x['confidence'], reverse=True)
                best_result = validated_results[0]
                
                self.logger.info(f"ğŸ¯ AI ê²€ì¦ ìµœì¢… ì„ íƒ: {best_result['url']} "
                               f"(ì‹ ë¢°ë„: {best_result['confidence']:.2f}, ì¶œì²˜: {best_result['source']})")
                
                return best_result['url']
            
            # AI ê²€ì¦ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ì‹ fallback
            self.logger.warning("AI ê²€ì¦ ê²°ê³¼ ì—†ìŒ, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback")
            unique_urls = list(set([url for url, source in candidate_urls]))
            return self.select_best_homepage(unique_urls, organization_name)
            
        except Exception as e:
            self.logger.error(f"AI ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            unique_urls = list(set([url for url, source in candidate_urls]))
            return self.select_best_homepage(unique_urls, organization_name)

    def _get_page_preview(self, url: str, max_chars: int = 2000) -> str:
        """í˜ì´ì§€ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (AI ê²€ì¦ìš©)"""
        try:
            original_window = self.driver.current_window_handle
            
            # ìƒˆ íƒ­ì—ì„œ í˜ì´ì§€ ë¡œë“œ
            self.driver.execute_script("window.open('');")
            self.driver.switch_to.window(self.driver.window_handles[-1])
            
            self.driver.get(url)
            time.sleep(2)
            
            # í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
            try:
                body_text = self.driver.find_element(By.TAG_NAME, "body").text
                preview = body_text[:max_chars] if len(body_text) > max_chars else body_text
            except:
                preview = ""
            
            # ì›ë˜ ì°½ìœ¼ë¡œ ëŒì•„ê°€ê¸°
            self.driver.close()
            self.driver.switch_to.window(original_window)
            
            return preview
            
        except Exception as e:
            self.logger.warning(f"í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {url} - {e}")
            try:
                # ì˜¤ë¥˜ ì‹œ ì›ë˜ ì°½ìœ¼ë¡œ ëŒì•„ê°€ê¸°
                if len(self.driver.window_handles) > 1:
                    self.driver.close()
                self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass
            return ""

    def _search_with_naver(self, organization_name: str) -> List[str]:
        """ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤í–‰"""
        found_urls = []
        
        search_queries = [
            f'{organization_name} í™ˆí˜ì´ì§€',
            f'{organization_name} ê³µì‹ì‚¬ì´íŠ¸',
            f'{organization_name} site:or.kr',
            f'{organization_name} site:org',
            f'{organization_name}',
        ]
        
        try:
            for i, query in enumerate(search_queries, 1):
                self.logger.info(f"ë„¤ì´ë²„ ê²€ìƒ‰ {i}/{len(search_queries)}: {query}")
                urls = self._perform_naver_search(query, organization_name)
                if urls:
                    found_urls.extend(urls)
                    if len(found_urls) >= self.max_search_results:
                        break
                
                # ì¿¼ë¦¬ ê°„ ì§§ì€ ì§€ì—°
                time.sleep(1)
                
        except Exception as e:
            self.logger.warning(f"ë„¤ì´ë²„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return list(set(found_urls))  # ì¤‘ë³µ ì œê±°

    def _search_with_google(self, query: str, organization_name: str) -> List[str]:
        """êµ¬ê¸€ ê²€ìƒ‰ ìˆ˜í–‰ (VPN ìš°íšŒ í¬í•¨)"""
        found_urls = []
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # êµ¬ê¸€ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
                google_url = f"https://www.google.com/search?q={quote_plus(query)}&hl=ko"
                self.driver.get(google_url)
                time.sleep(3)
                
                # ì°¨ë‹¨ ì—¬ë¶€ í™•ì¸
                if self.is_google_blocked():
                    self.logger.warning(f"êµ¬ê¸€ ì°¨ë‹¨ ê°ì§€ (ì‹œë„ {attempt + 1}/{max_retries})")
                    
                    if attempt < max_retries - 1:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ VPN ìš°íšŒ
                        if self.try_vpn_bypass():
                            self.logger.info("VPN ìš°íšŒ ì„±ê³µ, êµ¬ê¸€ ê²€ìƒ‰ ì¬ì‹œë„")
                            continue
                        else:
                            self.logger.error("VPN ìš°íšŒ ì‹¤íŒ¨")
                            break
                    else:
                        self.logger.error("ëª¨ë“  VPN ìš°íšŒ ì‹œë„ ì‹¤íŒ¨")
                        break
                
                # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ë§í¬ ì„ íƒìë“¤
                link_selectors = [
                    'h3 a[href*="http"]',
                    '.g a[href*="http"]',
                    '[data-ved] a[href*="http"]',
                    '.yuRUbf a[href*="http"]',
                    '.kCrYT a[href*="http"]',
                    '.tF2Cxc a[href*="http"]',
                    '.LC20lb a[href*="http"]',
                ]
                
                for selector in link_selectors:
                    try:
                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        self.logger.info(f"êµ¬ê¸€ ì„ íƒì '{selector}'ë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                        
                        for element in elements[:15]:
                            try:
                                href = element.get_attribute('href')
                                if not href:
                                    continue
                                
                                # êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬
                                if 'google.com' in href and '/url?q=' in href:
                                    actual_url = self._extract_google_redirect_url(href)
                                    if actual_url:
                                        href = actual_url
                                    else:
                                        continue
                                
                                # êµ¬ê¸€ ë‚´ë¶€ ë§í¬ ì œì™¸
                                if 'google.com' in href:
                                    continue
                                
                                if self.is_valid_homepage_url(href, organization_name):
                                    found_urls.append(href)
                                    self.logger.info(f"êµ¬ê¸€ì—ì„œ ìœ íš¨í•œ URL ë°œê²¬: {href}")
                                    
                            except Exception:
                                continue
                                
                        if found_urls:
                            break
                            
                    except Exception as e:
                        self.logger.warning(f"êµ¬ê¸€ ì„ íƒì '{selector}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                # ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì„±ê³µ
                if found_urls:
                    break
                else:
                    self.logger.warning(f"êµ¬ê¸€ ê²€ìƒ‰ì—ì„œ ê²°ê³¼ ì—†ìŒ (ì‹œë„ {attempt + 1})")
                    
            except Exception as e:
                self.logger.warning(f"êµ¬ê¸€ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                continue
        
        return found_urls[:self.max_search_results]

    def _perform_naver_search(self, query: str, organization_name: str) -> List[str]:
        """ì‹¤ì œ ë„¤ì´ë²„ ê²€ìƒ‰ ìˆ˜í–‰ (ê¸°ì¡´ ë¡œì§ ê°œì„ )"""
        found_urls = []
        
        try:
            # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
            self.driver.get('https://www.naver.com')
            time.sleep(2)
            
            # ê²€ìƒ‰ì°½ ì°¾ê¸° ë° ê²€ìƒ‰ì–´ ì…ë ¥
            search_box = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "query"))
            )
            
            search_box.clear()
            search_box.send_keys(query)
            search_box.send_keys(Keys.RETURN)
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œ ëŒ€ê¸°
            time.sleep(3)
            
            # ë‹¤ì–‘í•œ ì„ íƒìë¡œ ë§í¬ ì°¾ê¸°
            link_selectors = [
                'h3.title a[href*="http"]',
                '.total_wrap a[href*="http"]',
                '.result_area a[href*="http"]',
                '.data_area a[href*="http"]',
                '.web_page a[href*="http"]',
                '.site_area a[href*="http"]',
                'a[href*="http"]:not([href*="naver.com"])',
            ]
            
            for selector in link_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    
                    for element in elements[:10]:
                        try:
                            href = element.get_attribute('href')
                            if not href:
                                continue
                            
                            # ë„¤ì´ë²„ ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬
                            if 'naver.com' in href:
                                actual_url = self._extract_redirect_url(href)
                                if actual_url:
                                    href = actual_url
                                else:
                                    continue
                            
                            if self.is_valid_homepage_url(href, organization_name):
                                found_urls.append(href)
                                
                        except Exception:
                            continue
                            
                    if found_urls:
                        break
                        
                except Exception:
                    continue
            
            return found_urls[:self.max_search_results]
            
        except Exception as e:
            self.logger.warning(f"ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def _extract_google_redirect_url(self, google_url: str) -> Optional[str]:
        """êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ URLì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ"""
        try:
            if '/url?q=' in google_url:
                # /url?q= ë’¤ì˜ URL ì¶”ì¶œ
                parts = google_url.split('/url?q=')
                if len(parts) > 1:
                    url_part = parts[1].split('&')[0]  # ì²« ë²ˆì§¸ & ì•ê¹Œì§€
                    actual_url = unquote(url_part)
                    return actual_url
            return None
        except Exception:
            return None

    def select_best_homepage(self, urls: List[str], organization_name: str) -> str:
        """ê°€ì¥ ì í•©í•œ í™ˆí˜ì´ì§€ URL ì„ íƒ (ê°œì„ ëœ ë²„ì „)"""
        if not urls:
            return ""
        
        if len(urls) == 1:
            return urls[0]
        
        scored_urls = []
        
        for url in urls:
            score = 0
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # 1. ë„ë©”ì¸ ì‹ ë¢°ë„ ì ìˆ˜
            if '.or.kr' in domain:
                score += 20  # ë¹„ì˜ë¦¬ê¸°ê´€
            elif '.go.kr' in domain:
                score += 18  # ì •ë¶€ê¸°ê´€
            elif '.ac.kr' in domain:
                score += 16  # êµìœ¡ê¸°ê´€
            elif '.church' in domain:
                score += 15  # êµíšŒ ë„ë©”ì¸
            elif '.org' in domain:
                score += 12  # ì¼ë°˜ ê¸°ê´€
            elif '.co.kr' in domain:
                score += 8   # ê¸°ì—…
            elif '.com' in domain:
                score += 5   # ì¼ë°˜ ìƒì—…
            
            # 2. ê¸°ê´€ëª… ë§¤ì¹­ ì ìˆ˜
            name_parts = re.findall(r'[ê°€-í£a-zA-Z]+', organization_name.lower())
            for part in name_parts:
                if len(part) > 2:
                    if part in domain:
                        score += 10
                    elif any(part in segment for segment in domain.split('.')):
                        score += 5
            
            # 3. URL êµ¬ì¡° ì ìˆ˜
            path_depth = len([p for p in parsed.path.strip('/').split('/') if p])
            if path_depth == 0:  # ë£¨íŠ¸ ë„ë©”ì¸
                score += 8
            elif path_depth == 1:
                score += 5
            elif path_depth > 3:
                score -= 3
            
            # 4. HTTPS ë³´ë„ˆìŠ¤
            if parsed.scheme == 'https':
                score += 3
            
            # 5. ì§§ì€ ë„ë©”ì¸ ì„ í˜¸
            if len(domain) < 20:
                score += 2
            elif len(domain) > 40:
                score -= 2
            
            # 6. íŠ¹ìˆ˜ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤/íŒ¨ë„í‹°
            if any(keyword in domain for keyword in ['official', 'main', 'home']):
                score += 5
            if any(keyword in url.lower() for keyword in ['blog', 'cafe', 'post', 'board']):
                score -= 10
            
            scored_urls.append((url, score))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_urls.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ë“¤ ë¡œê¹…
        self.logger.info("URL ì ìˆ˜ ìˆœìœ„:")
        for i, (url, score) in enumerate(scored_urls[:3], 1):
            self.logger.info(f"  {i}. {url} (ì ìˆ˜: {score})")
        
        return scored_urls[0][0]

    def _extract_redirect_url(self, naver_url: str) -> Optional[str]:
        """ë„¤ì´ë²„ ë¦¬ë‹¤ì´ë ‰íŠ¸ URLì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ"""
        try:
            if 'url=' in naver_url:
                import urllib.parse
                parsed_url = urllib.parse.urlparse(naver_url)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                if 'url' in query_params:
                    actual_url = query_params['url'][0]
                    return unquote(actual_url)
            return None
        except:
            return None
    
    def _search_by_title_text(self, organization_name: str) -> List[str]:
        """ì œëª© í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€ ê²€ìƒ‰"""
        found_urls = []
        try:
            # í™ˆí˜ì´ì§€ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë§í¬ ì°¾ê¸°
            homepage_keywords = ['í™ˆí˜ì´ì§€', 'ê³µì‹', 'official', 'home', 'www', 'ì‚¬ì´íŠ¸']
            
            all_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*="http"]')
            
            for link in all_links[:50]:
                try:
                    text = link.text.strip().lower()
                    href = link.get_attribute('href')
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ì— í™ˆí˜ì´ì§€ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                    if any(keyword in text for keyword in homepage_keywords):
                        if href and self.is_valid_homepage_url(href, organization_name):
                            found_urls.append(href)
                            self.logger.info(f"ì œëª© í…ìŠ¤íŠ¸ë¡œ ë°œê²¬: {href}")
                            
                            if len(found_urls) >= self.max_search_results:
                                break
                                
                except Exception:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"ì œëª© í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return found_urls
    
    def is_valid_homepage_url(self, url: str, organization_name: str) -> bool:
        """ìœ íš¨í•œ í™ˆí˜ì´ì§€ URLì¸ì§€ í™•ì¸"""
        try:
            if not url or len(url) < 10:
                return False
            
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return False
            
            domain = parsed.netloc.lower()
            
            # ì œì™¸í•  ë„ë©”ì¸ ì²´í¬
            for exclude_domain in EXCLUDE_DOMAINS:
                if exclude_domain in domain:
                    return False
            
            # ì¶”ê°€ ì œì™¸ íŒ¨í„´
            exclude_patterns = [
                r'\.pdf$', r'\.jpg$', r'\.png$', r'\.gif$',
                r'/board/', r'/bbs/', r'/community/',
                r'blog\.', r'cafe\.', r'post\.',
                r'search\.', r'maps\.', r'news\.'
            ]
            
            for pattern in exclude_patterns:
                if re.search(pattern, url.lower()):
                    return False
            
            # ë„ˆë¬´ ê¸´ URL ì œì™¸
            if len(url) > 200:
                return False
            
            # ê³µì‹ ë„ë©”ì¸ ìš°ì„ ìˆœìœ„
            official_tlds = ['.or.kr', '.go.kr', '.ac.kr', '.org', '.church']
            if any(tld in domain for tld in official_tlds):
                self.logger.info(f"ê³µì‹ ë„ë©”ì¸ ë°œê²¬: {domain}")
                return True
            
            # ê¸°ê´€ëª…ì´ ë„ë©”ì¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            name_parts = re.findall(r'[ê°€-í£a-zA-Z]+', organization_name.lower())
            for part in name_parts:
                if len(part) > 2 and part in domain:
                    self.logger.info(f"ê¸°ê´€ëª…ì´ ë„ë©”ì¸ì— í¬í•¨ë¨: {part} in {domain}")
                    return True
            
            return True
            
        except Exception:
            return False
    
    def verify_homepage_url(self, url: str) -> bool:
        """í™ˆí˜ì´ì§€ URLì´ ì‹¤ì œë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸"""
        try:
            original_window = self.driver.current_window_handle
            
            # ìƒˆ íƒ­ì—ì„œ URL ê²€ì¦
            self.driver.execute_script("window.open('');")
            self.driver.switch_to.window(self.driver.window_handles[-1])
            
            self.driver.get(url)
            time.sleep(3)
            
            # í˜ì´ì§€ ë¡œë“œ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            is_accessible = "404" not in self.driver.title.lower() and \
                           "error" not in self.driver.title.lower() and \
                           len(self.driver.page_source) > 1000
            
            # ì›ë˜ ì°½ìœ¼ë¡œ ëŒì•„ê°€ê¸°
            self.driver.close()
            self.driver.switch_to.window(original_window)
            
            if is_accessible:
                self.logger.info(f"URL ì ‘ê·¼ ì„±ê³µ: {url}")
            else:
                self.logger.warning(f"URL ì ‘ê·¼ ì‹¤íŒ¨: {url}")
            
            return is_accessible
            
        except Exception as e:
            self.logger.warning(f"URL ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {url} - {e}")
            try:
                # ì˜¤ë¥˜ ì‹œ ì›ë˜ ì°½ìœ¼ë¡œ ëŒì•„ê°€ê¸°
                self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass
     
    def process_organizations(self, organizations: List[Dict]) -> List[Dict]:
        """ê¸°ê´€ ëª©ë¡ ì²˜ë¦¬"""
        processed_orgs = []
        total_count = len(organizations)
        success_count = 0
        
        try:
            for i, org in enumerate(organizations, 1):
                try:
                    self.logger.info(f"ì§„í–‰ ìƒí™©: {i}/{total_count} - {org.get('name', 'Unknown')}")
                    
                    # ê¸°ì¡´ ë°ì´í„° ì™„ì „ ë³µì‚¬
                    org_copy = org.copy()
                    
                    # ì´ë¯¸ í™ˆí˜ì´ì§€ê°€ ìˆê³  ìœ íš¨í•œ URLì´ë©´ ìŠ¤í‚µ
                    if org.get('homepage') and org['homepage'].strip() and org['homepage'].startswith(('http://', 'https://')):
                        self.logger.info(f"ì´ë¯¸ ìœ íš¨í•œ í™ˆí˜ì´ì§€ê°€ ìˆìŒ: {org['homepage']}")
                        processed_orgs.append(org_copy)
                        if org['homepage'].strip():
                            success_count += 1
                        continue
                    
                    # í™ˆí˜ì´ì§€ ê²€ìƒ‰ (ì§€ì—­ ì •ë³´ ì‚¬ìš© ì•ˆí•¨)
                    homepage_url = self.search_organization_homepage(
                        org.get('name', ''),
                        ""  # ì§€ì—­ ì •ë³´ ì „ë‹¬í•˜ì§€ ì•ŠìŒ
                    )
                    
                    # homepage í•„ë“œ ì—…ë°ì´íŠ¸
                    if homepage_url:
                        # URL ê²€ì¦
                        if self.verify_homepage_url(homepage_url):
                            org_copy['homepage'] = homepage_url
                            success_count += 1
                            self.logger.info(f"âœ… í™ˆí˜ì´ì§€ ì°¾ìŒ: {homepage_url}")
                        else:
                            org_copy['homepage'] = ""
                            self.logger.warning(f"âš ï¸ URL ì ‘ê·¼ ë¶ˆê°€: {homepage_url}")
                    else:
                        org_copy['homepage'] = ""
                        self.logger.warning(f"âŒ í™ˆí˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í•¨")
                    
                    processed_orgs.append(org_copy)
                    
                    # ì§„í–‰ ìƒí™© ì¶œë ¥
                    if i % 5 == 0:
                        self.logger.info(f"ğŸ“Š ì¤‘ê°„ í†µê³„: {success_count}/{i} ì„±ê³µ ({success_count/i*100:.1f}%)")
                    
                    # ìš”ì²­ ê°„ ì§€ì—°
                    if i < total_count:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ì§€ì—°
                        self.add_delay()
                    
                except Exception as e:
                    self.logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    org_copy = org.copy()
                    if not org.get('homepage'):
                        org_copy['homepage'] = ""
                    processed_orgs.append(org_copy)
                    continue
                    
        finally:
            # ë“œë¼ì´ë²„ ì¢…ë£Œ
            self.close_driver()
        
        return processed_orgs

    def save_results(self, organizations: List[Dict], output_file: str) -> bool:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(organizations, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ” ê¸°ê´€ í™ˆí˜ì´ì§€ URL ì¶”ì¶œê¸° (Selenium ê¸°ë°˜)")
    print("=" * 60)
    
    try:
        # headless ëª¨ë“œ ì„¤ì • (Falseë¡œ ì„¤ì •í•˜ì—¬ ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ)
        use_headless = False
        print(f"ğŸŒ ë¸Œë¼ìš°ì € ëª¨ë“œ: {'Headless' if use_headless else 'GUI'}")
        
        # ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        extractor = URLExtractor(headless=use_headless)
        
        # ì…ë ¥/ì¶œë ¥ íŒŒì¼ ì„¤ì •
        input_file = r"C:\Users\kimyh\makedb\Python\cradcrawl_adv\undefined_converted_20250609_134731.json"
        output_file = f"raw_data_with_homepages_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {input_file}")
        print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼: {output_file}")
        
        # ì…ë ¥ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(input_file):
            print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
            return 1
        
        # JSON íŒŒì¼ ë¡œë“œ
        print("ğŸ“– ë°ì´í„° ë¡œë”© ì¤‘...")
        with open(input_file, 'r', encoding='utf-8') as f:
            organizations = json.load(f)
        
        print(f"ğŸ“Š ë¡œë“œëœ ê¸°ê´€ ìˆ˜: {len(organizations)}")
        
        # ë¹ˆ homepage í•„ë“œ ê°œìˆ˜ í™•ì¸
        empty_homepage_count = sum(1 for org in organizations if not org.get('homepage') or not org['homepage'].strip())
        print(f"ğŸ” í™ˆí˜ì´ì§€ê°€ ì—†ëŠ” ê¸°ê´€: {empty_homepage_count}ê°œ")
        
        # ì²˜ë¦¬í•  ê¸°ê´€ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
        max_process = int(input(f"ì²˜ë¦¬í•  ê¸°ê´€ ìˆ˜ (ì „ì²´: {len(organizations)}, ì—”í„° ì‹œ ì „ì²´): ") or len(organizations))
        organizations = organizations[:max_process]
        
        print(f"ğŸ”„ {len(organizations)}ê°œ ê¸°ê´€ ì²˜ë¦¬ ì‹œì‘...")
        print("ğŸ“ Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë¸Œë¼ìš°ì €ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        
        # ê¸°ê´€ ì²˜ë¦¬
        processed_organizations = extractor.process_organizations(organizations)
        
        # ê²°ê³¼ ì €ì¥
        if extractor.save_results(processed_organizations, output_file):
            print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {output_file}")
            
            # í†µê³„ ì¶œë ¥
            homepage_found = sum(1 for org in processed_organizations if org.get('homepage') and org['homepage'].strip())
            original_homepage_count = sum(1 for org in organizations if org.get('homepage') and org['homepage'].strip())
            new_homepage_count = homepage_found - original_homepage_count
            
            print(f"ğŸ“ˆ ìµœì¢… í†µê³„:")
            print(f"  - ê¸°ì¡´ í™ˆí˜ì´ì§€: {original_homepage_count}ê°œ")
            print(f"  - ìƒˆë¡œ ì°¾ì€ í™ˆí˜ì´ì§€: {new_homepage_count}ê°œ")
            print(f"  - ì „ì²´ í™ˆí˜ì´ì§€: {homepage_found}/{len(processed_organizations)}ê°œ")
            print(f"  - ì„±ê³µë¥ : {homepage_found/len(processed_organizations)*100:.1f}%")
            
        else:
            print(f"\nâŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")
            return 1
        
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())