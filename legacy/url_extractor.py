#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
URL ì¶”ì¶œ ë° í™ˆí˜ì´ì§€ ì°¾ê¸° ë„êµ¬ (Selenium ê¸°ë°˜)
"""

import json
import os
import sys
import time
import random
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import quote_plus, urljoin, urlparse, unquote
from datetime import datetime

# Selenium imports
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# ê°•í™”ëœ ìƒìˆ˜ ì„¤ì •
EXCLUDE_DOMAINS = [
    'youtube.com', 'facebook.com', 'instagram.com', 'twitter.com',
    'naver.com', 'daum.net', 'google.com', 'yahoo.com',
    'blog.naver.com', 'cafe.naver.com', 'tistory.com',
    'wikipedia.org', 'namu.wiki', 'blogspot.com', 'wordpress.com'
]

class URLExtractor:
    """URL ì¶”ì¶œ ë° í™ˆí˜ì´ì§€ ê²€ìƒ‰ í´ë˜ìŠ¤ (Selenium ê¸°ë°˜)"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.driver = None
        self.logger = self.setup_logger()
        
        # ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
        self.delay_range = (3, 6)
        
        # ê²€ìƒ‰ ê²°ê³¼ ì œí•œ
        self.max_search_results = 5
        self.max_retries = 3
    
    def setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('URLExtractor')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def setup_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        try:
            chrome_options = Options()
            
            # headless ëª¨ë“œ ì„¤ì •
            if self.headless:
                chrome_options.add_argument('--headless')
            
            # ê¸°ë³¸ ì˜µì…˜ë“¤
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # ìë™í™” íƒì§€ ìš°íšŒ
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ë“œë¼ì´ë²„ ìƒì„±
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.logger.info(f"Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ (headless: {self.headless})")
            
        except Exception as e:
            self.logger.error(f"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    def close_driver(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            self.logger.info("ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")
    
    def add_delay(self):
        """ìš”ì²­ ê°„ ì§€ì—°"""
        delay = random.uniform(*self.delay_range)
        self.logger.info(f"ì§€ì—° ì‹œê°„: {delay:.1f}ì´ˆ")
        time.sleep(delay)
    
    def clean_search_query(self, organization_name: str, location: str = "") -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ì •ì œ (ì§€ì—­ ì •ë³´ ì œê±°)"""
        # ê¸°ë³¸ ì •ì œ
        cleaned = re.sub(r'[^\w\sê°€-í£]', ' ', organization_name)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # ì§€ì—­ ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ê²€ìƒ‰ ê²°ê³¼ ì œí•œ ë°©ì§€)
        
        # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ì „ëµ
        if any(keyword in cleaned for keyword in ['êµíšŒ', 'ì„±ë‹¹', 'ì ˆ']):
            return f'{cleaned} í™ˆí˜ì´ì§€'
        elif any(keyword in cleaned for keyword in ['ì„¼í„°', 'ê´€', 'ì›']):
            return f'{cleaned} ê³µì‹í™ˆí˜ì´ì§€'
        else:
            return f'{cleaned} í™ˆí˜ì´ì§€'
    
    def search_organization_homepage(self, organization_name: str, location: str = "") -> Optional[str]:
        """ê¸°ê´€ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (ë„¤ì´ë²„ ê¸°ë°˜) - ì§€ì—­ ì •ë³´ ì‚¬ìš© ì•ˆí•¨"""
        try:
            if not self.driver:
                self.setup_driver()
            
            # ì§€ì—­ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ì¿¼ë¦¬ë“¤
            search_queries = [
                self.clean_search_query(organization_name),  # location ì œê±°
                f'{organization_name} í™ˆí˜ì´ì§€',
                f'{organization_name} ê³µì‹ì‚¬ì´íŠ¸',
                f'{organization_name} site:or.kr',
                f'{organization_name} site:org',
            ]
            
            for i, query in enumerate(search_queries, 1):
                self.logger.info(f"ë„¤ì´ë²„ ê²€ìƒ‰ ì‹œë„ {i}/{len(search_queries)}: {query}")
                result = self._perform_naver_search(query, organization_name)
                if result:
                    return result
                
                # ê° ì‹œë„ ê°„ ì§€ì—°
                self.add_delay()
            
            return None
            
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def _perform_naver_search(self, query: str, organization_name: str) -> Optional[str]:
        """ì‹¤ì œ ë„¤ì´ë²„ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
            self.driver.get('https://www.naver.com')
            time.sleep(2)
            
            # ê²€ìƒ‰ì°½ ì°¾ê¸° ë° ê²€ìƒ‰ì–´ ì…ë ¥
            search_box = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "query"))
            )
            
            # ê²€ìƒ‰ì°½ í´ë¦¬ì–´ í›„ ê²€ìƒ‰ì–´ ì…ë ¥
            search_box.clear()
            search_box.send_keys(query)
            search_box.send_keys(Keys.RETURN)
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œ ëŒ€ê¸°
            time.sleep(3)
            
            self.logger.info(f"ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤í–‰: {query}")
            
            # ë‹¤ì–‘í•œ ì„ íƒìë¡œ ë§í¬ ì°¾ê¸°
            link_selectors = [
                # í†µí•©ê²€ìƒ‰ ê²°ê³¼
                'h3.title a[href*="http"]',
                '.total_wrap a[href*="http"]',
                '.result_area a[href*="http"]',
                '.data_area a[href*="http"]',
                
                # ì›¹ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼
                '.web_page a[href*="http"]',
                '.site_area a[href*="http"]',
                
                # ì¼ë°˜ ë§í¬ë“¤
                'a[href*="http"]:not([href*="naver.com"])',
                '.info_area a[href*="http"]',
                '.source_area a[href*="http"]'
            ]
            
            found_urls = []
            
            for selector in link_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    self.logger.info(f"ì„ íƒì '{selector}'ë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                    
                    for element in elements[:15]:  # ê° ì„ íƒìë‹¹ 15ê°œì”©
                        try:
                            href = element.get_attribute('href')
                            
                            if not href or href.startswith('#') or href.startswith('javascript:'):
                                continue
                            
                            # ë„¤ì´ë²„ ë‚´ë¶€ ë§í¬ í•„í„°ë§
                            if 'naver.com' in href and not self._extract_redirect_url(href):
                                continue
                            
                            # ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬
                            if 'naver.com' in href:
                                actual_url = self._extract_redirect_url(href)
                                if actual_url:
                                    url = actual_url
                                else:
                                    continue
                            else:
                                url = href
                            
                            # URL ìœ íš¨ì„± ê²€ì‚¬
                            if self.is_valid_homepage_url(url, organization_name):
                                found_urls.append(url)
                                self.logger.info(f"ìœ íš¨í•œ URL ë°œê²¬: {url}")
                                
                                if len(found_urls) >= self.max_search_results:
                                    break
                                    
                        except Exception as e:
                            continue
                    
                    if found_urls:
                        break
                        
                except Exception as e:
                    self.logger.warning(f"ì„ íƒì '{selector}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            # ì¶”ê°€ë¡œ ì œëª© í…ìŠ¤íŠ¸ í™•ì¸
            if not found_urls:
                self.logger.info("ì œëª© í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ê°€ ê²€ìƒ‰")
                found_urls = self._search_by_title_text(organization_name)
            
            # ê°€ì¥ ì í•©í•œ URL ì„ íƒ
            if found_urls:
                best_url = self.select_best_homepage(found_urls, organization_name)
                self.logger.info(f"ìµœì¢… ì„ íƒëœ URL: {best_url}")
                return best_url
            
            self.logger.warning(f"'{query}' ê²€ìƒ‰ì—ì„œ ìœ íš¨í•œ URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            return None
            
        except Exception as e:
            self.logger.warning(f"ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_redirect_url(self, naver_url: str) -> Optional[str]:
        """ë„¤ì´ë²„ ë¦¬ë‹¤ì´ë ‰íŠ¸ URLì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ"""
        try:
            if 'url=' in naver_url:
                import urllib.parse
                parsed_url = urllib.parse.urlparse(naver_url)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                if 'url' in query_params:
                    actual_url = query_params['url'][0]
                    return unquote(actual_url)
            return None
        except:
            return None
    
    def _search_by_title_text(self, organization_name: str) -> List[str]:
        """ì œëª© í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€ ê²€ìƒ‰"""
        found_urls = []
        try:
            # í™ˆí˜ì´ì§€ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë§í¬ ì°¾ê¸°
            homepage_keywords = ['í™ˆí˜ì´ì§€', 'ê³µì‹', 'official', 'home', 'www', 'ì‚¬ì´íŠ¸']
            
            all_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*="http"]')
            
            for link in all_links[:50]:
                try:
                    text = link.text.strip().lower()
                    href = link.get_attribute('href')
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ì— í™ˆí˜ì´ì§€ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                    if any(keyword in text for keyword in homepage_keywords):
                        if href and self.is_valid_homepage_url(href, organization_name):
                            found_urls.append(href)
                            self.logger.info(f"ì œëª© í…ìŠ¤íŠ¸ë¡œ ë°œê²¬: {href}")
                            
                            if len(found_urls) >= self.max_search_results:
                                break
                                
                except Exception:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"ì œëª© í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return found_urls
    
    def is_valid_homepage_url(self, url: str, organization_name: str) -> bool:
        """ìœ íš¨í•œ í™ˆí˜ì´ì§€ URLì¸ì§€ í™•ì¸"""
        try:
            if not url or len(url) < 10:
                return False
            
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return False
            
            domain = parsed.netloc.lower()
            
            # ì œì™¸í•  ë„ë©”ì¸ ì²´í¬
            for exclude_domain in EXCLUDE_DOMAINS:
                if exclude_domain in domain:
                    return False
            
            # ì¶”ê°€ ì œì™¸ íŒ¨í„´
            exclude_patterns = [
                r'\.pdf$', r'\.jpg$', r'\.png$', r'\.gif$',
                r'/board/', r'/bbs/', r'/community/',
                r'blog\.', r'cafe\.', r'post\.',
                r'search\.', r'maps\.', r'news\.'
            ]
            
            for pattern in exclude_patterns:
                if re.search(pattern, url.lower()):
                    return False
            
            # ë„ˆë¬´ ê¸´ URL ì œì™¸
            if len(url) > 200:
                return False
            
            # ê³µì‹ ë„ë©”ì¸ ìš°ì„ ìˆœìœ„
            official_tlds = ['.or.kr', '.go.kr', '.ac.kr', '.org', '.church']
            if any(tld in domain for tld in official_tlds):
                self.logger.info(f"ê³µì‹ ë„ë©”ì¸ ë°œê²¬: {domain}")
                return True
            
            # ê¸°ê´€ëª…ì´ ë„ë©”ì¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            name_parts = re.findall(r'[ê°€-í£a-zA-Z]+', organization_name.lower())
            for part in name_parts:
                if len(part) > 2 and part in domain:
                    self.logger.info(f"ê¸°ê´€ëª…ì´ ë„ë©”ì¸ì— í¬í•¨ë¨: {part} in {domain}")
                    return True
            
            return True
            
        except Exception:
            return False
    
    def select_best_homepage(self, urls: List[str], organization_name: str) -> str:
        """ê°€ì¥ ì í•©í•œ í™ˆí˜ì´ì§€ URL ì„ íƒ"""
        scored_urls = []
        
        for url in urls:
            score = 0
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # ê³µì‹ ë„ë©”ì¸ ë†’ì€ ì ìˆ˜
            if any(tld in domain for tld in ['.or.kr', '.go.kr', '.ac.kr']):
                score += 10
            elif '.org' in domain:
                score += 8
            elif '.church' in domain:
                score += 7
            elif '.com' in domain:
                score += 3
            
            # ê¸°ê´€ëª… ë§¤ì¹­ ì ìˆ˜
            name_parts = re.findall(r'[ê°€-í£a-zA-Z]+', organization_name.lower())
            for part in name_parts:
                if len(part) > 1 and part in domain:
                    score += 5
            
            # ê²½ë¡œ ì ìˆ˜
            path_depth = len(parsed.path.strip('/').split('/')) if parsed.path != '/' else 0
            score += max(0, 3 - path_depth)
            
            scored_urls.append((url, score))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_urls.sort(key=lambda x: x[1], reverse=True)
        
        best_url = scored_urls[0][0]
        self.logger.info(f"ìµœê³  ì ìˆ˜ URL ì„ íƒ: {best_url} (ì ìˆ˜: {scored_urls[0][1]})")
        
        return best_url
    
    def verify_homepage_url(self, url: str) -> bool:
        """í™ˆí˜ì´ì§€ URLì´ ì‹¤ì œë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸"""
        try:
            original_window = self.driver.current_window_handle
            
            # ìƒˆ íƒ­ì—ì„œ URL ê²€ì¦
            self.driver.execute_script("window.open('');")
            self.driver.switch_to.window(self.driver.window_handles[-1])
            
            self.driver.get(url)
            time.sleep(3)
            
            # í˜ì´ì§€ ë¡œë“œ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            is_accessible = "404" not in self.driver.title.lower() and \
                           "error" not in self.driver.title.lower() and \
                           len(self.driver.page_source) > 1000
            
            # ì›ë˜ ì°½ìœ¼ë¡œ ëŒì•„ê°€ê¸°
            self.driver.close()
            self.driver.switch_to.window(original_window)
            
            if is_accessible:
                self.logger.info(f"URL ì ‘ê·¼ ì„±ê³µ: {url}")
            else:
                self.logger.warning(f"URL ì ‘ê·¼ ì‹¤íŒ¨: {url}")
            
            return is_accessible
            
        except Exception as e:
            self.logger.warning(f"URL ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {url} - {e}")
            try:
                # ì˜¤ë¥˜ ì‹œ ì›ë˜ ì°½ìœ¼ë¡œ ëŒì•„ê°€ê¸°
                self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass
     
    def process_organizations(self, organizations: List[Dict]) -> List[Dict]:
        """ê¸°ê´€ ëª©ë¡ ì²˜ë¦¬"""
        processed_orgs = []
        total_count = len(organizations)
        success_count = 0
        
        try:
            for i, org in enumerate(organizations, 1):
                try:
                    self.logger.info(f"ì§„í–‰ ìƒí™©: {i}/{total_count} - {org.get('name', 'Unknown')}")
                    
                    # ê¸°ì¡´ ë°ì´í„° ì™„ì „ ë³µì‚¬
                    org_copy = org.copy()
                    
                    # ì´ë¯¸ í™ˆí˜ì´ì§€ê°€ ìˆê³  ìœ íš¨í•œ URLì´ë©´ ìŠ¤í‚µ
                    if org.get('homepage') and org['homepage'].strip() and org['homepage'].startswith(('http://', 'https://')):
                        self.logger.info(f"ì´ë¯¸ ìœ íš¨í•œ í™ˆí˜ì´ì§€ê°€ ìˆìŒ: {org['homepage']}")
                        processed_orgs.append(org_copy)
                        if org['homepage'].strip():
                            success_count += 1
                        continue
                    
                    # í™ˆí˜ì´ì§€ ê²€ìƒ‰ (ì§€ì—­ ì •ë³´ ì‚¬ìš© ì•ˆí•¨)
                    homepage_url = self.search_organization_homepage(
                        org.get('name', ''),
                        ""  # ì§€ì—­ ì •ë³´ ì „ë‹¬í•˜ì§€ ì•ŠìŒ
                    )
                    
                    # homepage í•„ë“œ ì—…ë°ì´íŠ¸
                    if homepage_url:
                        # URL ê²€ì¦
                        if self.verify_homepage_url(homepage_url):
                            org_copy['homepage'] = homepage_url
                            success_count += 1
                            self.logger.info(f"âœ… í™ˆí˜ì´ì§€ ì°¾ìŒ: {homepage_url}")
                        else:
                            org_copy['homepage'] = ""
                            self.logger.warning(f"âš ï¸ URL ì ‘ê·¼ ë¶ˆê°€: {homepage_url}")
                    else:
                        org_copy['homepage'] = ""
                        self.logger.warning(f"âŒ í™ˆí˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í•¨")
                    
                    processed_orgs.append(org_copy)
                    
                    # ì§„í–‰ ìƒí™© ì¶œë ¥
                    if i % 5 == 0:
                        self.logger.info(f"ğŸ“Š ì¤‘ê°„ í†µê³„: {success_count}/{i} ì„±ê³µ ({success_count/i*100:.1f}%)")
                    
                    # ìš”ì²­ ê°„ ì§€ì—°
                    if i < total_count:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ì§€ì—°
                        self.add_delay()
                    
                except Exception as e:
                    self.logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    org_copy = org.copy()
                    if not org.get('homepage'):
                        org_copy['homepage'] = ""
                    processed_orgs.append(org_copy)
                    continue
                    
        finally:
            # ë“œë¼ì´ë²„ ì¢…ë£Œ
            self.close_driver()
        
        return processed_orgs

    def save_results(self, organizations: List[Dict], output_file: str) -> bool:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(organizations, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ” ê¸°ê´€ í™ˆí˜ì´ì§€ URL ì¶”ì¶œê¸° (Selenium ê¸°ë°˜)")
    print("=" * 60)
    
    try:
        # headless ëª¨ë“œ ì„¤ì • (Falseë¡œ ì„¤ì •í•˜ì—¬ ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ)
        use_headless = False
        print(f"ğŸŒ ë¸Œë¼ìš°ì € ëª¨ë“œ: {'Headless' if use_headless else 'GUI'}")
        
        # ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        extractor = URLExtractor(headless=use_headless)
        
        # ì…ë ¥/ì¶œë ¥ íŒŒì¼ ì„¤ì •
        input_file = r"C:\Users\kimyh\makedb\Python\cradcrawl_adv\undefined_converted_20250609_134731.json"
        output_file = f"raw_data_with_homepages_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {input_file}")
        print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼: {output_file}")
        
        # ì…ë ¥ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(input_file):
            print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
            return 1
        
        # JSON íŒŒì¼ ë¡œë“œ
        print("ğŸ“– ë°ì´í„° ë¡œë”© ì¤‘...")
        with open(input_file, 'r', encoding='utf-8') as f:
            organizations = json.load(f)
        
        print(f"ğŸ“Š ë¡œë“œëœ ê¸°ê´€ ìˆ˜: {len(organizations)}")
        
        # ë¹ˆ homepage í•„ë“œ ê°œìˆ˜ í™•ì¸
        empty_homepage_count = sum(1 for org in organizations if not org.get('homepage') or not org['homepage'].strip())
        print(f"ğŸ” í™ˆí˜ì´ì§€ê°€ ì—†ëŠ” ê¸°ê´€: {empty_homepage_count}ê°œ")
        
        # ì²˜ë¦¬í•  ê¸°ê´€ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
        max_process = int(input(f"ì²˜ë¦¬í•  ê¸°ê´€ ìˆ˜ (ì „ì²´: {len(organizations)}, ì—”í„° ì‹œ ì „ì²´): ") or len(organizations))
        organizations = organizations[:max_process]
        
        print(f"ğŸ”„ {len(organizations)}ê°œ ê¸°ê´€ ì²˜ë¦¬ ì‹œì‘...")
        print("ğŸ“ Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë¸Œë¼ìš°ì €ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        
        # ê¸°ê´€ ì²˜ë¦¬
        processed_organizations = extractor.process_organizations(organizations)
        
        # ê²°ê³¼ ì €ì¥
        if extractor.save_results(processed_organizations, output_file):
            print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {output_file}")
            
            # í†µê³„ ì¶œë ¥
            homepage_found = sum(1 for org in processed_organizations if org.get('homepage') and org['homepage'].strip())
            original_homepage_count = sum(1 for org in organizations if org.get('homepage') and org['homepage'].strip())
            new_homepage_count = homepage_found - original_homepage_count
            
            print(f"ğŸ“ˆ ìµœì¢… í†µê³„:")
            print(f"  - ê¸°ì¡´ í™ˆí˜ì´ì§€: {original_homepage_count}ê°œ")
            print(f"  - ìƒˆë¡œ ì°¾ì€ í™ˆí˜ì´ì§€: {new_homepage_count}ê°œ")
            print(f"  - ì „ì²´ í™ˆí˜ì´ì§€: {homepage_found}/{len(processed_organizations)}ê°œ")
            print(f"  - ì„±ê³µë¥ : {homepage_found/len(processed_organizations)*100:.1f}%")
            
        else:
            print(f"\nâŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")
            return 1
        
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())