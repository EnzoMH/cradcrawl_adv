#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê³ ê¸‰ êµíšŒ/ê¸°ê´€/ê³µë¶€ë°© /ê¸°ê´€/ê³µë¶€ë°© ì—°ë½ì²˜ í¬ë¡¤ëŸ¬
í™ˆí˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  AIë¥¼ í™œìš©í•˜ì—¬ ì—°ë½ì²˜ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import json
import requests
import asyncio
import re
import time
import random
import os
import glob
from datetime import datetime
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse
import logging

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    print("âœ… .env íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install python-dotenv ì‹¤í–‰ í•„ìš”")
    print("ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì„ ì‹œë„í•©ë‹ˆë‹¤...")
    
    # .env íŒŒì¼ ìˆ˜ë™ ë¡œë“œ
    try:
        env_path = os.path.join(os.getcwd(), '.env')
        if os.path.exists(env_path):
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip().strip('"\'')  # ë”°ì˜´í‘œ ì œê±°
                        os.environ[key] = value
            print("âœ… .env íŒŒì¼ ìˆ˜ë™ ë¡œë“œ ì™„ë£Œ")
        else:
            print("âŒ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

from bs4 import BeautifulSoup
from parser import WebPageParser
from validator import ContactValidator, AIValidator
from ai_helpers import AIModelManager
import urllib3

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class AdvancedChurchCrawler:
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.setup_logger()
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv('GEMINI_API_KEY')
        if api_key:
            print(f"ğŸ”‘ GEMINI_API_KEY ë¡œë“œ ì„±ê³µ: {api_key[:10]}...{api_key[-4:]}")
        else:
            print("âŒ GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ .env íŒŒì¼ì— GEMINI_API_KEY='your_api_key' í˜•ì‹ìœ¼ë¡œ ì €ì¥í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.web_parser = WebPageParser()
        self.validator = ContactValidator()
        
        # AI ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.ai_manager = None
        self.use_ai = False
        
        try:
            if api_key:
                self.ai_manager = AIModelManager()
                self.use_ai = True
                print("ğŸ¤– AI ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì„±ê³µ")
            else:
                print("ğŸ”§ AI ê¸°ëŠ¥ ë¹„í™œì„±í™” (API í‚¤ ì—†ìŒ)")
        except Exception as e:
            print(f"âŒ AI ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ai_manager = None
            self.use_ai = False
        
        # êµ¬ê¸€ ê²€ìƒ‰ ê¸°ë°˜ ì—°ë½ì²˜ ê²€ìƒ‰ê¸° (ì§€ì—° ì´ˆê¸°í™”)
        self.google_searcher = None
        
        # URL ì¶”ì¶œê¸° ì¶”ê°€
        try:
            from legacy.url_extractor_enhanced import URLExtractorEnhanced
            self.url_extractor = URLExtractorEnhanced(headless=False)
            print("ğŸ” URL ì¶”ì¶œê¸° ì´ˆê¸°í™” ì„±ê³µ")
        except ImportError:
            print("âš ï¸ URL ì¶”ì¶œê¸° import ì‹¤íŒ¨")
            self.url_extractor = None
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.timeout = 15  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
        self.max_retries = 3
        self.delay_range = (2, 5)  # ë”œë ˆì´ ì¦ê°€
        
        # í†µê³„ (êµ¬ê¸€ ê²€ìƒ‰ í†µê³„ ì¶”ê°€)
        self.stats = {
            'total_processed': 0,
            'successful_crawls': 0,
            'failed_crawls': 0,
            'ai_enhanced': 0,
            'contacts_found': 0,
            'api_calls_made': 0,
            'ai_failures': 0,
            'google_searches_performed': 0,  # ìƒˆ í†µê³„
            'google_contacts_found': 0       # ìƒˆ í†µê³„
        }
        
        # ì„¸ì…˜ ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # AI ê²€ì¦ê¸°
        try:
            from validator import AIValidator
            self.ai_validator = AIValidator()
            print("ğŸ” AI URL/ì—°ë½ì²˜ ê²€ì¦ê¸° ì´ˆê¸°í™” ì„±ê³µ")
        except ImportError:
            print("âš ï¸ AI ê²€ì¦ê¸° import ì‹¤íŒ¨")
            self.ai_validator = None
        except Exception as e:
            print(f"âŒ AI ê²€ì¦ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ai_validator = None
        
        print("ğŸš€ ê³ ê¸‰ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ - êµ¬ê¸€ ê²€ìƒ‰ ê¸°ëŠ¥ í¬í•¨")

    def setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        self.logger = logging.getLogger('adv_crawler')
        self.logger.setLevel(logging.INFO)
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # í¬ë§·í„°
        formatter = logging.Formatter('%(asctime)s - [ê³ ê¸‰í¬ë¡¤ëŸ¬] - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(console_handler)
    
    def load_json_data(self, filepath: str) -> List[Dict]:
        """JSON íŒŒì¼ ë¡œë“œ"""
        try:
            print(f"ğŸ“‚ JSON íŒŒì¼ ë¡œë”©: {filepath}")
            
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                print(f"âœ… {len(data)}ê°œ êµíšŒ/ê¸°ê´€/ê³µë¶€ë°©  ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
                return data
            else:
                print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” JSON í˜•ì‹ì…ë‹ˆë‹¤.")
                return []
                
        except Exception as e:
            print(f"âŒ JSON íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return []
    
    def fetch_webpage(self, url: str) -> Optional[str]:
        """ì›¹í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        if not url or not url.startswith(('http://', 'https://')):
            return None
        
        try:
            self.logger.info(f"ì›¹í˜ì´ì§€ ìš”ì²­: {url}")
            
            response = self.session.get(
                url, 
                timeout=self.timeout, 
                verify=False,
                allow_redirects=True
            )
            
            # ì¸ì½”ë”© ì„¤ì •
            response.encoding = response.apparent_encoding or 'utf-8'
            
            if response.status_code == 200:
                self.logger.info(f"ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {url}")
                return response.text
            else:
                self.logger.warning(f"HTTP ì˜¤ë¥˜ {response.status_code}: {url}")
                return None
                
        except requests.exceptions.Timeout:
            self.logger.warning(f"íƒ€ì„ì•„ì›ƒ: {url}")
            return None
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"ìš”ì²­ ì‹¤íŒ¨: {url} - {e}")
            return None
        except Exception as e:
            self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {url} - {e}")
            return None
    
    def fetch_webpage_enhanced(self, url: str, max_retries: int = 3) -> Optional[str]:
        """ê°•í™”ëœ ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ì°¨ë‹¨ ìš°íšŒ)"""
        if not url or not url.startswith(('http://', 'https://')):
            return None
        
        # ë‹¤ì–‘í•œ í—¤ë” ì„¸íŠ¸
        header_sets = [
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1'
            },
            {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            },
            {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko,en-US;q=0.9,en;q=0.8',
                'Cache-Control': 'max-age=0'
            }
        ]
        
        for attempt in range(max_retries):
            try:
                # í—¤ë” ë¡œí…Œì´ì…˜
                headers = header_sets[attempt % len(header_sets)]
                
                self.logger.info(f"ì›¹í˜ì´ì§€ ìš”ì²­ ì‹œë„ {attempt + 1}/{max_retries}: {url}")
                
                # ìƒˆ ì„¸ì…˜ ìƒì„± (í•„ìš”ì‹œ)
                if attempt > 0:
                    self.session.close()
                    self.session = requests.Session()
                
                # í—¤ë” ì„¤ì •
                self.session.headers.clear()
                self.session.headers.update(headers)
                
                # ìš”ì²­ íŒŒë¼ë¯¸í„°
                request_params = {
                    'timeout': (10, 30),  # (ì—°ê²°, ì½ê¸°) íƒ€ì„ì•„ì›ƒ
                    'verify': False,
                    'allow_redirects': True,
                    'stream': False
                }
                
                # ë¦¬í¼ëŸ¬ ì„¤ì • (2ë²ˆì§¸ ì‹œë„ë¶€í„°)
                if attempt > 0:
                    parsed_url = urlparse(url)
                    referer = f"{parsed_url.scheme}://{parsed_url.netloc}"
                    self.session.headers['Referer'] = referer
                
                response = self.session.get(url, **request_params)
                
                # ì‘ë‹µ ìƒíƒœ í™•ì¸
                if response.status_code == 200:
                    # ì¸ì½”ë”© ì„¤ì •
                    if response.encoding is None:
                        response.encoding = response.apparent_encoding or 'utf-8'
                    
                    content = response.text
                    
                    # ì°¨ë‹¨ í˜ì´ì§€ ê°ì§€
                    if self.is_blocked_content(content):
                        self.logger.warning(f"ì°¨ë‹¨ëœ ì½˜í…ì¸  ê°ì§€ (ì‹œë„ {attempt + 1}): {url}")
                        
                        if attempt < max_retries - 1:
                            delay = random.uniform(5, 10)
                            self.logger.info(f"{delay:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                            time.sleep(delay)
                            continue
                        else:
                            return None
                    
                    self.logger.info(f"ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {url}")
                    return content
                    
                elif response.status_code == 403:
                    self.logger.warning(f"ì ‘ê·¼ ê±°ë¶€ (403) - ì‹œë„ {attempt + 1}: {url}")
                    
                    if attempt < max_retries - 1:
                        # ë” ê¸´ ëŒ€ê¸°
                        delay = random.uniform(10, 20)
                        time.sleep(delay)
                        continue
                        
                elif response.status_code == 429:
                    self.logger.warning(f"ìš”ì²­ ì œí•œ (429) - ì‹œë„ {attempt + 1}: {url}")
                    
                    if attempt < max_retries - 1:
                        # ë§¤ìš° ê¸´ ëŒ€ê¸°
                        delay = random.uniform(30, 60)
                        self.logger.info(f"ìš”ì²­ ì œí•œìœ¼ë¡œ ì¸í•œ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                        time.sleep(delay)
                        continue
                        
                else:
                    self.logger.warning(f"HTTP ì˜¤ë¥˜ {response.status_code} (ì‹œë„ {attempt + 1}): {url}")
                    
            except requests.exceptions.Timeout:
                self.logger.warning(f"íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}): {url}")
                
            except requests.exceptions.ConnectionError:
                self.logger.warning(f"ì—°ê²° ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {url}")
                
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {url} - {e}")
                
            except Exception as e:
                self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {url} - {e}")
            
            # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            if attempt < max_retries - 1:
                delay = random.uniform(3, 8)
                time.sleep(delay)
        
        self.logger.error(f"ëª¨ë“  ì‹œë„ ì‹¤íŒ¨: {url}")
        return None

    def is_blocked_content(self, content: str) -> bool:
        """ì°¨ë‹¨ëœ ì½˜í…ì¸  ê°ì§€"""
        if not content or len(content) < 100:
            return True
        
        block_indicators = [
            'access denied',
            'forbidden',
            'blocked',
            'ì ‘ê·¼ì´ ê±°ë¶€',
            'ì°¨ë‹¨ëœ',
            'cloudflare',
            'checking your browser',
            'ddos protection',
            'security check',
            'captcha'
        ]
        
        content_lower = content.lower()
        return any(indicator in content_lower for indicator in block_indicators)

    def parse_with_bs4(self, html_content: str, base_url: str) -> Dict[str, Any]:
        """BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            parsed_data = {
                'title': '',
                'meta_description': '',
                'all_text': '',
                'footer_text': '',
                'contact_sections': []
            }
            
            # ì œëª© ì¶”ì¶œ
            title_tag = soup.find('title')
            if title_tag:
                parsed_data['title'] = title_tag.get_text().strip()
            
            # ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                parsed_data['meta_description'] = meta_desc.get('content', '')
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°)
            for script in soup(["script", "style"]):
                script.decompose()
            
            parsed_data['all_text'] = soup.get_text()
            
            # footer ì˜ì—­ ì¶”ì¶œ
            footer_elements = soup.find_all(['footer', 'div'], 
                                        class_=re.compile(r'footer|bottom|contact|info', re.I))
            footer_texts = []
            for footer in footer_elements:
                footer_text = footer.get_text().strip()
                if footer_text and len(footer_text) > 20:
                    footer_texts.append(footer_text)
            
            parsed_data['footer_text'] = '\n'.join(footer_texts)
            
            # ì—°ë½ì²˜ ê´€ë ¨ ì„¹ì…˜ ì¶”ì¶œ (ìˆ˜ì •ëœ ë¶€ë¶„)
            contact_keywords = ['ì—°ë½ì²˜', 'contact', 'ì „í™”', 'phone', 'íŒ©ìŠ¤', 'fax', 'ì´ë©”ì¼', 'email']
            # text= ëŒ€ì‹  string= ì‚¬ìš©
            contact_elements = soup.find_all(string=re.compile('|'.join(contact_keywords), re.I))
            
            for element in contact_elements[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                parent = element.parent
                if parent:
                    section_text = parent.get_text().strip()
                    if len(section_text) > 10:
                        parsed_data['contact_sections'].append(section_text)
            
            return parsed_data
            
        except Exception as e:
            self.logger.error(f"BS4 íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {'all_text': html_content[:5000]}  # ì‹¤íŒ¨ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì¼ë¶€ ë°˜í™˜
    
    def extract_with_parser(self, parsed_data: Dict[str, Any]) -> Dict[str, List]:
        """parser.pyë¥¼ ì´ìš©í•œ ê¸°ë³¸ ì¶”ì¶œ"""
        try:
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
            all_text = parsed_data.get('all_text', '')
            footer_text = parsed_data.get('footer_text', '')
            contact_sections = ' '.join(parsed_data.get('contact_sections', []))
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ í…ìŠ¤íŠ¸ ê²°í•©
            combined_text = f"{footer_text}\n{contact_sections}\n{all_text}"
            
            # parserë¡œ ì¶”ì¶œ
            extracted_contacts = self.web_parser.extract_contact_info(combined_text)
            
            self.logger.info(f"Parser ì¶”ì¶œ ì™„ë£Œ: {len(extracted_contacts.get('phones', []))}ê°œ ì „í™”ë²ˆí˜¸, "
                           f"{len(extracted_contacts.get('faxes', []))}ê°œ íŒ©ìŠ¤ë²ˆí˜¸, "
                           f"{len(extracted_contacts.get('emails', []))}ê°œ ì´ë©”ì¼")
            
            return extracted_contacts
            
        except Exception as e:
            self.logger.error(f"Parser ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {'phones': [], 'faxes': [], 'emails': [], 'addresses': []}
    
    def validate_with_validator(self, extracted_data: Dict[str, List]) -> Dict[str, List]:
        """validator.pyë¥¼ ì´ìš©í•œ ê²€ì¦"""
        try:
            validated_data = {
                'phones': [],
                'faxes': [],
                'emails': [],
                'addresses': [],
                'postal_codes': []
            }
            
            # ì „í™”ë²ˆí˜¸ ê²€ì¦
            for phone in extracted_data.get('phones', []):
                is_valid, result = self.validator.validate_phone_number(phone)
                if is_valid:
                    validated_data['phones'].append(result)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦
            for fax in extracted_data.get('faxes', []):
                is_valid, result = self.validator.validate_fax_number(fax)
                if is_valid:
                    validated_data['faxes'].append(result)
            
            # ì´ë©”ì¼ì€ ê¸°ë³¸ ê²€ì¦ë§Œ (validatorì— ì´ë©”ì¼ ê²€ì¦ í•¨ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ)
            validated_data['emails'] = extracted_data.get('emails', [])
            validated_data['addresses'] = extracted_data.get('addresses', [])
            
            # ìš°í¸ë²ˆí˜¸ ì¶”ì¶œ (ì£¼ì†Œì—ì„œ)
            for address in validated_data['addresses']:
                postal_matches = re.findall(r'\b\d{5}\b', address)
                for postal in postal_matches:
                    if postal not in validated_data['postal_codes']:
                        validated_data['postal_codes'].append(postal)
            
            self.logger.info(f"Validator ê²€ì¦ ì™„ë£Œ: {len(validated_data['phones'])}ê°œ ìœ íš¨ ì „í™”ë²ˆí˜¸, "
                           f"{len(validated_data['faxes'])}ê°œ ìœ íš¨ íŒ©ìŠ¤ë²ˆí˜¸")
            
            return validated_data
            
        except Exception as e:
            self.logger.error(f"Validator ê²€ì¦ ì˜¤ë¥˜: {e}")
            return extracted_data
    
    async def enhance_with_ai(self, parsed_data: Dict[str, Any], church_name: str) -> Dict[str, List]:
        """AIë¥¼ ì´ìš©í•œ ì¶”ê°€ ì¶”ì¶œ"""
        if not self.use_ai or not self.ai_manager:
            print(f"  ğŸ”§ AI ê¸°ëŠ¥ ë¹„í™œì„±í™” - ê¸°ë³¸ ì¶”ì¶œë§Œ ì‚¬ìš©")
            return {}
        
        try:
            print(f"  ğŸ¤– AI ì¶”ê°€ ì¶”ì¶œ ì‹œì‘: {church_name}")
            self.stats['api_calls_made'] += 1
            
            # AIìš© í…ìŠ¤íŠ¸ ì¤€ë¹„ (ê¸¸ì´ ì œí•œ)
            all_text = parsed_data.get('all_text', '')
            footer_text = parsed_data.get('footer_text', '')
            contact_sections = ' '.join(parsed_data.get('contact_sections', []))
            
            # ì¤‘ìš” ì„¹ì…˜ ìš°ì„  ì¡°í•©
            ai_text_parts = []
            if footer_text:
                ai_text_parts.append(f"=== Footer ì •ë³´ ===\n{footer_text[:1500]}")
            if contact_sections:
                ai_text_parts.append(f"=== ì—°ë½ì²˜ ì„¹ì…˜ ===\n{contact_sections[:1500]}")
            if all_text:
                ai_text_parts.append(f"=== ê¸°íƒ€ ë‚´ìš© ===\n{all_text[:2000]}")
            
            ai_text = '\n\n'.join(ai_text_parts)
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ìµœëŒ€ 5000ì)
            if len(ai_text) > 5000:
                ai_text = ai_text[:5000]
            
            # AI í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt_template = """
'{church_name}' êµíšŒ/ê¸°ê´€/ê³µë¶€ë°© ì˜ ì—°ë½ì²˜ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**êµíšŒ/ê¸°ê´€/ê³µë¶€ë°© ëª…:** {church_name}

**ì¶”ì¶œí•  ì •ë³´:**
- ì „í™”ë²ˆí˜¸: í•œêµ­ í˜•ì‹ (02-1234-5678, 031-123-4567, 010-1234-5678)
- íŒ©ìŠ¤ë²ˆí˜¸: í•œêµ­ í˜•ì‹ (02-1234-5679)
- ì´ë©”ì¼: ìœ íš¨í•œ í˜•ì‹ (info@church.com)
- íœ´ëŒ€í°: 010ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë²ˆí˜¸
- ìš°í¸ë²ˆí˜¸: 5ìë¦¬ ìˆ«ì
- ì£¼ì†Œ: ì™„ì „í•œ ì£¼ì†Œ

**ì‘ë‹µ í˜•ì‹:** (ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”)
```markdown
## ì—°ë½ì²˜ ì •ë³´

**ì „í™”ë²ˆí˜¸:** [ë°œê²¬ëœ ë²ˆí˜¸ ë˜ëŠ” "ì—†ìŒ"]
**íŒ©ìŠ¤ë²ˆí˜¸:** [ë°œê²¬ëœ ë²ˆí˜¸ ë˜ëŠ” "ì—†ìŒ"]  
**ì´ë©”ì¼:** [ë°œê²¬ëœ ì´ë©”ì¼ ë˜ëŠ” "ì—†ìŒ"]
**íœ´ëŒ€í°:** [ë°œê²¬ëœ íœ´ëŒ€í° ë˜ëŠ” "ì—†ìŒ"]
**ìš°í¸ë²ˆí˜¸:** [ë°œê²¬ëœ ìš°í¸ë²ˆí˜¸ ë˜ëŠ” "ì—†ìŒ"]
**ì£¼ì†Œ:** [ë°œê²¬ëœ ì£¼ì†Œ ë˜ëŠ” "ì—†ìŒ"]
```

**ì¤‘ìš” ê·œì¹™:**
1. {church_name}ì™€ ì§ì ‘ ê´€ë ¨ëœ ì—°ë½ì²˜ë§Œ ì¶”ì¶œ
2. ëŒ€í‘œë²ˆí˜¸, ë©”ì¸ ì—°ë½ì²˜ ìš°ì„ 
3. ëŒ€í‘œë²ˆí™”ì™€ íŒ©ìŠ¤ë²ˆí˜¸ëŠ” ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë‹ˆ ì´ì  í™•ì‹¤íˆ í•  ê²ƒ
4. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œ

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
{{text_content}}
"""
            
            # í”„ë¡¬í”„íŠ¸ì— êµíšŒ/ê¸°ê´€/ê³µë¶€ë°© ëª…ê³¼ í…ìŠ¤íŠ¸ ì‚½ì…
            final_prompt = prompt_template.format(church_name=church_name)
            
            # AI í˜¸ì¶œ
            ai_response = await self.ai_manager.extract_with_gemini(ai_text, final_prompt)
            
            if ai_response:
                ai_extracted = self.parse_ai_response(ai_response)
                self.stats['ai_enhanced'] += 1
                print(f"  âœ… AI ì¶”ì¶œ ì™„ë£Œ: {church_name}")
                return ai_extracted
            else:
                print(f"  âš ï¸ AI ì‘ë‹µ ì—†ìŒ: {church_name}")
                self.stats['ai_failures'] += 1
                return {}
                
        except Exception as e:
            print(f"  âŒ AI ì¶”ì¶œ ì˜¤ë¥˜ ({church_name}): {e}")
            self.stats['ai_failures'] += 1
            self.logger.error(f"AI ì¶”ì¶œ ì˜¤ë¥˜ ({church_name}): {e}")
            return {}
    
    def parse_ai_response(self, ai_response: str) -> Dict[str, List]:
        """AI ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”"""
        try:
            result = {
                'phones': [],
                'faxes': [],
                'emails': [],
                'mobiles': [],
                'postal_codes': [],
                'addresses': []
            }
            
            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ íŒŒì‹±
            lines = ai_response.split('\n')
            
            for line in lines:
                line = line.strip()
                if ':' in line and ('**' in line or '*' in line):
                    # ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ ì œê±°
                    line = line.replace('**', '').replace('*', '')
                    
                    try:
                        key, value = line.split(':', 1)
                        key = key.strip().lower()
                        value = value.strip()
                        
                        if value and value not in ["ì—†ìŒ", "ì •ë³´ì—†ìŒ", "í™•ì¸ì•ˆë¨", "-"]:
                            if 'ì „í™”ë²ˆí˜¸' in key or 'phone' in key:
                                if self._is_valid_phone_format(value):
                                    result['phones'].append(value)
                            elif 'íŒ©ìŠ¤' in key or 'fax' in key:
                                if self._is_valid_phone_format(value):
                                    result['faxes'].append(value)
                            elif 'ì´ë©”ì¼' in key or 'email' in key:
                                if self._is_valid_email_format(value):
                                    result['emails'].append(value)
                            elif 'íœ´ëŒ€í°' in key or 'mobile' in key:
                                if value.startswith('010') and self._is_valid_phone_format(value):
                                    result['mobiles'].append(value)
                            elif 'ìš°í¸ë²ˆí˜¸' in key or 'postal' in key:
                                if re.match(r'^\d{5}$', value):
                                    result['postal_codes'].append(value)
                            elif 'ì£¼ì†Œ' in key or 'address' in key:
                                if len(value) > 10:
                                    result['addresses'].append(value)
                    except ValueError:
                        continue
            
            return result
            
        except Exception as e:
            self.logger.error(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {}
    
    def _is_valid_phone_format(self, phone: str) -> bool:
        """ì „í™”ë²ˆí˜¸ í˜•ì‹ ê²€ì¦"""
        phone_pattern = r'^\d{2,3}-\d{3,4}-\d{4}$'
        return bool(re.match(phone_pattern, phone))
    
    def _is_valid_email_format(self, email: str) -> bool:
        """ì´ë©”ì¼ í˜•ì‹ ê²€ì¦"""
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return bool(re.match(email_pattern, email))
    
    def merge_ai_and_parser_results(self, ai_result: Dict, parser_result: Dict) -> Dict:
        """AI ê²°ê³¼ì™€ íŒŒì„œ ê²°ê³¼ ë³‘í•©"""
        merged = {
            'phone': [],
            'fax': [],
            'email': [],
            'address': [],
            'postal_code': []
        }
        
        # AI ê²°ê³¼ ìš°ì„ , íŒŒì„œ ê²°ê³¼ë¡œ ë³´ì™„
        field_mappings = [
            ('phones', 'phone'),
            ('faxes', 'fax'),
            ('emails', 'email'),
            ('addresses', 'address'),
            ('postal_codes', 'postal_code')
        ]
        
        for ai_field, merged_field in field_mappings:
            # AI ê²°ê³¼
            ai_values = ai_result.get(ai_field, [])
            # íŒŒì„œ ê²°ê³¼
            parser_values = parser_result.get(ai_field, [])
            
            # ì¤‘ë³µ ì œê±°í•˜ì—¬ ë³‘í•©
            all_values = list(set(ai_values + parser_values))
            merged[merged_field] = all_values[0] if all_values else ""
        
        return merged
    
    async def process_single_church(self, church_data: Dict) -> Dict:
        """ë‹¨ì¼ êµíšŒ/ê¸°ê´€ ì²˜ë¦¬ (URL ê²€ìƒ‰ + ì—°ë½ì²˜ ì¶”ì¶œ + êµ¬ê¸€ ê²€ìƒ‰)"""
        church_name = church_data.get('name', 'Unknown')
        homepage = church_data.get('homepage', '')
        
        print(f"\nğŸ¢ ì²˜ë¦¬ ì¤‘: {church_name}")
        
        result = church_data.copy()
        extraction_summary = {
            'url_search_performed': False,
            'homepage_status': 'existing' if homepage else 'none',
            'parser_extracted': {},
            'validator_result': {},
            'ai_enhanced': {},
            'google_search_result': {},  # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
            'final_merged': {},
            'extraction_timestamp': datetime.now().isoformat(),
            'ai_used': self.use_ai
        }
        
        self.stats['total_processed'] += 1
        
        # êµ¬ê¸€ ê²€ìƒ‰ ê¸°ë°˜ ì—°ë½ì²˜ ê²€ìƒ‰ ì´ˆê¸°í™” (ì•ˆì „í•œ ì´ˆê¸°í™”)
        if not hasattr(self, 'google_searcher') or self.google_searcher is None:
            try:
                print("  ğŸ”§ êµ¬ê¸€ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
                self.google_searcher = GoogleContactSearcher()
                print("  âœ… êµ¬ê¸€ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"  âŒ êµ¬ê¸€ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.google_searcher = None
        
        # í™ˆí˜ì´ì§€ê°€ ì—†ëŠ” ê²½ìš° URL ê²€ìƒ‰
        if not homepage and self.url_extractor:
            print(f"  ğŸ” í™ˆí˜ì´ì§€ URL ê²€ìƒ‰ ì¤‘...")
            try:
                homepage = self.url_extractor.search_organization_homepage(church_name)
                if homepage:
                    result['homepage'] = homepage
                    extraction_summary['url_search_performed'] = True
                    extraction_summary['homepage_status'] = 'found'
                    print(f"  âœ… í™ˆí˜ì´ì§€ ë°œê²¬: {homepage}")
                else:
                    print(f"  âŒ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹¤íŒ¨")
                    extraction_summary['homepage_status'] = 'not_found'
            except Exception as e:
                print(f"  âŒ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                extraction_summary['homepage_status'] = 'search_error'
        
        # 1ë‹¨ê³„: êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ì—°ë½ì²˜ ì§ì ‘ ê²€ìƒ‰ (ìš°ì„  ì‹œë„)
        google_contacts = {'phones': [], 'faxes': [], 'emails': [], 'addresses': []}
        
        if self.google_searcher is not None:
            try:
                print(f"  ğŸ” êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ì—°ë½ì²˜ ì •ë³´ ê²€ìƒ‰...")
                google_contacts = await self.google_searcher.search_organization_contacts(church_name)
                extraction_summary['google_search_result'] = google_contacts
                self.stats['google_searches_performed'] += 1
                
                if sum(len(v) for v in google_contacts.values()) > 0:
                    self.stats['google_contacts_found'] += 1
            except Exception as e:
                print(f"  âŒ êµ¬ê¸€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                extraction_summary['google_search_result'] = {'error': str(e)}
        else:
            print(f"  âš ï¸ êµ¬ê¸€ ê²€ìƒ‰ê¸° ì‚¬ìš© ë¶ˆê°€ - ê±´ë„ˆëœ€")
            extraction_summary['google_search_result'] = {'disabled': 'GoogleContactSearcher ì´ˆê¸°í™” ì‹¤íŒ¨'}
        
        # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•œ ê²½ìš° í™ˆí˜ì´ì§€ í¬ë¡¤ë§ ìƒëµ
        google_contact_count = sum(len(v) for v in google_contacts.values())
        skip_homepage_crawl = google_contact_count >= 3
        
        if skip_homepage_crawl:
            print(f"  âš¡ êµ¬ê¸€ ê²€ìƒ‰ì—ì„œ ì¶©ë¶„í•œ ì—°ë½ì²˜ ë°œê²¬ ({google_contact_count}ê°œ), í™ˆí˜ì´ì§€ í¬ë¡¤ë§ ìƒëµ")
        
        # 2ë‹¨ê³„: í™ˆí˜ì´ì§€ê°€ ìˆê³  êµ¬ê¸€ ê²€ìƒ‰ì´ ë¶ˆì¶©ë¶„í•œ ê²½ìš°ì—ë§Œ í¬ë¡¤ë§
        parser_result = {}
        validator_result = {}
        ai_result = {}
        
        if homepage and not skip_homepage_crawl:
            try:
                # ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ê°•í™”ëœ ë²„ì „ ì‚¬ìš©)
                print(f"  ğŸŒ í™ˆí˜ì´ì§€ ì ‘ì†: {homepage}")
                html_content = self.fetch_webpage_enhanced(homepage)
                
                if html_content:
                    # BS4ë¡œ íŒŒì‹±
                    print(f"  ğŸ“„ HTML íŒŒì‹± ì¤‘...")
                    parsed_data = self.parse_with_bs4(html_content, homepage)
                    extraction_summary['homepage_status'] = 'parsed'
                    
                    # parser.pyë¡œ ê¸°ë³¸ ì¶”ì¶œ
                    print(f"  ğŸ” ê¸°ë³¸ ì—°ë½ì²˜ ì¶”ì¶œ ì¤‘...")
                    parser_result = self.extract_with_parser(parsed_data)
                    extraction_summary['parser_extracted'] = parser_result
                    
                    # validator.pyë¡œ ê²€ì¦
                    print(f"  âœ… ì—°ë½ì²˜ ê²€ì¦ ì¤‘...")
                    validator_result = self.validate_with_validator(parser_result)
                    extraction_summary['validator_result'] = validator_result
                    
                    # AIë¡œ ì¶”ê°€ ì¶”ì¶œ
                    ai_result = await self.enhance_with_ai(parsed_data, church_name)
                    extraction_summary['ai_enhanced'] = ai_result
                    
                    extraction_summary['homepage_status'] = 'completed'
                    self.stats['successful_crawls'] += 1
                else:
                    print(f"  âŒ í™ˆí˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨")
                    extraction_summary['homepage_status'] = 'fetch_failed'
                    self.stats['failed_crawls'] += 1
                    
            except Exception as e:
                print(f"  âŒ í™ˆí˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                extraction_summary['homepage_status'] = 'error'
                extraction_summary['error'] = str(e)
                self.stats['failed_crawls'] += 1
        
        # 3ë‹¨ê³„: ëª¨ë“  ê²°ê³¼ ë³‘í•© (êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨)
        print(f"  ğŸ”„ ê²°ê³¼ ë³‘í•© ì¤‘...")
        merged_result = self.merge_all_results(google_contacts, ai_result, parser_result)
        extraction_summary['final_merged'] = merged_result
        
        # 4ë‹¨ê³„: ê¸°ì¡´ ë¹ˆ ê°’ì„ ì¶”ì¶œëœ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        contact_fields = ['phone', 'fax', 'email', 'mobile', 'postal_code', 'address']
        updated_fields = []
        
        for field in contact_fields:
            if not result.get(field) and merged_result.get(field):
                result[field] = merged_result[field]
                updated_fields.append(field)
        
        if updated_fields:
            print(f"  âœ¨ ì—…ë°ì´íŠ¸ëœ í•„ë“œ: {', '.join(updated_fields)}")
            self.stats['contacts_found'] += 1
        
        extraction_summary['updated_fields'] = updated_fields
        result['extraction_summary'] = extraction_summary
        
        print(f"  âœ… ì²˜ë¦¬ ì™„ë£Œ: {church_name}")
        return result

    def merge_all_results(self, google_result: Dict, ai_result: Dict, parser_result: Dict) -> Dict:
        """êµ¬ê¸€ ê²€ìƒ‰, AI, íŒŒì„œ ê²°ê³¼ë¥¼ ëª¨ë‘ ë³‘í•© (ìš°ì„ ìˆœìœ„: êµ¬ê¸€ > AI > íŒŒì„œ)"""
        merged = {
            'phone': '',
            'fax': '',
            'email': '',
            'address': '',
            'postal_code': ''
        }
        
        # í•„ë“œë³„ ë§¤í•‘
        field_mappings = {
            'phone': ['phones'],
            'fax': ['faxes'],
            'email': ['emails'],
            'address': ['addresses'],
            'postal_code': ['postal_codes']
        }
        
        for merged_field, source_fields in field_mappings.items():
            # 1ìˆœìœ„: êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼
            for source_field in source_fields:
                if google_result.get(source_field):
                    merged[merged_field] = google_result[source_field][0]
                    break
            
            # 2ìˆœìœ„: AI ê²°ê³¼ (êµ¬ê¸€ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°)
            if not merged[merged_field]:
                for source_field in source_fields:
                    if ai_result.get(source_field):
                        merged[merged_field] = ai_result[source_field][0]
                        break
            
            # 3ìˆœìœ„: íŒŒì„œ ê²°ê³¼ (ë‘˜ ë‹¤ ì—†ëŠ” ê²½ìš°)
            if not merged[merged_field]:
                for source_field in source_fields:
                    if parser_result.get(source_field):
                        merged[merged_field] = parser_result[source_field][0]
                        break
        
        return merged
    
    async def process_all_churches(self, churches_data: List[Dict]) -> List[Dict]:
        """ëª¨ë“  êµíšŒ/ê¸°ê´€/ê³µë¶€ë°©  ì²˜ë¦¬"""
        print(f"\nğŸš€ ì´ {len(churches_data)}ê°œ êµíšŒ/ê¸°ê´€/ê³µë¶€ë°©  ì²˜ë¦¬ ì‹œì‘")
        
        results = []
        
        for i, church in enumerate(churches_data):
            print(f"\nğŸ“ ì§„í–‰ìƒí™©: {i+1}/{len(churches_data)}")
            
            # êµíšŒ/ê¸°ê´€/ê³µë¶€ë°©  ì²˜ë¦¬
            result = await self.process_single_church(church)
            results.append(result)
            
            # ì¤‘ê°„ ì €ì¥ (50ê°œë§ˆë‹¤)
            if (i + 1) % 50 == 0:
                await self.save_intermediate_results(results, i + 1)
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            delay = random.uniform(*self.delay_range)
            await asyncio.sleep(delay)
        
        return results
    
    async def save_intermediate_results(self, results: List[Dict], count: int):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ì´ì „ íŒŒì¼ ìë™ ì‚­ì œ)"""
        try:
            # ì´ì „ ì¤‘ê°„ íŒŒì¼ ì‚­ì œ (í˜„ì¬ ì €ì¥í•  íŒŒì¼ ì œì™¸)
            self.cleanup_previous_intermediate_files(count)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"churches_enhanced_intermediate_{count}_{timestamp}.json"
            
            # ì‹¤ì œ ì €ì¥ë˜ëŠ” ë°ì´í„° ê°œìˆ˜ í™•ì¸
            actual_count = len(results)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {filename}")
            print(f"ğŸ“Š ì €ì¥ëœ ë°ì´í„°: {actual_count}ê°œ (ì²˜ë¦¬ ì§„í–‰ë¥ : {count}ê°œ)")
            
            # ë°ì´í„° ê°œìˆ˜ ë¶ˆì¼ì¹˜ ê²½ê³ 
            if actual_count != count:
                print(f"âš ï¸ ë°ì´í„° ê°œìˆ˜ ë¶ˆì¼ì¹˜! ì˜ˆìƒ: {count}ê°œ, ì‹¤ì œ: {actual_count}ê°œ")
            
        except Exception as e:
            print(f"âŒ ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")

    def cleanup_previous_intermediate_files(self, current_count: int):
        """ì´ì „ ì¤‘ê°„ íŒŒì¼ë“¤ ì‚­ì œ (í˜„ì¬ ì €ì¥í•  íŒŒì¼ ì œì™¸)"""
        try:
            # churches_enhanced_intermediate_*.json íŒ¨í„´ìœ¼ë¡œ íŒŒì¼ ì°¾ê¸°
            intermediate_files = glob.glob("churches_enhanced_intermediate_*.json")
            
            if not intermediate_files:
                return
            
            deleted_count = 0
            for file in intermediate_files:
                try:
                    # íŒŒì¼ëª…ì—ì„œ ì¹´ìš´íŠ¸ ì¶”ì¶œ
                    # íŒ¨í„´: churches_enhanced_intermediate_{count}_{timestamp}.json
                    parts = os.path.basename(file).split('_')
                    if len(parts) >= 4:
                        try:
                            file_count = int(parts[3])  # count ë¶€ë¶„ ì¶”ì¶œ
                            
                            # í˜„ì¬ ì €ì¥í•  íŒŒì¼ì˜ ì¹´ìš´íŠ¸ë³´ë‹¤ ì‘ì€ ê²½ìš°ì—ë§Œ ì‚­ì œ
                            if file_count < current_count:
                                # íŒŒì¼ í¬ê¸° í™•ì¸ (ë””ë²„ê¹…ìš©)
                                file_size = os.path.getsize(file)
                                print(f"ğŸ—‘ï¸ ì‚­ì œí•  íŒŒì¼: {file} (í¬ê¸°: {file_size:,} bytes)")
                                
                                os.remove(file)
                                deleted_count += 1
                        except ValueError:
                            print(f"âš ï¸ íŒŒì¼ëª…ì—ì„œ ì¹´ìš´íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {file}")
                            continue
                                
                except (OSError, IOError) as e:
                    print(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ({file}): {e}")
                    continue
            
            if deleted_count > 0:
                print(f"âœ… {deleted_count}ê°œ ì´ì „ ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ ì´ì „ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def save_final_results(self, results: List[Dict]) -> str:
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"churches_enhanced_final_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ìµœì¢… ê²°ê³¼ ì €ì¥: {filename}")
            
            # ì¤‘ê°„ íŒŒì¼ë“¤ ì •ë¦¬
            print("ğŸ§¹ ì¤‘ê°„ íŒŒì¼ ì •ë¦¬ ì¤‘...")
            self.cleanup_intermediate_files()
            
            return filename
            
        except Exception as e:
            print(f"âŒ ìµœì¢… ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def cleanup_intermediate_files(self):
        """ëª¨ë“  ì¤‘ê°„ íŒŒì¼ ì •ë¦¬"""
        try:
            intermediate_files = glob.glob("churches_enhanced_intermediate_*.json")
            
            if not intermediate_files:
                print("ğŸ“ ì •ë¦¬í•  ì¤‘ê°„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            deleted_count = 0
            total_size = 0
            
            for file in intermediate_files:
                try:
                    file_size = os.path.getsize(file)
                    total_size += file_size
                    print(f"ğŸ—‘ï¸ ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {file} (í¬ê¸°: {file_size:,} bytes)")
                    os.remove(file)
                    deleted_count += 1
                except OSError as e:
                    print(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({file}): {e}")
            
            if deleted_count > 0:
                print(f"âœ… {deleted_count}ê°œ ì¤‘ê°„ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ (ì´ {total_size:,} bytes ì ˆì•½)")
            
        except Exception as e:
            print(f"âŒ ì¤‘ê°„ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    def save_final_results(self, results: List[Dict]) -> str:
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"churches_enhanced_final_{timestamp}.json"
            
            # ì‹¤ì œ ì €ì¥í•  ë°ì´í„° ê°œìˆ˜ í™•ì¸
            actual_count = len(results)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(filename)
            
            print(f"âœ… ìµœì¢… ê²°ê³¼ ì €ì¥: {filename}")
            print(f"ğŸ“Š ì €ì¥ëœ ë°ì´í„°: {actual_count}ê°œ")
            print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
            
            # ì¤‘ê°„ íŒŒì¼ë“¤ ì •ë¦¬
            print("ğŸ§¹ ì¤‘ê°„ íŒŒì¼ ì •ë¦¬ ì¤‘...")
            self.cleanup_intermediate_files()
            
            return filename
            
        except Exception as e:
            print(f"âŒ ìµœì¢… ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
        
    def validate_final_data(self, results: List[Dict], expected_count: int):
        """ìµœì¢… ë°ì´í„° ê²€ì¦"""
        actual_count = len(results)
        
        print(f"\nğŸ” ë°ì´í„° ê²€ì¦:")
        print(f"  ì˜ˆìƒ ê°œìˆ˜: {expected_count}ê°œ")
        print(f"  ì‹¤ì œ ê°œìˆ˜: {actual_count}ê°œ")
        
        if actual_count != expected_count:
            print(f"âš ï¸ ë°ì´í„° ê°œìˆ˜ ë¶ˆì¼ì¹˜!")
            
            # ì¤‘ë³µ ë°ì´í„° í™•ì¸
            names = [item.get('name', '') for item in results]
            unique_names = set(names)
            duplicates = len(names) - len(unique_names)
            
            if duplicates > 0:
                print(f"ğŸ” ì¤‘ë³µëœ ê¸°ê´€ëª…: {duplicates}ê°œ")
                
                # ì¤‘ë³µ ì œê±°
                seen_names = set()
                unique_results = []
                
                for item in results:
                    name = item.get('name', '')
                    if name not in seen_names:
                        unique_results.append(item)
                        seen_names.add(name)
                    else:
                        print(f"  ì¤‘ë³µ ì œê±°: {name}")
                
                print(f"âœ… ì¤‘ë³µ ì œê±° í›„: {len(unique_results)}ê°œ")
                return unique_results
        
        print(f"âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
        return results

    def print_final_statistics(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ í†µê³„:")
        print(f"  ğŸ“‹ ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"  âœ… ì„±ê³µ: {self.stats['successful_crawls']}ê°œ")
        print(f"  âŒ ì‹¤íŒ¨: {self.stats['failed_crawls']}ê°œ")
        print(f"  ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ìˆ˜í–‰: {self.stats['google_searches_performed']}ê°œ")
        print(f"  ğŸ“ êµ¬ê¸€ì—ì„œ ì—°ë½ì²˜ ë°œê²¬: {self.stats['google_contacts_found']}ê°œ")
        print(f"  ğŸ¤– AI í˜¸ì¶œ: {self.stats['api_calls_made']}íšŒ")
        print(f"  ğŸ¯ AI ì„±ê³µ: {self.stats['ai_enhanced']}ê°œ")
        print(f"  âš ï¸ AI ì‹¤íŒ¨: {self.stats['ai_failures']}ê°œ")
        print(f"  ğŸ“ ì´ ì—°ë½ì²˜ ë°œê²¬: {self.stats['contacts_found']}ê°œ")
        
        if self.stats['total_processed'] > 0:
            success_rate = (self.stats['successful_crawls'] / self.stats['total_processed']) * 100
            print(f"  ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        if self.stats['api_calls_made'] > 0:
            ai_success_rate = (self.stats['ai_enhanced'] / self.stats['api_calls_made']) * 100
            print(f"  ğŸ¤– AI ì„±ê³µë¥ : {ai_success_rate:.1f}%")
        
        if self.stats['google_searches_performed'] > 0:
            google_success_rate = (self.stats['google_contacts_found'] / self.stats['google_searches_performed']) * 100
            print(f"  ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ì„±ê³µë¥ : {google_success_rate:.1f}%")

class GoogleContactSearcher:
    """êµ¬ê¸€ ê²€ìƒ‰ì„ í†µí•œ ì—°ë½ì²˜ ì •ë³´ ì§ì ‘ ê²€ìƒ‰"""
    
    def __init__(self):
        try:
            self.session = requests.Session()
            # ë” ë‹¤ì–‘í•œ User-Agent ë¡œí…Œì´ì…˜
            self.user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
            ]
            self.current_ua_index = 0
            
            # ê²€ìƒ‰ í†µê³„
            self.search_stats = {
                'google_searches': 0,
                'successful_extractions': 0,
                'blocked_attempts': 0
            }
            
            # ì„¸ì…˜ ì„¤ì •
            self.setup_session()
            
            print("ğŸ” GoogleContactSearcher ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ GoogleContactSearcher ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def setup_session(self):
        """ì„¸ì…˜ ì„¤ì • ê°•í™”"""
        try:
            # User-Agent ë¡œí…Œì´ì…˜
            ua = self.user_agents[self.current_ua_index % len(self.user_agents)]
            self.current_ua_index += 1
            
            self.session.headers.clear()
            self.session.headers.update({
                'User-Agent': ua,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"Windows"'
            })
            
        except Exception as e:
            print(f"âŒ ì„¸ì…˜ ì„¤ì • ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ í—¤ë”ë¼ë„ ì„¤ì •
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
    
    def search_google_with_retry(self, query: str, max_retries: int = 3) -> Optional[str]:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ êµ¬ê¸€ ê²€ìƒ‰"""
        for attempt in range(max_retries):
            try:
                # User-Agent ë³€ê²½
                if attempt > 0:
                    self.setup_session()
                
                # êµ¬ê¸€ ê²€ìƒ‰ URL
                search_url = f"https://www.google.com/search?q={requests.utils.quote(query)}&hl=ko&num=10"
                
                print(f"  ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ì‹œë„ {attempt + 1}/{max_retries}: {query}")
                
                response = self.session.get(
                    search_url,
                    timeout=15,
                    verify=False
                )
                
                self.search_stats['google_searches'] += 1
                
                if response.status_code == 200:
                    # ì°¨ë‹¨ ê°ì§€
                    if self.is_blocked_response(response.text):
                        print(f"  âš ï¸ êµ¬ê¸€ ì°¨ë‹¨ ê°ì§€ (ì‹œë„ {attempt + 1})")
                        self.search_stats['blocked_attempts'] += 1
                        
                        if attempt < max_retries - 1:
                            delay = random.uniform(10, 20)  # ê¸´ ëŒ€ê¸°
                            print(f"  â³ {delay:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                            time.sleep(delay)
                            continue
                        else:
                            return None
                    
                    return response.text
                else:
                    print(f"  âŒ HTTP ì˜¤ë¥˜ {response.status_code}")
                    
            except Exception as e:
                print(f"  âŒ ê²€ìƒ‰ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                
            # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            if attempt < max_retries - 1:
                delay = random.uniform(5, 10)
                time.sleep(delay)
        
        return None
    
    def is_blocked_response(self, html: str) -> bool:
        """êµ¬ê¸€ ì°¨ë‹¨ ì‘ë‹µ ê°ì§€"""
        if not html:
            return True
            
        block_indicators = [
            'unusual traffic',
            'automated queries',
            'captcha',
            'blocked',
            'suspicious activity',
            'Our systems have detected',
            'ë¹„ì •ìƒì ì¸ íŠ¸ë˜í”½'
        ]
        
        html_lower = html.lower()
        return any(indicator in html_lower for indicator in block_indicators)
    
    
    def extract_contacts_from_search_results(self, html: str, organization_name: str) -> Dict[str, List]:
        """êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
            all_text = soup.get_text()
            
            contacts = {
                'phones': [],
                'faxes': [],
                'emails': [],
                'addresses': []
            }
            
            # ì „í™”ë²ˆí˜¸ íŒ¨í„´ (í•œêµ­)
            phone_patterns = [
                r'\b0\d{1,2}-\d{3,4}-\d{4}\b',      # 02-1234-5678
                r'\b\d{3}-\d{3,4}-\d{4}\b',         # 031-123-4567
                r'\b010-\d{4}-\d{4}\b',             # 010-1234-5678
                r'\b0\d{1,2}\.\d{3,4}\.\d{4}\b',    # 02.1234.5678
                r'\b\d{3}\.\d{3,4}\.\d{4}\b'        # 031.123.4567
            ]
            
            # íŒ©ìŠ¤ íŒ¨í„´ (FAX, íŒ©ìŠ¤ í‚¤ì›Œë“œ í¬í•¨)
            fax_patterns = [
                r'(?:fax|íŒ©ìŠ¤|íŒ©ì‹œë°€ë¦¬)[:ï¼š\s]*(\d{2,3}[-.\s]?\d{3,4}[-.\s]?\d{4})',
                r'(?:F|f)[:ï¼š\s]*(\d{2,3}[-.\s]?\d{3,4}[-.\s]?\d{4})'
            ]
            
            # ì´ë©”ì¼ íŒ¨í„´
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            
            # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in phone_patterns:
                matches = re.findall(pattern, all_text)
                for match in matches:
                    # ì •ê·œí™”
                    clean_phone = re.sub(r'[^\d-]', '', match)
                    if len(clean_phone) >= 9 and clean_phone not in contacts['phones']:
                        contacts['phones'].append(clean_phone)
            
            # íŒ©ìŠ¤ ì¶”ì¶œ
            for pattern in fax_patterns:
                matches = re.findall(pattern, all_text, re.IGNORECASE)
                for match in matches:
                    clean_fax = re.sub(r'[^\d-]', '', match)
                    if len(clean_fax) >= 9 and clean_fax not in contacts['faxes']:
                        contacts['faxes'].append(clean_fax)
            
            # ì´ë©”ì¼ ì¶”ì¶œ
            email_matches = re.findall(email_pattern, all_text)
            for email in email_matches:
                if email not in contacts['emails']:
                    contacts['emails'].append(email)
            
            # ê¸°ê´€ëª…ê³¼ ì—°ê´€ì„± í•„í„°ë§
            filtered_contacts = self.filter_relevant_contacts(contacts, organization_name, all_text)
            
            if any(len(v) > 0 for v in filtered_contacts.values()):
                self.search_stats['successful_extractions'] += 1
            
            return filtered_contacts
            
        except Exception as e:
            print(f"  âŒ ì—°ë½ì²˜ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {'phones': [], 'faxes': [], 'emails': [], 'addresses': []}
    
    def filter_relevant_contacts(self, contacts: Dict[str, List], org_name: str, context: str) -> Dict[str, List]:
        """ê¸°ê´€ëª…ê³¼ ì—°ê´€ì„±ì´ ë†’ì€ ì—°ë½ì²˜ë§Œ í•„í„°ë§"""
        try:
            filtered = {key: [] for key in contacts.keys()}
            
            # ê¸°ê´€ëª… í‚¤ì›Œë“œ ì¶”ì¶œ
            org_keywords = re.findall(r'[ê°€-í£a-zA-Z]+', org_name.lower())
            
            for contact_type, contact_list in contacts.items():
                for contact in contact_list:
                    # ì—°ë½ì²˜ ì£¼ë³€ í…ìŠ¤íŠ¸ í™•ì¸
                    contact_context = self.get_contact_context(contact, context, 100)
                    
                    # ê¸°ê´€ëª…ì´ ì—°ë½ì²˜ ê·¼ì²˜ì— ìˆëŠ”ì§€ í™•ì¸
                    relevance_score = 0
                    for keyword in org_keywords:
                        if len(keyword) > 2 and keyword in contact_context.lower():
                            relevance_score += 1
                    
                    # ì—°ê´€ì„±ì´ ìˆê±°ë‚˜ ì—°ë½ì²˜ê°€ ì ì€ ê²½ìš° í¬í•¨
                    if relevance_score > 0 or len(contact_list) <= 2:
                        filtered[contact_type].append(contact)
            
            return filtered
        except Exception as e:
            print(f"  âŒ ì—°ë½ì²˜ í•„í„°ë§ ì˜¤ë¥˜: {e}")
            return contacts  # í•„í„°ë§ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    
    def get_contact_context(self, contact: str, full_text: str, window_size: int = 100) -> str:
        """ì—°ë½ì²˜ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            index = full_text.find(contact)
            if index == -1:
                return ""
            
            start = max(0, index - window_size)
            end = min(len(full_text), index + len(contact) + window_size)
            
            return full_text[start:end]
        except:
            return ""
    
    async def search_organization_contacts(self, organization_name: str) -> Dict[str, List]:
        """ê¸°ê´€ëª…ìœ¼ë¡œ ì—°ë½ì²˜ ì •ë³´ ê²€ìƒ‰"""
        print(f"  ğŸ“ êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ì—°ë½ì²˜ ì°¾ê¸°: {organization_name}")
        
        all_contacts = {
            'phones': [],
            'faxes': [],
            'emails': [],
            'addresses': []
        }
        
        try:
            # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬
            search_queries = [
                f'"{organization_name}" ì „í™”ë²ˆí˜¸',
                f'"{organization_name}" ì—°ë½ì²˜',
                f'"{organization_name}" ëŒ€í‘œë²ˆí˜¸',
                f'"{organization_name}" íŒ©ìŠ¤ë²ˆí˜¸',
                f'"{organization_name}" ì´ë©”ì¼',
                f'{organization_name} tel phone',
                f'{organization_name} contact'
            ]
            
            # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
            for i, query in enumerate(search_queries):
                try:
                    print(f"  ğŸ” ì¿¼ë¦¬ {i+1}/{len(search_queries)}: {query}")
                    
                    # êµ¬ê¸€ ê²€ìƒ‰
                    html_content = self.search_google_with_retry(query)
                    
                    if html_content:
                        # ì—°ë½ì²˜ ì¶”ì¶œ
                        extracted = self.extract_contacts_from_search_results(html_content, organization_name)
                        
                        # ê²°ê³¼ ë³‘í•©
                        for contact_type, contact_list in extracted.items():
                            for contact in contact_list:
                                if contact not in all_contacts[contact_type]:
                                    all_contacts[contact_type].append(contact)
                        
                        # ì—°ë½ì²˜ë¥¼ ì¶©ë¶„íˆ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                        total_found = sum(len(v) for v in all_contacts.values())
                        if total_found >= 5:  # ì¶©ë¶„í•œ ì—°ë½ì²˜ ë°œê²¬
                            print(f"  âœ… ì¶©ë¶„í•œ ì—°ë½ì²˜ ë°œê²¬ ({total_found}ê°œ), ê²€ìƒ‰ ì¤‘ë‹¨")
                            break
                    
                    # ê²€ìƒ‰ ê°„ê²©
                    delay = random.uniform(2, 5)
                    await asyncio.sleep(delay)
                    
                except Exception as e:
                    print(f"  âŒ ì¿¼ë¦¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    continue
            
            total_found = sum(len(v) for v in all_contacts.values())
            print(f"  ğŸ“Š ì´ {total_found}ê°œ ì—°ë½ì²˜ ë°œê²¬: "
                  f"ì „í™” {len(all_contacts['phones'])}ê°œ, "
                  f"íŒ©ìŠ¤ {len(all_contacts['faxes'])}ê°œ, "
                  f"ì´ë©”ì¼ {len(all_contacts['emails'])}ê°œ")
        
        except Exception as e:
            print(f"  âŒ ì „ì²´ ê²€ìƒ‰ ê³¼ì •ì—ì„œ ì˜¤ë¥˜: {e}")
        
        return all_contacts
    
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸš€ ê³ ê¸‰ êµíšŒ/ê¸°ê´€/ê³µë¶€ë°©  ì—°ë½ì²˜ í¬ë¡¤ëŸ¬ v2.0")
    print("=" * 60)
    
    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    api_key = os.getenv('GEMINI_API_KEY')
    if api_key:
        print(f"ğŸ”‘ API í‚¤ í™•ì¸: ...{api_key[-10:]}")
    else:
        print("âš ï¸ GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ”§ AI ê¸°ëŠ¥ ì—†ì´ ê¸°ë³¸ í¬ë¡¤ë§ë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = AdvancedChurchCrawler()
    
    # ì…ë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ì ì§€ì •)
    input_file = r"C:\Users\kimyh\makedb\Python\cradcrawl_adv\raw_data_with_homepages_20250609_134906.json"
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(input_file):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ëŒ€ì²´ íŒŒì¼ ì°¾ê¸°
        alternative_files = glob.glob("raw_data_with_homepages_*.json")
        if alternative_files:
            latest_file = max(alternative_files, key=os.path.getctime)
            print(f"ğŸ” ëŒ€ì‹  ì‚¬ìš©í•  íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {latest_file}")
            input_file = latest_file
        else:
            print("âŒ ëŒ€ì²´ íŒŒì¼ë„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
    
    # JSON íŒŒì¼ ë¡œë“œ
    churches_data = crawler.load_json_data(input_file)
    
    if not churches_data:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {input_file}")
    print(f"ğŸ“Š ì²˜ë¦¬í•  êµíšŒ/ê¸°ê´€/ê³µë¶€ë°©  ìˆ˜: {len(churches_data)}")
    
    # ì›ë³¸ ë°ì´í„° ê°œìˆ˜ ì €ì¥
    original_count = len(churches_data)
    
    # ì²˜ë¦¬í•  ê°œìˆ˜ ì œí•œ ì˜µì…˜ ì¶”ê°€
    max_process = input(f"ì²˜ë¦¬í•  êµíšŒ/ê¸°ê´€/ê³µë¶€ë°©  ìˆ˜ (ì „ì²´: {len(churches_data)}ê°œ, ì—”í„°=ì „ì²´): ").strip()
    
    if max_process and max_process.isdigit():
        max_process = int(max_process)
        churches_data = churches_data[:max_process]
        print(f"ğŸ“Š ì‹¤ì œ ì²˜ë¦¬í•  êµíšŒ/ê¸°ê´€/ê³µë¶€ë°©  ìˆ˜: {len(churches_data)}")
        original_count = len(churches_data)  # ì‹¤ì œ ì²˜ë¦¬í•  ê°œìˆ˜ë¡œ ì—…ë°ì´íŠ¸
    
    # ì‚¬ìš©ì í™•ì¸
    print(f"\nâš ï¸ {len(churches_data)}ê°œ êµíšŒ/ê¸°ê´€/ê³µë¶€ë°© ì˜ í™ˆí˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
    print("ì´ ì‘ì—…ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    proceed = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if proceed not in ['y', 'yes']:
        print("âŒ ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    try:
        # ëª¨ë“  êµíšŒ/ê¸°ê´€/ê³µë¶€ë°©  ì²˜ë¦¬
        enhanced_results = await crawler.process_all_churches(churches_data)
        
        # ë°ì´í„° ê²€ì¦
        validated_results = crawler.validate_final_data(enhanced_results, original_count)
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        output_file = crawler.save_final_results(validated_results)
        
        # í†µê³„ ì¶œë ¥
        crawler.print_final_statistics()
        
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
        print(f"ğŸ“Š ìµœì¢… ë°ì´í„° ê°œìˆ˜: {len(validated_results)}ê°œ")
        
        # Excel ë³€í™˜ ì˜µì…˜
        excel_convert = input("\nExcel íŒŒì¼ë¡œ ë³€í™˜í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if excel_convert in ['y', 'yes']:
            print("ğŸ“Š Excel ë³€í™˜ì„ ìœ„í•´ jsontoexcel.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            print(f"ğŸ’¡ ëª…ë ¹ì–´: python jsontoexcel.py")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main())