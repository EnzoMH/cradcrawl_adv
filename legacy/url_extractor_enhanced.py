#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
URL ì¶”ì¶œ ë° í™ˆí˜ì´ì§€ ì°¾ê¸° ë„êµ¬ (Selenium ê¸°ë°˜ + VPN ìš°íšŒ)
êµ¬ê¸€ì˜ ë§¤í¬ë¡œ ì°¨ë‹¨ì„ VPN Gateë¥¼ í†µí•´ ìš°íšŒí•˜ëŠ” ê¸°ëŠ¥ í¬í•¨
"""

import json
import os
import sys
import time
import random
import re
import logging
import requests
import base64
import tempfile
import subprocess
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import quote_plus, urljoin, urlparse, unquote
from datetime import datetime

# Selenium imports
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# ìƒìœ„ ë””ë ‰í† ë¦¬ ëª¨ë“ˆ import (ê²½ë¡œ ì„¤ì • í›„)
try:
    from validator import AIValidator
    AI_VALIDATOR_AVAILABLE = True
    print("âœ… AIValidator import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ AIValidator import ì‹¤íŒ¨: {e}")
    print("ğŸ”§ AIValidator ì—†ì´ ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    AIValidator = None
    AI_VALIDATOR_AVAILABLE = False

# ê°•í™”ëœ ìƒìˆ˜ ì„¤ì •
EXCLUDE_DOMAINS = [
    'youtube.com', 'facebook.com', 'instagram.com', 'twitter.com',
    'naver.com', 'daum.net', 'google.com', 'yahoo.com',
    'blog.naver.com', 'cafe.naver.com', 'tistory.com',
    'wikipedia.org', 'namu.wiki', 'blogspot.com', 'wordpress.com'
]

class VPNManager:
    """VPN ì—°ê²° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.current_process = None
        self.current_config = None
        self.logger = logging.getLogger('VPNManager')
    
    def get_vpn_servers(self, country: str) -> List[Dict]:
        """VPN Gateì—ì„œ ì„œë²„ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            self.logger.info(f"VPN ì„œë²„ ê²€ìƒ‰ ì¤‘: {country}")
            vpn_data = requests.get("http://www.vpngate.net/api/iphone/", timeout=10).text.replace("\r", "")
            servers = [line.split(",") for line in vpn_data.split("\n")]
            
            if len(servers) < 3:
                return []
                
            labels = servers[1]
            labels[0] = labels[0][1:]
            servers = [s for s in servers[2:] if len(s) > 1]
            
            # êµ­ê°€ ë§¤ì¹­
            if len(country) == 2:
                i = 6  # short name for country
            else:
                i = 5  # long name for country
            
            desired = [s for s in servers if len(s) > i and country.lower() in s[i].lower()]
            supported = [s for s in desired if len(s) > 14 and len(s[-1]) > 0]
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_servers = sorted(supported, key=lambda s: float(s[2].replace(",", ".")), reverse=True)
            
            server_list = []
            for server in sorted_servers[:3]:  # ìƒìœ„ 3ê°œë§Œ
                server_info = {
                    'config': server[-1],
                    'score': float(server[2].replace(",", ".")),
                    'country': server[5] if len(server) > 5 else country,
                }
                server_list.append(server_info)
            
            self.logger.info(f"{country}ì—ì„œ {len(server_list)}ê°œ VPN ì„œë²„ ë°œê²¬")
            return server_list
            
        except Exception as e:
            self.logger.error(f"VPN ì„œë²„ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    def connect_vpn(self, country: str) -> bool:
        """VPN ì—°ê²°"""
        try:
            servers = self.get_vpn_servers(country)
            if not servers:
                return False
            
            # ê¸°ì¡´ ì—°ê²° ì¢…ë£Œ
            self.disconnect_vpn()
            
            # ê°€ì¥ ì¢‹ì€ ì„œë²„ ì„ íƒ
            server = servers[0]
            self.logger.info(f"VPN ì—°ê²° ì‹œë„: {country} (ì ìˆ˜: {server['score']})")
            
            # ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±
            _, config_path = tempfile.mkstemp(suffix='.ovpn')
            self.current_config = config_path
            
            with open(config_path, 'w') as f:
                config_content = base64.b64decode(server['config']).decode()
                f.write(config_content)
            
            # OpenVPN ì—°ê²° (ë°±ê·¸ë¼ìš´ë“œ)
            try:
                self.current_process = subprocess.Popen(
                    ["openvpn", "--config", config_path],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                )
                
                # ì—°ê²° ëŒ€ê¸° (ìµœëŒ€ 15ì´ˆ)
                for i in range(15):
                    if self.current_process.poll() is None:
                        time.sleep(1)
                        if i > 5:  # 5ì´ˆ í›„ë¶€í„°ëŠ” ì—°ê²°ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                            self.logger.info(f"VPN ì—°ê²° ì„±ê³µ: {country}")
                            return True
                    else:
                        break
                
                self.logger.warning(f"VPN ì—°ê²° ì‹¤íŒ¨: {country}")
                return False
                
            except FileNotFoundError:
                self.logger.error("OpenVPNì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. VPN ìš°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return False
            except Exception as e:
                self.logger.error(f"VPN ì—°ê²° ì¤‘ ì˜¤ë¥˜: {e}")
                return False
                
        except Exception as e:
            self.logger.error(f"VPN ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def disconnect_vpn(self):
        """VPN ì—°ê²° ì¢…ë£Œ"""
        try:
            if self.current_process:
                self.current_process.terminate()
                for _ in range(3):
                    if self.current_process.poll() is not None:
                        break
                    time.sleep(1)
                else:
                    self.current_process.kill()
                
                self.current_process = None
                self.logger.info("VPN ì—°ê²° ì¢…ë£Œ")
            
            if self.current_config and os.path.exists(self.current_config):
                os.unlink(self.current_config)
                self.current_config = None
                
        except Exception as e:
            self.logger.error(f"VPN ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

class URLExtractorEnhanced:
    """í–¥ìƒëœ URL ì¶”ì¶œê¸° (VPN ìš°íšŒ í¬í•¨)"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.driver = None
        self.logger = self.setup_logger()
        self.vpn_manager = VPNManager()
        
        # AI ê²€ì¦ê¸° ì´ˆê¸°í™” (ìƒˆë¡œ ì¶”ê°€)
        self.ai_validator = None
        self.use_ai_validation = False
        
        if AI_VALIDATOR_AVAILABLE:
            try:
                self.ai_validator = AIValidator()
                self.use_ai_validation = True
                print("ğŸ¤– Enhanced URL Extractor: AI ê²€ì¦ ê¸°ëŠ¥ í™œì„±í™”")
            except Exception as e:
                print(f"âŒ AI ê²€ì¦ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_ai_validation = False
        else:
            print("ğŸ”§ Enhanced URL Extractor: AI ê²€ì¦ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
        
        # ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
        self.delay_range = (2, 4)
        
        # ê²€ìƒ‰ ê²°ê³¼ ì œí•œ
        self.max_search_results = 5
        self.max_retries = 2
        
        # VPN ê´€ë ¨
        self.vpn_countries = ['japan', 'korea', 'singapore']
        self.current_vpn_index = 0
    
    def setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('URLExtractorEnhanced')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def setup_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        try:
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument('--headless')
            
            # ê¸°ë³¸ ì˜µì…˜ë“¤
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            
            # ìë™í™” íƒì§€ ìš°íšŒ
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.logger.info(f"Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    def close_driver(self):
        """ë“œë¼ì´ë²„ ë° VPN ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
        
        self.vpn_manager.disconnect_vpn()
    
    def is_google_blocked(self) -> bool:
        """êµ¬ê¸€ ì°¨ë‹¨ ê°ì§€"""
        try:
            page_source = self.driver.page_source.lower()
            title = self.driver.title.lower()
            
            # ì°¨ë‹¨ í‚¤ì›Œë“œ
            block_keywords = [
                'unusual traffic', 'captcha', 'blocked', 'forbidden',
                'access denied', 'suspicious activity', 'automated queries',
                'robot', 'bot detected', 'ë¹„ì •ìƒì ì¸ íŠ¸ë˜í”½'
            ]
            
            for keyword in block_keywords:
                if keyword in page_source or keyword in title:
                    self.logger.warning(f"êµ¬ê¸€ ì°¨ë‹¨ ê°ì§€: {keyword}")
                    return True
            
            # CAPTCHA í™•ì¸
            try:
                if self.driver.find_elements(By.CSS_SELECTOR, '#captcha, .captcha, .g-recaptcha'):
                    self.logger.warning("CAPTCHA ê°ì§€ë¨")
                    return True
            except:
                pass
            
            return False
            
        except Exception:
            return False
    
    def try_vpn_bypass(self) -> bool:
        """VPN ìš°íšŒ ì‹œë„"""
        try:
            self.logger.info("ğŸ”§ VPN ìš°íšŒ ì‹œë„ ì¤‘...")
            
            for country in self.vpn_countries:
                self.logger.info(f"ğŸŒ VPN ì—°ê²° ì‹œë„: {country}")
                
                if self.vpn_manager.connect_vpn(country):
                    time.sleep(3)
                    
                    # ë“œë¼ì´ë²„ ì¬ì‹œì‘
                    if self.driver:
                        self.driver.quit()
                    time.sleep(2)
                    self.setup_driver()
                    
                    # êµ¬ê¸€ ì ‘ì† í…ŒìŠ¤íŠ¸
                    try:
                        self.driver.get("https://www.google.com")
                        time.sleep(2)
                        
                        if not self.is_google_blocked():
                            self.logger.info(f"âœ… VPN ìš°íšŒ ì„±ê³µ: {country}")
                            return True
                        else:
                            self.logger.warning(f"âŒ {country} VPNìœ¼ë¡œë„ ì°¨ë‹¨ë¨")
                            
                    except Exception as e:
                        self.logger.warning(f"VPN í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            self.logger.error("ëª¨ë“  VPN ì‹œë„ ì‹¤íŒ¨")
            return False
            
        except Exception as e:
            self.logger.error(f"VPN ìš°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def add_delay(self):
        """ìš”ì²­ ê°„ ì§€ì—°"""
        delay = random.uniform(*self.delay_range)
        time.sleep(delay)

    def search_organization_homepage(self, organization_name: str) -> Optional[str]:
        """ê¸°ê´€ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (AI ê²€ì¦ + VPN ìš°íšŒ)"""
        try:
            if not self.driver:
                self.setup_driver()
            
            all_urls = []
            
            # 1. ë„¤ì´ë²„ ê²€ìƒ‰
            self.logger.info(f"ğŸ” ë„¤ì´ë²„ ê²€ìƒ‰: {organization_name}")
            naver_urls = self._search_naver(organization_name)
            if naver_urls:
                all_urls.extend([(url, 'naver') for url in naver_urls])
            
            self.add_delay()
            
            # 2. êµ¬ê¸€ ê²€ìƒ‰ (VPN ìš°íšŒ í¬í•¨)
            self.logger.info(f"ğŸ” êµ¬ê¸€ ê²€ìƒ‰: {organization_name}")
            google_urls = self._search_google_with_vpn(organization_name)
            if google_urls:
                all_urls.extend([(url, 'google') for url in google_urls])
            
            # 3. AI ê²€ì¦ìœ¼ë¡œ ìµœì  URL ì„ íƒ
            if all_urls:
                if self.use_ai_validation:
                    best_url = self._select_best_url_with_ai(all_urls, organization_name)
                else:
                    # AI ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹
                    unique_urls = list(set([url for url, _ in all_urls]))
                    best_url = self.select_best_homepage(unique_urls, organization_name)
                
                return best_url
            
            self.logger.warning(f"âŒ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {organization_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
        
    def _select_best_url_with_ai(self, candidate_urls: List[Tuple[str, str]], 
                                 organization_name: str) -> Optional[str]:
        """AI ê²€ì¦ìœ¼ë¡œ ìµœì  URL ì„ íƒ (asyncio ë¬¸ì œ ìˆ˜ì •)"""
        
        # asyncio.run() ëŒ€ì‹  ì§ì ‘ await ì‚¬ìš© (ì´ë¯¸ async ì»¨í…ìŠ¤íŠ¸ì—ì„œ í˜¸ì¶œë˜ë¯€ë¡œ)
        async def validate_candidates():
            validated_results = []
            
            for url, source in candidate_urls:
                try:
                    # í˜ì´ì§€ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                    page_preview = self._get_quick_page_content(url)
                    
                    # AI ê²€ì¦
                    validation = await self.ai_validator.validate_homepage_url_relevance(
                        organization_name, url, page_preview, source
                    )
                    
                    score = validation.get("confidence", 0) * 100
                    if validation.get("is_relevant", False):
                        score += 20  # ê´€ë ¨ì„± ë³´ë„ˆìŠ¤
                    
                    validated_results.append({
                        'url': url,
                        'score': score,
                        'confidence': validation.get("confidence", 0),
                        'source': source,
                        'reasoning': validation.get("reasoning", "")
                    })
                    
                    self.logger.info(f"ê²€ì¦ ì™„ë£Œ: {url} (ì ìˆ˜: {score:.1f}, ì¶œì²˜: {source})")
                    
                except Exception as e:
                    self.logger.warning(f"URL ê²€ì¦ ì‹¤íŒ¨: {url} - {e}")
                    continue
            
            return validated_results
        
        try:
            # asyncio.run() ì œê±°í•˜ê³  ì§ì ‘ í˜¸ì¶œ
            import asyncio
            
            # í˜„ì¬ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
            try:
                loop = asyncio.get_running_loop()
                # ì´ë¯¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ taskë¡œ ìƒì„±
                task = asyncio.create_task(validate_candidates())
                validated_results = loop.run_until_complete(task)
            except RuntimeError:
                # ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ì‹¤í–‰
                validated_results = asyncio.run(validate_candidates())
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            if validated_results:
                validated_results.sort(key=lambda x: x['score'], reverse=True)
                best_result = validated_results[0]
                
                self.logger.info(f"ğŸ† ìµœì¢… ì„ íƒ: {best_result['url']} "
                               f"(ì ìˆ˜: {best_result['score']:.1f}, ì‹ ë¢°ë„: {best_result['confidence']:.2f})")
                
                return best_result['url']
            
            # AI ê²€ì¦ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ì‹
            self.logger.warning("AI ê²€ì¦ ê²°ê³¼ ì—†ìŒ, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©")
            unique_urls = list(set([url for url, _ in candidate_urls]))
            return self.select_best_homepage(unique_urls, organization_name)
            
        except Exception as e:
            self.logger.error(f"AI ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            # fallback ë¡œì§
            unique_urls = list(set([url for url, _ in candidate_urls]))
            return self.select_best_homepage(unique_urls, organization_name)
    
    def _get_quick_page_content(self, url: str) -> str:
        """ë¹ ë¥¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ requestsë¡œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸°ìš©)
            import requests
            response = requests.get(url, timeout=5, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            if response.status_code == 200:
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(response.text, 'html.parser')
                    # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
                    for script in soup(["script", "style"]):
                        script.decompose()
                    text = soup.get_text()[:2000]  # ì•ë¶€ë¶„ë§Œ
                    return text
                except ImportError:
                    # BeautifulSoup ì—†ëŠ” ê²½ìš° ì›ë³¸ í…ìŠ¤íŠ¸ ì¼ë¶€ ì‚¬ìš©
                    return response.text[:2000]
        
        except Exception as e:
            self.logger.warning(f"í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
        
        return ""

    def _search_naver(self, organization_name: str) -> List[str]:
        """ë„¤ì´ë²„ ê²€ìƒ‰"""
        try:
            self.driver.get('https://www.naver.com')
            time.sleep(2)
            
            search_box = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "query"))
            )
            
            query = f'{organization_name} í™ˆí˜ì´ì§€'
            search_box.clear()
            search_box.send_keys(query)
            search_box.send_keys(Keys.RETURN)
            time.sleep(3)
            
            found_urls = []
            selectors = [
                'h3.title a[href*="http"]',
                '.total_wrap a[href*="http"]',
                '.result_area a[href*="http"]',
            ]
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements[:10]:
                        try:
                            href = element.get_attribute('href')
                            if href and 'naver.com' not in href:
                                if self.is_valid_homepage_url(href, organization_name):
                                    found_urls.append(href)
                        except:
                            continue
                    if found_urls:
                        break
                except:
                    continue
            
            return found_urls[:self.max_search_results]
            
        except Exception as e:
            self.logger.warning(f"ë„¤ì´ë²„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _search_google_with_vpn(self, organization_name: str) -> List[str]:
        """êµ¬ê¸€ ê²€ìƒ‰ (VPN ìš°íšŒ í¬í•¨)"""
        for attempt in range(self.max_retries):
            try:
                query = f'{organization_name} í™ˆí˜ì´ì§€'
                google_url = f"https://www.google.com/search?q={quote_plus(query)}&hl=ko"
                self.driver.get(google_url)
                time.sleep(3)
                
                # ì°¨ë‹¨ í™•ì¸
                if self.is_google_blocked():
                    self.logger.warning(f"êµ¬ê¸€ ì°¨ë‹¨ ê°ì§€ (ì‹œë„ {attempt + 1})")
                    
                    if attempt < self.max_retries - 1:
                        if self.try_vpn_bypass():
                            continue
                        else:
                            break
                    else:
                        self.logger.error("VPN ìš°íšŒ ì‹¤íŒ¨")
                        break
                
                # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
                found_urls = []
                selectors = [
                    'h3 a[href*="http"]',
                    '.g a[href*="http"]',
                    '.yuRUbf a[href*="http"]',
                ]
                
                for selector in selectors:
                    try:
                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        for element in elements[:10]:
                            try:
                                href = element.get_attribute('href')
                                if not href or 'google.com' in href:
                                    continue
                                
                                # êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬
                                if '/url?q=' in href:
                                    actual_url = self._extract_google_url(href)
                                    if actual_url:
                                        href = actual_url
                                
                                if self.is_valid_homepage_url(href, organization_name):
                                    found_urls.append(href)
                                    
                            except:
                                continue
                        if found_urls:
                            break
                    except:
                        continue
                
                if found_urls:
                    return found_urls[:self.max_search_results]
                else:
                    self.logger.warning(f"êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ì‹œë„ {attempt + 1})")
                    
            except Exception as e:
                self.logger.warning(f"êµ¬ê¸€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2)
        
        return []

    def _extract_google_url(self, google_url: str) -> Optional[str]:
        """êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì¶”ì¶œ"""
        try:
            if '/url?q=' in google_url:
                parts = google_url.split('/url?q=')
                if len(parts) > 1:
                    url_part = parts[1].split('&')[0]
                    return unquote(url_part)
            return None
        except:
            return None

    def is_valid_homepage_url(self, url: str, organization_name: str) -> bool:
        """ìœ íš¨í•œ í™ˆí˜ì´ì§€ URL í™•ì¸"""
        try:
            if not url or len(url) < 10:
                return False
            
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return False
            
            domain = parsed.netloc.lower()
            
            # ì œì™¸ ë„ë©”ì¸ ì²´í¬
            for exclude_domain in EXCLUDE_DOMAINS:
                if exclude_domain in domain:
                    return False
            
            # íŒŒì¼ í™•ì¥ì ì²´í¬
            if any(url.lower().endswith(ext) for ext in ['.pdf', '.jpg', '.png', '.gif']):
                return False
            
            # ê²Œì‹œíŒ/ë¸”ë¡œê·¸ íŒ¨í„´ ì²´í¬
            if any(pattern in url.lower() for pattern in ['/board/', '/bbs/', 'blog.', 'cafe.']):
                return False
            
            # ê³µì‹ ë„ë©”ì¸ ìš°ì„ 
            official_tlds = ['.or.kr', '.go.kr', '.ac.kr', '.org', '.church']
            if any(tld in domain for tld in official_tlds):
                return True
            
            # ê¸°ê´€ëª… ë§¤ì¹­
            name_parts = re.findall(r'[ê°€-í£a-zA-Z]+', organization_name.lower())
            for part in name_parts:
                if len(part) > 2 and part in domain:
                    return True
            
            return len(domain) < 50  # ë„ˆë¬´ ê¸´ ë„ë©”ì¸ ì œì™¸
            
        except:
            return False

    def select_best_homepage(self, urls: List[str], organization_name: str) -> str:
        """ìµœì  í™ˆí˜ì´ì§€ URL ì„ íƒ"""
        if not urls:
            return ""
        
        if len(urls) == 1:
            return urls[0]
        
        scored_urls = []
        
        for url in urls:
            score = 0
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # ë„ë©”ì¸ ì‹ ë¢°ë„
            if '.or.kr' in domain:
                score += 20
            elif '.go.kr' in domain:
                score += 18
            elif '.ac.kr' in domain:
                score += 16
            elif '.church' in domain:
                score += 15
            elif '.org' in domain:
                score += 12
            elif '.com' in domain:
                score += 5
            
            # ê¸°ê´€ëª… ë§¤ì¹­
            name_parts = re.findall(r'[ê°€-í£a-zA-Z]+', organization_name.lower())
            for part in name_parts:
                if len(part) > 2 and part in domain:
                    score += 10
            
            # URL êµ¬ì¡°
            path_depth = len([p for p in parsed.path.strip('/').split('/') if p])
            if path_depth == 0:
                score += 8
            elif path_depth <= 2:
                score += 5
            
            # HTTPS ë³´ë„ˆìŠ¤
            if parsed.scheme == 'https':
                score += 3
            
            scored_urls.append((url, score))
        
        # ì ìˆ˜ ìˆœ ì •ë ¬
        scored_urls.sort(key=lambda x: x[1], reverse=True)
        return scored_urls[0][0]

    def process_organizations(self, organizations: List[Dict]) -> List[Dict]:
        """ê¸°ê´€ ëª©ë¡ ì²˜ë¦¬"""
        processed_orgs = []
        total_count = len(organizations)
        success_count = 0
        
        try:
            for i, org in enumerate(organizations, 1):
                try:
                    self.logger.info(f"ì²˜ë¦¬ ì¤‘ ({i}/{total_count}): {org.get('name', 'Unknown')}")
                    
                    org_copy = org.copy()
                    
                    # ê¸°ì¡´ í™ˆí˜ì´ì§€ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
                    if org.get('homepage') and org['homepage'].strip():
                        if org['homepage'].startswith(('http://', 'https://')):
                            processed_orgs.append(org_copy)
                            success_count += 1
                            continue
                    
                    # í™ˆí˜ì´ì§€ ê²€ìƒ‰
                    homepage_url = self.search_organization_homepage(org.get('name', ''))
                    
                    if homepage_url:
                        org_copy['homepage'] = homepage_url
                        success_count += 1
                        self.logger.info(f"âœ… í™ˆí˜ì´ì§€ ë°œê²¬: {homepage_url}")
                    else:
                        org_copy['homepage'] = ""
                        self.logger.warning(f"âŒ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹¤íŒ¨")
                    
                    processed_orgs.append(org_copy)
                    
                    # ì§„í–‰ë¥  ì¶œë ¥
                    if i % 10 == 0:
                        success_rate = success_count / i * 100
                        self.logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {success_count}/{i} ({success_rate:.1f}%)")
                    
                    # ì§€ì—°
                    if i < total_count:
                        self.add_delay()
                    
                except Exception as e:
                    self.logger.error(f"ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    org_copy = org.copy()
                    if not org.get('homepage'):
                        org_copy['homepage'] = ""
                    processed_orgs.append(org_copy)
                    
        finally:
            self.close_driver()
        
        return processed_orgs

    def save_results(self, organizations: List[Dict], output_file: str) -> bool:
        """ê²°ê³¼ ì €ì¥"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(organizations, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            return True
        except Exception as e:
            self.logger.error(f"ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 70)
    print("ğŸ” ê¸°ê´€ í™ˆí˜ì´ì§€ URL ì¶”ì¶œê¸° (VPN ìš°íšŒ ê¸°ëŠ¥ í¬í•¨)")
    print("=" * 70)
    
    try:
        # ì„¤ì •
        use_headless = False
        print(f"ğŸŒ ë¸Œë¼ìš°ì € ëª¨ë“œ: {'Headless' if use_headless else 'GUI'}")
        print("ğŸ”§ VPN ìš°íšŒ: í™œì„±í™” (êµ¬ê¸€ ì°¨ë‹¨ ì‹œ ìë™ ì‹¤í–‰)")
        
        # ì¶”ì¶œê¸° ìƒì„±
        extractor = URLExtractorEnhanced(headless=use_headless)
        
        # íŒŒì¼ ì„¤ì •
        input_file = r"C:\Users\kimyh\makedb\Python\cradcrawl_adv\undefined_converted_20250609_134731.json"
        output_file = f"enhanced_urls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {input_file}")
        print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼: {output_file}")
        
        # íŒŒì¼ í™•ì¸
        if not os.path.exists(input_file):
            print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
            return 1
        
        # ë°ì´í„° ë¡œë“œ
        print("ğŸ“– ë°ì´í„° ë¡œë”© ì¤‘...")
        with open(input_file, 'r', encoding='utf-8') as f:
            organizations = json.load(f)
        
        print(f"ğŸ“Š ë¡œë“œëœ ê¸°ê´€ ìˆ˜: {len(organizations)}")
        
        # ì²˜ë¦¬í•  ê°œìˆ˜ ì„ íƒ
        max_process = int(input(f"ì²˜ë¦¬í•  ê¸°ê´€ ìˆ˜ (ì „ì²´: {len(organizations)}, ì—”í„°=ì „ì²´): ") or len(organizations))
        organizations = organizations[:max_process]
        
        print(f"\nğŸ”„ {len(organizations)}ê°œ ê¸°ê´€ ì²˜ë¦¬ ì‹œì‘...")
        print("âš ï¸  OpenVPN ì„¤ì¹˜ ê¶Œì¥ (VPN ìš°íšŒ ê¸°ëŠ¥)")
        
        # ì²˜ë¦¬ ì‹¤í–‰
        processed_organizations = extractor.process_organizations(organizations)
        
        # ê²°ê³¼ ì €ì¥
        if extractor.save_results(processed_organizations, output_file):
            print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {output_file}")
            
            # í†µê³„
            homepage_found = sum(1 for org in processed_organizations if org.get('homepage') and org['homepage'].strip())
            success_rate = homepage_found / len(processed_organizations) * 100
            
            print(f"ğŸ“ˆ ìµœì¢… í†µê³„:")
            print(f"  - ì „ì²´ ê¸°ê´€: {len(processed_organizations)}ê°œ")
            print(f"  - í™ˆí˜ì´ì§€ ë°œê²¬: {homepage_found}ê°œ")
            print(f"  - ì„±ê³µë¥ : {success_rate:.1f}%")
        else:
            print(f"\nâŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")
            return 1
        
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 