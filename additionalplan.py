#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Agentic Workflow ê¸°ë°˜ í†µí•© í¬ë¡¤ë§ ì—”ì§„ - ê°œì„ ëœ ë²„ì „
ê¸°ì¡´ ëª¨ë“  ëª¨ë“ˆì„ í†µí•©í•˜ê³  AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì ìš©
"""

import asyncio
import json
import time
import logging
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable
from pathlib import Path
from dataclasses import dataclass
from enum import Enum
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# í”„ë¡œì íŠ¸ ì„¤ì • import
from settings import *
from utils.logger_utils import LoggerUtils
from utils.file_utils import FileUtils
from utils.phone_utils import PhoneUtils
from utils.crawler_utils import CrawlerUtils
from ai_helpers import AIModelManager

class CrawlingStage(Enum):
    """í¬ë¡¤ë§ ë‹¨ê³„ ì •ì˜"""
    INITIALIZATION = "ì´ˆê¸°í™”"
    HOMEPAGE_SEARCH = "í™ˆí˜ì´ì§€_ê²€ìƒ‰"
    HOMEPAGE_ANALYSIS = "í™ˆí˜ì´ì§€_ë¶„ì„"
    CONTACT_PAGE_SEARCH = "ì—°ë½ì²˜í˜ì´ì§€_ê²€ìƒ‰"  # ìƒˆë¡œ ì¶”ê°€
    CONTACT_EXTRACTION = "ì—°ë½ì²˜_ì¶”ì¶œ"
    FAX_SEARCH = "íŒ©ìŠ¤_ê²€ìƒ‰"
    AI_VERIFICATION = "AI_ê²€ì¦"
    DATA_VALIDATION = "ë°ì´í„°_ê²€ì¦"
    COMPLETION = "ì™„ë£Œ"

@dataclass
class CrawlingContext:
    """í¬ë¡¤ë§ ì»¨í…ìŠ¤íŠ¸ (AI Agent ê°„ ê³µìœ  ë°ì´í„°)"""
    organization: Dict[str, Any]
    current_stage: CrawlingStage
    extracted_data: Dict[str, Any]
    ai_insights: Dict[str, Any]
    error_log: List[str]
    processing_time: float
    confidence_scores: Dict[str, float]

class AIAgent:
    """AI ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, ai_manager: AIModelManager, logger: logging.Logger):
        self.name = name
        self.ai_manager = ai_manager
        self.logger = logger
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    async def should_execute(self, context: CrawlingContext) -> bool:
        """ì‹¤í–‰ ì¡°ê±´ í™•ì¸"""
        return True
    
    def update_confidence(self, context: CrawlingContext, field: str, score: float):
        """ì‹ ë¢°ë„ ì ìˆ˜ ì—…ë°ì´íŠ¸"""
        context.confidence_scores[field] = score

class HomepageSearchAgent(AIAgent):
    """í™ˆí˜ì´ì§€ ê²€ìƒ‰ AI ì—ì´ì „íŠ¸ (ê°œì„ ëœ ë²„ì „)"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger):
        super().__init__("HomepageSearchAgent", ai_manager, logger)
        self.crawler_utils = CrawlerUtils()
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """í™ˆí˜ì´ì§€ URL ê²€ìƒ‰ ë° ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            category = context.organization.get('category', '')
            self.logger.info(f"ğŸ” [{self.name}] í™ˆí˜ì´ì§€ ê²€ìƒ‰: {org_name}")
            
            # ê¸°ì¡´ í™ˆí˜ì´ì§€ê°€ ìˆìœ¼ë©´ ê²€ì¦
            existing_homepage = context.organization.get('homepage', '')
            if existing_homepage:
                verification_result = await self._verify_homepage_with_ai(existing_homepage, org_name, category)
                if verification_result['is_valid']:
                    context.extracted_data['homepage'] = existing_homepage
                    context.extracted_data['homepage_type'] = verification_result['type']
                    self.update_confidence(context, 'homepage', 0.9)
                    return context
            
            # ìƒˆë¡œìš´ í™ˆí˜ì´ì§€ ê²€ìƒ‰
            search_results = await self._search_homepage_comprehensive(org_name, category)
            if search_results:
                best_result = search_results[0]  # ìµœê³  ì ìˆ˜
                context.extracted_data['homepage'] = best_result['url']
                context.extracted_data['homepage_type'] = best_result['type']
                context.extracted_data['homepage_confidence'] = best_result['confidence']
                self.update_confidence(context, 'homepage', best_result['confidence'])
                self.logger.info(f"âœ… í™ˆí˜ì´ì§€ ë°œê²¬: {best_result['url']} ({best_result['type']})")
            
            context.current_stage = CrawlingStage.HOMEPAGE_ANALYSIS
            return context
            
        except Exception as e:
            context.error_log.append(f"HomepageSearchAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _search_homepage_comprehensive(self, org_name: str, category: str) -> List[Dict]:
        """ì¢…í•©ì ì¸ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (ê³µì‹ì‚¬ì´íŠ¸ + ì†Œì…œë¯¸ë””ì–´)"""
        driver = None
        try:
            driver = self.crawler_utils.setup_driver(headless=True)
            if not driver:
                return []
            
            all_results = []
            
            # 1. ê³µì‹ í™ˆí˜ì´ì§€ ê²€ìƒ‰
            official_results = await self._search_official_homepage(driver, org_name, category)
            all_results.extend(official_results)
            
            # 2. ì†Œê·œëª¨ ê¸°ê´€ì¸ ê²½ìš° ì†Œì…œë¯¸ë””ì–´ ê²€ìƒ‰
            if is_small_organization(org_name, category):
                social_results = await self._search_social_media(driver, org_name, category)
                all_results.extend(social_results)
            
            # ì ìˆ˜ìˆœ ì •ë ¬
            all_results.sort(key=lambda x: x['confidence'], reverse=True)
            return all_results[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
            
        except Exception as e:
            self.logger.error(f"ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
        finally:
            if driver:
                self.crawler_utils.safe_close_driver(driver)
    
    async def _search_official_homepage(self, driver, org_name: str, category: str) -> List[Dict]:
        """ê³µì‹ í™ˆí˜ì´ì§€ ê²€ìƒ‰"""
        results = []
        
        search_queries = [
            f"{org_name} í™ˆí˜ì´ì§€ site:*.kr",
            f"{org_name} ê³µì‹ì‚¬ì´íŠ¸ site:*.org",
            f"{org_name} {category} í™ˆí˜ì´ì§€"
        ]
        
        for query in search_queries:
            try:
                success = self.crawler_utils.search_google(driver, query)
                if success:
                    urls = self.crawler_utils.extract_urls_from_page(driver)
                    for url in urls[:3]:
                        if self._is_official_homepage_url(url, org_name):
                            verification = await self._verify_homepage_with_ai(url, org_name, category)
                            if verification['is_valid']:
                                results.append({
                                    'url': url,
                                    'type': 'official',
                                    'confidence': verification['confidence'],
                                    'source': query
                                })
                
                await asyncio.sleep(2)  # ê²€ìƒ‰ ê°„ ë”œë ˆì´
                
            except Exception as e:
                self.logger.warning(f"ê³µì‹ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return results
    
    async def _search_social_media(self, driver, org_name: str, category: str) -> List[Dict]:
        """ì†Œì…œë¯¸ë””ì–´ í™ˆí˜ì´ì§€ ê²€ìƒ‰"""
        results = []
        
        social_queries = [
            f"{org_name} site:blog.naver.com",
            f"{org_name} site:cafe.naver.com", 
            f"{org_name} site:facebook.com",
            f"{org_name} site:instagram.com",
            f"{org_name} site:youtube.com"
        ]
        
        for query in social_queries:
            try:
                success = self.crawler_utils.search_google(driver, query)
                if success:
                    urls = self.crawler_utils.extract_urls_from_page(driver)
                    for url in urls[:2]:
                        if self._is_social_media_url(url, org_name):
                            verification = await self._verify_homepage_with_ai(url, org_name, category)
                            if verification['is_valid']:
                                results.append({
                                    'url': url,
                                    'type': self._get_social_media_type(url),
                                    'confidence': verification['confidence'] * 0.7,  # ì†Œì…œë¯¸ë””ì–´ëŠ” ì‹ ë¢°ë„ ë‚®ì¶¤
                                    'source': query
                                })
                
                await asyncio.sleep(2)
                
            except Exception as e:
                self.logger.warning(f"ì†Œì…œë¯¸ë””ì–´ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _is_official_homepage_url(self, url: str, org_name: str) -> bool:
        """ê³µì‹ í™ˆí˜ì´ì§€ URL ê²€ì¦"""
        if not url or not url.startswith(('http://', 'https://')):
            return False
        
        # STRICT_EXCLUDE_DOMAINSë§Œ ì œì™¸
        for exclude_domain in STRICT_EXCLUDE_DOMAINS:
            if exclude_domain in url.lower():
                return False
        
        return True
    
    def _is_social_media_url(self, url: str, org_name: str) -> bool:
        """ì†Œì…œë¯¸ë””ì–´ URL ê²€ì¦"""
        if not url or not url.startswith(('http://', 'https://')):
            return False
        
        # ì†Œì…œë¯¸ë””ì–´ ë„ë©”ì¸ í™•ì¸
        for domain in SOCIAL_MEDIA_DOMAINS.keys():
            if domain in url.lower():
                return True
        
        return False
    
    def _get_social_media_type(self, url: str) -> str:
        """ì†Œì…œë¯¸ë””ì–´ íƒ€ì… ë°˜í™˜"""
        for domain, type_name in SOCIAL_MEDIA_DOMAINS.items():
            if domain in url.lower():
                return type_name
        return "ì†Œì…œë¯¸ë””ì–´"
    
    async def _verify_homepage_with_ai(self, url: str, org_name: str, category: str) -> Dict:
        """AIë¡œ í™ˆí˜ì´ì§€ ê´€ë ¨ì„± ê²€ì¦ (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸)"""
        try:
            prompt = f"""
            ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™ˆí˜ì´ì§€ì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”:

            **ê¸°ê´€ ì •ë³´:**
            - ê¸°ê´€ëª…: {org_name}
            - ì¹´í…Œê³ ë¦¬: {category}
            - í™ˆí˜ì´ì§€ URL: {url}

            **íŒë‹¨ ê¸°ì¤€:**
            1. ë„ë©”ì¸ëª…ì´ ê¸°ê´€ëª…ê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€?
            2. URLì´ í•´ë‹¹ ê¸°ê´€ì˜ ê³µì‹ ë˜ëŠ” ê´€ë ¨ í˜ì´ì§€ì¸ê°€?
            3. ì†Œê·œëª¨ ê¸°ê´€ì˜ ê²½ìš° ë¸”ë¡œê·¸/ì¹´í˜/SNSë„ ê³µì‹ í˜ì´ì§€ë¡œ ì¸ì •
            4. ê¸°ê´€ì˜ ì„±ê²©ê³¼ URL íƒ€ì…ì´ ì í•©í•œê°€?

            **ì‘ë‹µ í˜•ì‹:**
            VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            TYPE: [ê³µì‹ì‚¬ì´íŠ¸/ë„¤ì´ë²„ë¸”ë¡œê·¸/ë„¤ì´ë²„ì¹´í˜/í˜ì´ìŠ¤ë¶/ì¸ìŠ¤íƒ€ê·¸ë¨/ìœ íŠœë¸Œ/ê¸°íƒ€]
            CONFIDENCE: [0.1-1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„ ì ìˆ˜]
            REASON: [íŒë‹¨ ì´ìœ  1-2ë¬¸ì¥]
            """
            
            response = await self.ai_manager.extract_with_gemini(url, prompt)
            return self._parse_verification_response(response)
            
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {'is_valid': False, 'type': 'ì•Œìˆ˜ì—†ìŒ', 'confidence': 0.0}
    
    def _parse_verification_response(self, response: str) -> Dict:
        """AI ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        result = {
            'is_valid': False,
            'type': 'ì•Œìˆ˜ì—†ìŒ',
            'confidence': 0.0,
            'reason': ''
        }
        
        try:
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('VALID:'):
                    result['is_valid'] = 'ì˜ˆ' in line
                elif line.startswith('TYPE:'):
                    result['type'] = line.replace('TYPE:', '').strip()
                elif line.startswith('CONFIDENCE:'):
                    confidence_text = line.replace('CONFIDENCE:', '').strip()
                    try:
                        result['confidence'] = float(confidence_text)
                    except:
                        result['confidence'] = 0.5
                elif line.startswith('REASON:'):
                    result['reason'] = line.replace('REASON:', '').strip()
        
        except Exception as e:
            self.logger.warning(f"ê²€ì¦ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return result

class HomepageAnalysisAgent(AIAgent):
    """í™ˆí˜ì´ì§€ ë¶„ì„ AI ì—ì´ì „íŠ¸ (ê°œì„ ëœ ë²„ì „)"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger):
        super().__init__("HomepageAnalysisAgent", ai_manager, logger)
        self.crawler_utils = CrawlerUtils()
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """í™ˆí˜ì´ì§€ ë‚´ìš© ë¶„ì„ ë° ì •ë³´ ì¶”ì¶œ"""
        try:
            homepage_url = context.extracted_data.get('homepage')
            if not homepage_url:
                context.current_stage = CrawlingStage.CONTACT_PAGE_SEARCH
                return context
            
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸŒ [{self.name}] í™ˆí˜ì´ì§€ ë¶„ì„: {homepage_url}")
            
            # í™ˆí˜ì´ì§€ íŒŒì‹±
            page_data = await self._extract_page_content(homepage_url)
            
            if page_data and page_data.get('accessible'):
                # ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
                contact_info = page_data.get('contact_info', {})
                self._store_contact_info(context, contact_info)
                
                # ì—°ë½ì²˜ í˜ì´ì§€ ë§í¬ ì°¾ê¸°
                contact_links = self._find_contact_page_links(page_data)
                if contact_links:
                    context.extracted_data['contact_page_links'] = contact_links
                
                # AI ë¶„ì„ ì‹¤í–‰
                ai_summary = await self._analyze_with_ai(org_name, page_data)
                context.ai_insights['homepage_analysis'] = ai_summary
                
                self.logger.info(f"âœ… í™ˆí˜ì´ì§€ ë¶„ì„ ì™„ë£Œ: ì—°ë½ì²˜ {len(contact_info)} í•­ëª©, ì—°ë½ì²˜ í˜ì´ì§€ {len(contact_links)} ê°œ")
            
            context.current_stage = CrawlingStage.CONTACT_PAGE_SEARCH
            return context
            
        except Exception as e:
            context.error_log.append(f"HomepageAnalysisAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    def _find_contact_page_links(self, page_data: Dict) -> List[Dict]:
        """ì—°ë½ì²˜ í˜ì´ì§€ ë§í¬ ì°¾ê¸°"""
        contact_links = []
        
        try:
            soup = page_data.get('soup')
            base_url = page_data.get('url', '')
            
            if not soup:
                return []
            
            # ë§í¬ ìš”ì†Œë“¤ ì°¾ê¸°
            links = soup.find_all('a', href=True)
            
            for link in links:
                link_text = link.get_text(strip=True).lower()
                href = link.get('href', '')
                
                # ì—°ë½ì²˜ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                for keyword in CONTACT_NAVIGATION_KEYWORDS:
                    if keyword.lower() in link_text:
                        full_url = self._resolve_url(href, base_url)
                        if full_url:
                            contact_links.append({
                                'url': full_url,
                                'text': link.get_text(strip=True),
                                'keyword': keyword
                            })
                        break
        
        except Exception as e:
            self.logger.warning(f"ì—°ë½ì²˜ í˜ì´ì§€ ë§í¬ ì°¾ê¸° ì˜¤ë¥˜: {e}")
        
        return contact_links[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
    
    def _resolve_url(self, href: str, base_url: str) -> Optional[str]:
        """ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜"""
        try:
            from urllib.parse import urljoin, urlparse
            
            if href.startswith(('http://', 'https://')):
                return href
            elif href.startswith('/'):
                parsed = urlparse(base_url)
                return f"{parsed.scheme}://{parsed.netloc}{href}"
            else:
                return urljoin(base_url, href)
        except:
            return None
    
    def _store_contact_info(self, context: CrawlingContext, contact_info: Dict):
        """ì—°ë½ì²˜ ì •ë³´ ì €ì¥"""
        # ê¸°ë³¸ ì—°ë½ì²˜ ì •ë³´
        if contact_info.get('phones'):
            context.extracted_data['phone'] = contact_info['phones'][0]
            self.update_confidence(context, 'phone', 0.9)
            
            # ì¶”ê°€ ì „í™”ë²ˆí˜¸ë“¤ ì €ì¥
            if len(contact_info['phones']) > 1:
                context.extracted_data['additional_phones'] = contact_info['phones'][1:5]  # ìµœëŒ€ 4ê°œ ì¶”ê°€
        
        if contact_info.get('faxes'):
            context.extracted_data['fax'] = contact_info['faxes'][0]
            self.update_confidence(context, 'fax', 0.9)
        
        if contact_info.get('emails'):
            context.extracted_data['email'] = contact_info['emails'][0]
            self.update_confidence(context, 'email', 0.9)
        
        if contact_info.get('addresses'):
            context.extracted_data['address'] = contact_info['addresses'][0]
            self.update_confidence(context, 'address', 0.8)
    
    async def _extract_page_content(self, url: str) -> Dict[str, Any]:
        """í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        driver = None
        try:
            driver = self.crawler_utils.setup_driver(headless=True)
            if not driver:
                return {}
            
            # í˜ì´ì§€ ë¡œë“œ
            driver.get(url)
            time.sleep(3)
            
            # ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸
            if not self._is_page_accessible(driver):
                return {'accessible': False}
            
            # BeautifulSoup íŒŒì‹±
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            page_text = soup.get_text(separator=' ', strip=True)
            
            # ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
            contact_info = self._extract_contact_info_enhanced(page_text)
            
            return {
                'accessible': True,
                'title': driver.title,
                'url': url,
                'text_content': page_text[:5000],
                'contact_info': contact_info,
                'soup': soup  # BeautifulSoup ê°ì²´ë„ ì €ì¥
            }
            
        except Exception as e:
            self.logger.error(f"í˜ì´ì§€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {'accessible': False, 'error': str(e)}
        finally:
            if driver:
                self.crawler_utils.safe_close_driver(driver)
    
    def _extract_contact_info_enhanced(self, text: str) -> Dict[str, List[str]]:
        """ê°•í™”ëœ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        contact_info = {
            "phones": [],
            "faxes": [],
            "emails": [],
            "addresses": [],
            "additional_phones": []
        }
        
        try:
            # ê¸°ë³¸ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in PHONE_EXTRACTION_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    phone = PhoneUtils.format_phone_number(match)
                    if phone and validate_phone_length(phone) and phone not in contact_info["phones"]:
                        contact_info["phones"].append(phone)
            
            # ì¶”ê°€ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in ADDITIONAL_PHONE_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    phone = PhoneUtils.format_phone_number(match)
                    if (phone and validate_phone_length(phone) and 
                        phone not in contact_info["phones"] and 
                        phone not in contact_info["additional_phones"]):
                        contact_info["additional_phones"].append(phone)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in FAX_EXTRACTION_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    fax = PhoneUtils.format_phone_number(match)
                    if fax and validate_phone_length(fax) and fax not in contact_info["faxes"]:
                        contact_info["faxes"].append(fax)
            
            # ì´ë©”ì¼ ì¶”ì¶œ
            for pattern in EMAIL_EXTRACTION_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if self._is_valid_email(match) and match not in contact_info["emails"]:
                        contact_info["emails"].append(match)
            
            # ì£¼ì†Œ ì¶”ì¶œ
            for pattern in ADDRESS_EXTRACTION_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    clean_address = self._clean_address(match)
                    if clean_address and clean_address not in contact_info["addresses"]:
                        contact_info["addresses"].append(clean_address)
        
        except Exception as e:
            self.logger.warning(f"ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return contact_info
    
    def _is_page_accessible(self, driver) -> bool:
        """í˜ì´ì§€ ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            title = driver.title.lower()
            if any(keyword in title for keyword in ['404', 'not found', 'error']):
                return False
            
            page_source = driver.page_source
            return len(page_source) > 1000
            
        except:
            return False
    
    def _is_valid_email(self, email: str) -> bool:
        """ì´ë©”ì¼ ìœ íš¨ì„± í™•ì¸"""
        import re
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return re.match(email_pattern, email) is not None
    
    def _clean_address(self, address: str) -> Optional[str]:
        """ì£¼ì†Œ ì •ë¦¬"""
        if not address or len(address) < 10:
            return None
        return re.sub(r'\s+', ' ', address.strip())
    
    async def _analyze_with_ai(self, org_name: str, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """AIë¡œ í˜ì´ì§€ ë‚´ìš© ë¶„ì„ (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸)"""
        try:
            contact_info = page_data.get('contact_info', {})
            
            prompt = f"""
            '{org_name}' ê¸°ê´€ì˜ í™ˆí˜ì´ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

            **ê¸°ë³¸ ì •ë³´:**
            - ì œëª©: {page_data.get('title', '')}
            - URL: {page_data.get('url', '')}

            **ì¶”ì¶œëœ ì—°ë½ì²˜:**
            - ì „í™”ë²ˆí˜¸: {', '.join(contact_info.get('phones', []))}
            - ì¶”ê°€ì „í™”ë²ˆí˜¸: {', '.join(contact_info.get('additional_phones', []))}
            - íŒ©ìŠ¤ë²ˆí˜¸: {', '.join(contact_info.get('faxes', []))}
            - ì´ë©”ì¼: {', '.join(contact_info.get('emails', []))}
            - ì£¼ì†Œ: {', '.join(contact_info.get('addresses', []))}

            **í™ˆí˜ì´ì§€ ë‚´ìš© (ì²˜ìŒ 2000ì):**
            {page_data.get('text_content', '')[:2000]}

            **ë¶„ì„ ìš”ì²­:**
            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
            
            SUMMARY: [ê¸°ê´€ ìš”ì•½ 3-4ë¬¸ì¥]
            CATEGORY: [ê¸°ê´€ ìœ í˜• - êµíšŒ, ë³‘ì›, í•™êµ, ì •ë¶€ê¸°ê´€, ê¸°ì—…, ë‹¨ì²´ ë“±]
            SERVICES: [ì£¼ìš” ì„œë¹„ìŠ¤ 3ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„]
            LOCATION: [ìœ„ì¹˜ì •ë³´]
            PHONE_VALIDATION: [ì¶”ì¶œëœ ì „í™”ë²ˆí˜¸ë“¤ì´ 9-11ìë¦¬ í•œêµ­ ì „í™”ë²ˆí˜¸ í˜•ì‹ì— ë§ëŠ”ì§€ - ì í•©/ë¶€ì í•©]
            FAX_PHONE_DUPLICATE: [íŒ©ìŠ¤ë²ˆí˜¸ì™€ ì „í™”ë²ˆí˜¸ê°€ ë™ì¼í•œì§€ - ë™ì¼/ë‹¤ë¦„/í™•ì¸ë¶ˆê°€]
            CONTACT_QUALITY: [ì—°ë½ì²˜ ì •ë³´ í’ˆì§ˆ - ìƒ/ì¤‘/í•˜]
            WEBSITE_QUALITY: [ì›¹ì‚¬ì´íŠ¸ í’ˆì§ˆ - ìƒ/ì¤‘/í•˜]
            FEATURES: [íŠ¹ë³„í•œ íŠ¹ì§• 2ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„]
            """
            
            response = await self.ai_manager.extract_with_gemini(page_data.get('text_content', ''), prompt)
            return self._parse_ai_response(response)
            
        except Exception as e:
            self.logger.error(f"AI ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'summary': f'AI ë¶„ì„ ì˜¤ë¥˜: {str(e)}'}
    
    def _parse_ai_response(self, response: str) -> Dict[str, Any]:
        """AI ì‘ë‹µ íŒŒì‹± (ê°œì„ ëœ ë²„ì „)"""
        result = {
            "summary": "",
            "category": "ê¸°íƒ€",
            "services": [],
            "location": "",
            "phone_validation": "í™•ì¸ë¶ˆê°€",
            "fax_phone_duplicate": "í™•ì¸ë¶ˆê°€",
            "contact_quality": "ì¤‘",
            "website_quality": "ì¤‘",
            "features": []
        }
        
        try:
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('SUMMARY:'):
                    result["summary"] = line.replace('SUMMARY:', '').strip()
                elif line.startswith('CATEGORY:'):
                    result["category"] = line.replace('CATEGORY:', '').strip()
                elif line.startswith('SERVICES:'):
                    services_text = line.replace('SERVICES:', '').strip()
                    result["services"] = [s.strip() for s in services_text.split(',')]
                elif line.startswith('LOCATION:'):
                    result["location"] = line.replace('LOCATION:', '').strip()
                elif line.startswith('PHONE_VALIDATION:'):
                    result["phone_validation"] = line.replace('PHONE_VALIDATION:', '').strip()
                elif line.startswith('FAX_PHONE_DUPLICATE:'):
                    result["fax_phone_duplicate"] = line.replace('FAX_PHONE_DUPLICATE:', '').strip()
                elif line.startswith('CONTACT_QUALITY:'):
                    result["contact_quality"] = line.replace('CONTACT_QUALITY:', '').strip()
                elif line.startswith('WEBSITE_QUALITY:'):
                    result["website_quality"] = line.replace('WEBSITE_QUALITY:', '').strip()
                elif line.startswith('FEATURES:'):
                    features_text = line.replace('FEATURES:', '').strip()
                    result["features"] = [f.strip() for f in features_text.split(',')]
        
        except Exception as e:
            self.logger.warning(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return result

class ContactPageSearchAgent(AIAgent):
    """ì—°ë½ì²˜ í˜ì´ì§€ ê²€ìƒ‰ AI ì—ì´ì „íŠ¸ (ìƒˆë¡œ ì¶”ê°€)"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger):
        super().__init__("ContactPageSearchAgent", ai_manager, logger)
        self.crawler_utils = CrawlerUtils()
    
    async def should_execute(self, context: CrawlingContext) -> bool:
        """ì—°ë½ì²˜ í˜ì´ì§€ ë§í¬ê°€ ìˆì„ ë•Œë§Œ ì‹¤í–‰"""
        return bool(context.extracted_data.get('contact_page_links'))
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì—°ë½ì²˜ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ"""
        try:
            contact_links = context.extracted_data.get('contact_page_links', [])
            org_name = context.organization.get('name', '')
            
            self.logger.info(f"ğŸ“ [{self.name}] ì—°ë½ì²˜ í˜ì´ì§€ ê²€ìƒ‰: {len(contact_links)}ê°œ ë§í¬")
            
            additional_contacts = []
            
            for link_info in contact_links[:3]:  # ìµœëŒ€ 3ê°œ í˜ì´ì§€ë§Œ í™•ì¸
                contact_data = await self._extract_contact_page(link_info['url'])
                if contact_data:
                    additional_contacts.append({
                        'url': link_info['url'],
                        'keyword': link_info['keyword'],
                        'contacts': contact_data
                    })
            
            if additional_contacts:
                context.extracted_data['additional_contact_pages'] = additional_contacts
                self._merge_additional_contacts(context, additional_contacts)
                self.logger.info(f"âœ… ì¶”ê°€ ì—°ë½ì²˜ í˜ì´ì§€ {len(additional_contacts)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
            
            context.current_stage = CrawlingStage.CONTACT_EXTRACTION
            return context
            
        except Exception as e:
            context.error_log.append(f"ContactPageSearchAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _extract_contact_page(self, url: str) -> Optional[Dict]:
        """ì—°ë½ì²˜ í˜ì´ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        driver = None
        try:
            driver = self.crawler_utils.setup_driver(headless=True)
            if not driver:
                return None
            
            driver.get(url)
            time.sleep(2)
            
            page_text = driver.find_element(By.TAG_NAME, "body").text
            
            # ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ (HomepageAnalysisAgentì™€ ë™ì¼í•œ ë¡œì§)
            contact_info = self._extract_contact_info_enhanced(page_text)
            
            return contact_info if any(contact_info.values()) else None
            
        except Exception as e:
            self.logger.warning(f"ì—°ë½ì²˜ í˜ì´ì§€ ì¶”ì¶œ ì˜¤ë¥˜ {url}: {e}")
            return None
        finally:
            if driver:
                self.crawler_utils.safe_close_driver(driver)
    
    def _extract_contact_info_enhanced(self, text: str) -> Dict[str, List[str]]:
        """ê°•í™”ëœ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ (HomepageAnalysisAgentì™€ ë™ì¼)"""
        contact_info = {
            "phones": [],
            "faxes": [],
            "emails": [],
            "addresses": []
        }
        
        try:
            # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in PHONE_EXTRACTION_PATTERNS + ADDITIONAL_PHONE_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    phone = PhoneUtils.format_phone_number(match)
                    if phone and validate_phone_length(phone) and phone not in contact_info["phones"]:
                        contact_info["phones"].append(phone)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in FAX_EXTRACTION_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    fax = PhoneUtils.format_phone_number(match)
                    if fax and validate_phone_length(fax) and fax not in contact_info["faxes"]:
                        contact_info["faxes"].append(fax)
            
            # ì´ë©”ì¼ ì¶”ì¶œ
            for pattern in EMAIL_EXTRACTION_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if self._is_valid_email(match) and match not in contact_info["emails"]:
                        contact_info["emails"].append(match)
        
        except Exception as e:
            self.logger.warning(f"ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return contact_info
    
    def _is_valid_email(self, email: str) -> bool:
        """ì´ë©”ì¼ ìœ íš¨ì„± í™•ì¸"""
        import re
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return re.match(email_pattern, email) is not None

class FaxSearchAgent(AIAgent):
    """íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ AI ì—ì´ì „íŠ¸ (faxextractor.py ë¡œì§ í†µí•©)"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger):
        super().__init__("FaxSearchAgent", ai_manager, logger)
        self.crawler_utils = CrawlerUtils()
    
    async def should_execute(self, context: CrawlingContext) -> bool:
        """íŒ©ìŠ¤ë²ˆí˜¸ê°€ ì—†ì„ ë•Œë§Œ ì‹¤í–‰"""
        return not context.extracted_data.get('fax')
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ íŒ©ìŠ¤ë²ˆí˜¸ ì°¾ê¸°"""
        try:
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ“  [{self.name}] íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰: {org_name}")
            
            fax_number = await self._search_fax_number(org_name)
            if fax_number:
                # ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µ í™•ì¸
                phone = context.extracted_data.get('phone', '')
                if fax_number != phone:
                    context.extracted_data['fax'] = fax_number
                    self.update_confidence(context, 'fax', 0.7)
                    self.logger.info(f"âœ… íŒ©ìŠ¤ë²ˆí˜¸ ë°œê²¬: {fax_number}")
                else:
                    self.logger.info(f"âš ï¸ ì „í™”ë²ˆí˜¸ì™€ ë™ì¼í•œ íŒ©ìŠ¤ë²ˆí˜¸: {fax_number}")
            
            context.current_stage = CrawlingStage.AI_VERIFICATION
            return context
            
        except Exception as e:
            context.error_log.append(f"FaxSearchAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _search_fax_number(self, org_name: str) -> Optional[str]:
        """êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ íŒ©ìŠ¤ë²ˆí˜¸ ì°¾ê¸° (faxextractor.py ë¡œì§)"""
        driver = None
        try:
            driver = self.crawler_utils.setup_driver(headless=True)
            if not driver:
                return None
            
            search_query = f"{org_name} íŒ©ìŠ¤ë²ˆí˜¸"
            success = self.crawler_utils.search_google(driver, search_query)
            
            if success:
                page_text = driver.find_element(By.TAG_NAME, "body").text
                fax_numbers = self._extract_fax_numbers(page_text)
                
                if fax_numbers:
                    return fax_numbers[0]  # ì²« ë²ˆì§¸ ë°œê²¬ëœ íŒ©ìŠ¤ë²ˆí˜¸
            
            return None
            
        except Exception as e:
            self.logger.error(f"íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
        finally:
            if driver:
                self.crawler_utils.safe_close_driver(driver)
    
    def _extract_fax_numbers(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ"""
        fax_numbers = []
        
        for pattern in FAX_EXTRACTION_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                fax = PhoneUtils.format_phone_number(match)
                if fax and validate_phone_length(fax) and fax not in fax_numbers:
                    fax_numbers.append(fax)
        
        return fax_numbers

class AIVerificationAgent(AIAgent):
    """AI ê²€ì¦ ì—ì´ì „íŠ¸"""
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì¶”ì¶œëœ ë°ì´í„°ë¥¼ AIë¡œ ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ¤– [{self.name}] AI ê²€ì¦: {org_name}")
            
            # ì¶”ì¶œëœ ë°ì´í„° ê²€ì¦
            verification_result = await self._verify_extracted_data(context)
            context.ai_insights['verification'] = verification_result
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ì¡°ì •
            self._adjust_confidence_scores(context, verification_result)
            
            context.current_stage = CrawlingStage.DATA_VALIDATION
            return context
            
        except Exception as e:
            context.error_log.append(f"AIVerificationAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _verify_extracted_data(self, context: CrawlingContext) -> Dict[str, Any]:
        """ì¶”ì¶œëœ ë°ì´í„° AI ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            extracted = context.extracted_data
            
            prompt = f"""
            ê¸°ê´€ëª…: {org_name}
            ì¶”ì¶œëœ ì •ë³´ë¥¼ ê²€ì¦í•´ì£¼ì„¸ìš”:
            
            - í™ˆí˜ì´ì§€: {extracted.get('homepage', 'ì—†ìŒ')}
            - ì „í™”ë²ˆí˜¸: {extracted.get('phone', 'ì—†ìŒ')}
            - íŒ©ìŠ¤ë²ˆí˜¸: {extracted.get('fax', 'ì—†ìŒ')}
            - ì´ë©”ì¼: {extracted.get('email', 'ì—†ìŒ')}
            - ì£¼ì†Œ: {extracted.get('address', 'ì—†ìŒ')}
            
            ê° ì •ë³´ê°€ í•´ë‹¹ ê¸°ê´€ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨í•˜ê³ , 
            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
            
            HOMEPAGE_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            PHONE_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            FAX_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            EMAIL_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            ADDRESS_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            OVERALL_CONFIDENCE: [ë†’ìŒ/ë³´í†µ/ë‚®ìŒ]
            ISSUES: [ë°œê²¬ëœ ë¬¸ì œì ë“¤]
            """
            
            response = await self.ai_manager.extract_with_gemini("", prompt)
            return self._parse_verification_response(response)
            
        except Exception as e:
            self.logger.error(f"AI ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {'overall_confidence': 'ë‚®ìŒ', 'issues': [str(e)]}
    
    def _parse_verification_response(self, response: str) -> Dict[str, Any]:
        """AI ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        result = {
            'homepage_valid': False,
            'phone_valid': False,
            'fax_valid': False,
            'email_valid': False,
            'address_valid': False,
            'overall_confidence': 'ë³´í†µ',
            'issues': []
        }
        
        try:
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('HOMEPAGE_VALID:'):
                    result['homepage_valid'] = 'ì˜ˆ' in line
                elif line.startswith('PHONE_VALID:'):
                    result['phone_valid'] = 'ì˜ˆ' in line
                elif line.startswith('FAX_VALID:'):
                    result['fax_valid'] = 'ì˜ˆ' in line
                elif line.startswith('EMAIL_VALID:'):
                    result['email_valid'] = 'ì˜ˆ' in line
                elif line.startswith('ADDRESS_VALID:'):
                    result['address_valid'] = 'ì˜ˆ' in line
                elif line.startswith('OVERALL_CONFIDENCE:'):
                    confidence = line.replace('OVERALL_CONFIDENCE:', '').strip()
                    result['overall_confidence'] = confidence
                elif line.startswith('ISSUES:'):
                    issues_text = line.replace('ISSUES:', '').strip()
                    if issues_text and issues_text != 'ì—†ìŒ':
                        result['issues'] = [issues_text]
        
        except Exception as e:
            self.logger.warning(f"ê²€ì¦ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return result
    
    def _adjust_confidence_scores(self, context: CrawlingContext, verification: Dict[str, Any]):
        """ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ì‹ ë¢°ë„ ì ìˆ˜ ì¡°ì •"""
        adjustments = {
            'homepage': verification.get('homepage_valid', False),
            'phone': verification.get('phone_valid', False),
            'fax': verification.get('fax_valid', False),
            'email': verification.get('email_valid', False),
            'address': verification.get('address_valid', False)
        }
        
        for field, is_valid in adjustments.items():
            if field in context.confidence_scores:
                if is_valid:
                    context.confidence_scores[field] = min(1.0, context.confidence_scores[field] + 0.1)
                else:
                    context.confidence_scores[field] = max(0.0, context.confidence_scores[field] - 0.2)

class DataValidationAgent(AIAgent):
    """ë°ì´í„° ê²€ì¦ ì—ì´ì „íŠ¸ (phone_utils.py ë¡œì§ í™œìš©)"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger):
        super().__init__("DataValidationAgent", ai_manager, logger)
        self.phone_utils = PhoneUtils()
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ìµœì¢… ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬"""
        try:
            self.logger.info(f"âœ… [{self.name}] ë°ì´í„° ê²€ì¦ ì‹œì‘")
            
            # ì „í™”ë²ˆí˜¸ ê²€ì¦
            phone = context.extracted_data.get('phone')
            if phone:
                if self.phone_utils.validate_korean_phone(phone):
                    context.extracted_data['phone'] = self.phone_utils.format_phone_number(phone)
                else:
                    context.error_log.append(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì „í™”ë²ˆí˜¸: {phone}")
                    context.extracted_data.pop('phone', None)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦
            fax = context.extracted_data.get('fax')
            if fax:
                if self.phone_utils.validate_korean_phone(fax):
                    context.extracted_data['fax'] = self.phone_utils.format_phone_number(fax)
                else:
                    context.error_log.append(f"ìœ íš¨í•˜ì§€ ì•Šì€ íŒ©ìŠ¤ë²ˆí˜¸: {fax}")
                    context.extracted_data.pop('fax', None)
            
            # ì „í™”ë²ˆí˜¸-íŒ©ìŠ¤ë²ˆí˜¸ ì¤‘ë³µ í™•ì¸
            if phone and fax and self.phone_utils.is_phone_fax_duplicate(phone, fax):
                context.error_log.append("ì „í™”ë²ˆí˜¸ì™€ íŒ©ìŠ¤ë²ˆí˜¸ê°€ ì¤‘ë³µë¨")
                context.extracted_data.pop('fax', None)
            
            # ì´ë©”ì¼ ê²€ì¦
            email = context.extracted_data.get('email')
            if email and not self._is_valid_email(email):
                context.error_log.append(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë©”ì¼: {email}")
                context.extracted_data.pop('email', None)
            
            context.current_stage = CrawlingStage.COMPLETION
            return context
            
        except Exception as e:
            context.error_log.append(f"DataValidationAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    def _is_valid_email(self, email: str) -> bool:
        """ì´ë©”ì¼ ìœ íš¨ì„± í™•ì¸"""
        import re
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return re.match(email_pattern, email) is not None

class EnhancedUnifiedCrawler:
    """AI Agentic Workflow ê¸°ë°˜ ê°•í™”ëœ í†µí•© í¬ë¡¤ëŸ¬"""
    
    def __init__(self, config_override=None):
        """ì´ˆê¸°í™”"""
        self.config = config_override or CRAWLING_CONFIG
        self.logger = LoggerUtils.setup_crawler_logger()
        self.ai_manager = AIModelManager()
        
        # AI ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™”
        self.agents = [
            HomepageSearchAgent(self.ai_manager, self.logger),
            HomepageAnalysisAgent(self.ai_manager, self.logger),
            FaxSearchAgent(self.ai_manager, self.logger),
            AIVerificationAgent(self.ai_manager, self.logger),
            DataValidationAgent(self.ai_manager, self.logger)
        ]
        
        # í†µê³„ ì •ë³´
        self.stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "start_time": None,
            "end_time": None,
            "agent_stats": {agent.name: {"executed": 0, "success": 0} for agent in self.agents}
        }
        
        self.logger.info("ğŸš€ AI Agentic Workflow í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def process_organizations_with_callback(self, organizations: List[Dict]) -> List[Dict]:
        """AI Agentic Workflowë¡œ ì¡°ì§ ì²˜ë¦¬"""
        if not organizations:
            return []
        
        self.stats["start_time"] = datetime.now()
        self.stats["total_processed"] = len(organizations)
        
        results = []
        
        for i, org in enumerate(organizations, 1):
            try:
                # í¬ë¡¤ë§ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
                context = CrawlingContext(
                    organization=org,
                    current_stage=CrawlingStage.INITIALIZATION,
                    extracted_data={},
                    ai_insights={},
                    error_log=[],
                    processing_time=0,
                    confidence_scores={}
                )
                
                start_time = time.time()
                
                # AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                context = await self._execute_agent_workflow(context, i)
                
                processing_time = time.time() - start_time
                context.processing_time = processing_time
                
                # ê²°ê³¼ ì¡°í•©
                processed_org = self._combine_results(org, context)
                results.append(processed_org)
                
                # ì½œë°± í˜¸ì¶œ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
                if hasattr(self, 'progress_callback') and self.progress_callback:
                    self._call_progress_callback(processed_org, i, len(organizations), processing_time)
                
                self.stats["successful"] += 1
                
                # ë”œë ˆì´
                await asyncio.sleep(self.config.get("default_delay", 2))
                
            except Exception as e:
                self.logger.error(f"âŒ ì¡°ì§ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {org.get('name', 'Unknown')} - {e}")
                self.stats["failed"] += 1
                results.append(org)
        
        self.stats["end_time"] = datetime.now()
        self._print_agent_statistics()
        
        return results
    
    async def _execute_agent_workflow(self, context: CrawlingContext, index: int) -> CrawlingContext:
        """AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        org_name = context.organization.get('name', 'Unknown')
        self.logger.info(f"ğŸ¤– AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹œì‘ [{index}]: {org_name}")
        
        for agent in self.agents:
            try:
                # ì‹¤í–‰ ì¡°ê±´ í™•ì¸
                if await agent.should_execute(context):
                    self.logger.info(f"ğŸ”„ ì—ì´ì „íŠ¸ ì‹¤í–‰: {agent.name}")
                    
                    # ì—ì´ì „íŠ¸ ì‹¤í–‰
                    context = await agent.execute(context)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.stats["agent_stats"][agent.name]["executed"] += 1
                    if not context.error_log or len(context.error_log) == 0:
                        self.stats["agent_stats"][agent.name]["success"] += 1
                    
                    self.logger.info(f"âœ… ì—ì´ì „íŠ¸ ì™„ë£Œ: {agent.name}")
                else:
                    self.logger.info(f"â­ï¸ ì—ì´ì „íŠ¸ ìŠ¤í‚µ: {agent.name}")
                
            except Exception as e:
                self.logger.error(f"âŒ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜ {agent.name}: {e}")
                context.error_log.append(f"{agent.name} ì˜¤ë¥˜: {str(e)}")
        
        self.logger.info(f"ğŸ‰ AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: {org_name}")
        return context
    
    def _combine_results(self, original_org: Dict, context: CrawlingContext) -> Dict:
        """ì›ë³¸ ì¡°ì§ ë°ì´í„°ì™€ ì¶”ì¶œëœ ë°ì´í„° ê²°í•©"""
        result = original_org.copy()
        
        # ì¶”ì¶œëœ ë°ì´í„° ì¶”ê°€
        result.update(context.extracted_data)
        
        # AI ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
        result['ai_insights'] = context.ai_insights
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ì¶”ê°€
        result['confidence_scores'] = context.confidence_scores
        
        # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result['processing_metadata'] = {
            'processing_time': context.processing_time,
            'final_stage': context.current_stage.value,
            'error_count': len(context.error_log),
            'errors': context.error_log,
            'extraction_method': 'ai_agentic_workflow',
            'timestamp': datetime.now().isoformat()
        }
        
        return result
    
    def _call_progress_callback(self, processed_org: Dict, current: int, total: int, processing_time: float):
        """ì§„í–‰ ìƒí™© ì½œë°± í˜¸ì¶œ"""
        try:
            callback_data = {
                'name': processed_org.get('name', 'Unknown'),
                'category': processed_org.get('category', ''),
                'homepage_url': processed_org.get('homepage', ''),
                'status': 'completed',
                'current_step': f'{current}/{total}',
                'processing_time': processing_time,
                'phone': processed_org.get('phone', ''),
                'fax': processed_org.get('fax', ''),
                'email': processed_org.get('email', ''),
                'address': processed_org.get('address', ''),
                'extraction_method': 'ai_agentic_workflow',
                'confidence_scores': processed_org.get('confidence_scores', {}),
                'ai_insights': processed_org.get('ai_insights', {})
            }
            self.progress_callback(callback_data)
            
        except Exception as e:
            self.logger.error(f"ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    def _print_agent_statistics(self):
        """ì—ì´ì „íŠ¸ í†µê³„ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ¤– AI Agentic Workflow í†µê³„")
        print("="*80)
        
        for agent_name, stats in self.stats["agent_stats"].items():
            executed = stats["executed"]
            success = stats["success"]
            success_rate = (success / executed * 100) if executed > 0 else 0
            print(f"ğŸ”§ {agent_name:<25} ì‹¤í–‰: {executed:>3}íšŒ, ì„±ê³µ: {success:>3}íšŒ ({success_rate:>5.1f}%)")
        
        duration = self.stats["end_time"] - self.stats["start_time"]
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  ğŸ“‹ ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"  âœ… ì„±ê³µ: {self.stats['successful']}ê°œ")
        print(f"  âŒ ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
        print(f"  ğŸ“ˆ ì„±ê³µë¥ : {(self.stats['successful']/self.stats['total_processed']*100):.1f}%")
        print(f"  â±ï¸ ì†Œìš”ì‹œê°„: {duration}")
        print("="*80)

    # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œë“¤
    async def process_json_file_async(self, json_file_path: str, test_mode: bool = False, test_count: int = 10, progress_callback=None) -> List[Dict]:
        """app.py í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ"""
        try:
            # JSON íŒŒì¼ ë¡œë“œ
            data = FileUtils.load_json(json_file_path)
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            organizations = []
            if isinstance(data, dict):
                for category, orgs in data.items():
                    if isinstance(orgs, list):
                        for org in orgs:
                            if isinstance(org, dict):
                                org["category"] = category
                                organizations.append(org)
            elif isinstance(data, list):
                organizations = [org for org in data if isinstance(org, dict)]
            
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
            if test_mode and test_count:
                organizations = organizations[:test_count]
            
            # progress_callback ì €ì¥
            self.progress_callback = progress_callback
            
            # AI Agentic Workflowë¡œ ì²˜ë¦¬
            results = await self.process_organizations_with_callback(organizations)
            
            return results
            
        except Exception as e:
            self.logger.error(f"JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
UnifiedCrawler = EnhancedUnifiedCrawler

# í¸ì˜ í•¨ìˆ˜ë“¤
async def crawl_with_ai_agents(input_file: str, options: Dict = None) -> List[Dict]:
    """AI ì—ì´ì „íŠ¸ë¡œ í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        data = FileUtils.load_json(input_file)
        if not data:
            raise ValueError(f"íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        
        crawler = EnhancedUnifiedCrawler()
        results = await crawler.process_organizations_with_callback(data)
        
        return results
        
    except Exception as e:
        logging.error(f"AI ì—ì´ì „íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¤– AI Agentic Workflow í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*80)
    
    try:
        # í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
        initialize_project()
        
        # AI ì—ì´ì „íŠ¸ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        latest_file = get_latest_input_file()
        if latest_file:
            results = await crawl_with_ai_agents(str(latest_file))
            
            if results:
                print(f"\nâœ… AI ì—ì´ì „íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ì¡°ì§ ì²˜ë¦¬")
            else:
                print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
        else:
            print("\nâŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main())