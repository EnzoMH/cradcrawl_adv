#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Agentic Workflow ê¸°ë°˜ í†µí•© í¬ë¡¤ë§ ì—”ì§„
ê¸°ì¡´ ëª¨ë“  ëª¨ë“ˆì„ í†µí•©í•˜ê³  AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì ìš©
"""

import asyncio
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

# í”„ë¡œì íŠ¸ ì„¤ì • import
from settings import *
from utils.logger_utils import LoggerUtils
from utils.file_utils import FileUtils
from utils.phone_utils import PhoneUtils
from utils.crawler_utils import CrawlerUtils
from ai_helpers import AIModelManager

class CrawlingStage(Enum):
    """í¬ë¡¤ë§ ë‹¨ê³„ ì •ì˜"""
    INITIALIZATION = "ì´ˆê¸°í™”"
    HOMEPAGE_SEARCH = "í™ˆí˜ì´ì§€_ê²€ìƒ‰"
    HOMEPAGE_ANALYSIS = "í™ˆí˜ì´ì§€_ë¶„ì„"
    CONTACT_EXTRACTION = "ì—°ë½ì²˜_ì¶”ì¶œ"
    FAX_SEARCH = "íŒ©ìŠ¤_ê²€ìƒ‰"
    AI_VERIFICATION = "AI_ê²€ì¦"
    DATA_VALIDATION = "ë°ì´í„°_ê²€ì¦"
    COMPLETION = "ì™„ë£Œ"

@dataclass
class CrawlingContext:
    """í¬ë¡¤ë§ ì»¨í…ìŠ¤íŠ¸ (AI Agent ê°„ ê³µìœ  ë°ì´í„°)"""
    organization: Dict[str, Any]
    current_stage: CrawlingStage
    extracted_data: Dict[str, Any]
    ai_insights: Dict[str, Any]
    error_log: List[str]
    processing_time: float
    confidence_scores: Dict[str, float]

class AIAgent:
    """AI ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, ai_manager: AIModelManager, logger: logging.Logger):
        self.name = name
        self.ai_manager = ai_manager
        self.logger = logger
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    async def should_execute(self, context: CrawlingContext) -> bool:
        """ì‹¤í–‰ ì¡°ê±´ í™•ì¸"""
        return True
    
    def update_confidence(self, context: CrawlingContext, field: str, score: float):
        """ì‹ ë¢°ë„ ì ìˆ˜ ì—…ë°ì´íŠ¸"""
        context.confidence_scores[field] = score

class HomepageSearchAgent(AIAgent):
    """í™ˆí˜ì´ì§€ ê²€ìƒ‰ AI ì—ì´ì „íŠ¸"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger):
        super().__init__("HomepageSearchAgent", ai_manager, logger)
        self.crawler_utils = CrawlerUtils()
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """í™ˆí˜ì´ì§€ URL ê²€ìƒ‰ ë° ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ” [{self.name}] í™ˆí˜ì´ì§€ ê²€ìƒ‰: {org_name}")
            
            # ê¸°ì¡´ í™ˆí˜ì´ì§€ê°€ ìˆìœ¼ë©´ ê²€ì¦
            existing_homepage = context.organization.get('homepage', '')
            if existing_homepage:
                is_valid = await self._verify_homepage_with_ai(existing_homepage, org_name)
                if is_valid:
                    context.extracted_data['homepage'] = existing_homepage
                    self.update_confidence(context, 'homepage', 0.9)
                    return context
            
            # ìƒˆë¡œìš´ í™ˆí˜ì´ì§€ ê²€ìƒ‰
            homepage_url = await self._search_homepage(org_name)
            if homepage_url:
                # AIë¡œ ê²€ìƒ‰ ê²°ê³¼ ê²€ì¦
                is_relevant = await self._verify_homepage_with_ai(homepage_url, org_name)
                if is_relevant:
                    context.extracted_data['homepage'] = homepage_url
                    self.update_confidence(context, 'homepage', 0.8)
                    self.logger.info(f"âœ… í™ˆí˜ì´ì§€ ë°œê²¬: {homepage_url}")
                else:
                    self.logger.warning(f"âŒ AI ê²€ì¦ ì‹¤íŒ¨: {homepage_url}")
            
            context.current_stage = CrawlingStage.HOMEPAGE_ANALYSIS
            return context
            
        except Exception as e:
            context.error_log.append(f"HomepageSearchAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _search_homepage(self, org_name: str) -> Optional[str]:
        """êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ í™ˆí˜ì´ì§€ ì°¾ê¸°"""
        driver = None
        try:
            driver = self.crawler_utils.setup_driver(headless=True)
            if not driver:
                return None
            
            search_query = f"{org_name} í™ˆí˜ì´ì§€ site:*.kr OR site:*.com"
            success = self.crawler_utils.search_google(driver, search_query)
            
            if success:
                urls = self.crawler_utils.extract_urls_from_page(driver)
                # ì²« ë²ˆì§¸ ìœ íš¨í•œ URL ë°˜í™˜
                for url in urls[:5]:  # ìƒìœ„ 5ê°œë§Œ í™•ì¸
                    if self._is_valid_homepage_url(url):
                        return url
            
            return None
            
        except Exception as e:
            self.logger.error(f"í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
        finally:
            if driver:
                self.crawler_utils.safe_close_driver(driver)
    
    async def _verify_homepage_with_ai(self, url: str, org_name: str) -> bool:
        """AIë¡œ í™ˆí˜ì´ì§€ ê´€ë ¨ì„± ê²€ì¦"""
        try:
            prompt = f"""
            ê¸°ê´€ëª…: {org_name}
            í™ˆí˜ì´ì§€ URL: {url}
            
            ìœ„ URLì´ í•´ë‹¹ ê¸°ê´€ì˜ ê³µì‹ í™ˆí˜ì´ì§€ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
            
            íŒë‹¨ ê¸°ì¤€:
            1. ë„ë©”ì¸ëª…ì´ ê¸°ê´€ëª…ê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€?
            2. URL êµ¬ì¡°ê°€ ê³µì‹ ì‚¬ì´íŠ¸ ê°™ì€ê°€?
            3. ë¸”ë¡œê·¸, ì¹´í˜, ìœ„í‚¤ ë“±ì´ ì•„ë‹Œ ê³µì‹ ì‚¬ì´íŠ¸ì¸ê°€?
            
            ê²°ê³¼: "ê´€ë ¨ìˆìŒ" ë˜ëŠ” "ê´€ë ¨ì—†ìŒ"
            ì´ìœ : [íŒë‹¨ ì´ìœ ]
            """
            
            response = await self.ai_manager.extract_with_gemini(url, prompt)
            return "ê´€ë ¨ìˆìŒ" in response
            
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return False
    
    def _is_valid_homepage_url(self, url: str) -> bool:
        """URL ìœ íš¨ì„± ê¸°ë³¸ ê²€ì¦"""
        if not url or not url.startswith(('http://', 'https://')):
            return False
        
        # ì œì™¸ ë„ë©”ì¸ í™•ì¸
        for exclude_domain in EXCLUDE_DOMAINS:
            if exclude_domain in url.lower():
                return False
        
        return True

class HomepageAnalysisAgent(AIAgent):
    """í™ˆí˜ì´ì§€ ë¶„ì„ AI ì—ì´ì „íŠ¸ (urlextractor_2.py ë¡œì§ í†µí•©)"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger):
        super().__init__("HomepageAnalysisAgent", ai_manager, logger)
        self.crawler_utils = CrawlerUtils()
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """í™ˆí˜ì´ì§€ ë‚´ìš© ë¶„ì„ ë° ì •ë³´ ì¶”ì¶œ"""
        try:
            homepage_url = context.extracted_data.get('homepage')
            if not homepage_url:
                context.current_stage = CrawlingStage.CONTACT_EXTRACTION
                return context
            
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸŒ [{self.name}] í™ˆí˜ì´ì§€ ë¶„ì„: {homepage_url}")
            
            # í™ˆí˜ì´ì§€ íŒŒì‹± (urlextractor_2.py ë¡œì§ í™œìš©)
            page_data = await self._extract_page_content(homepage_url)
            
            if page_data and page_data.get('accessible'):
                # AIë¡œ í˜ì´ì§€ ë‚´ìš© ìš”ì•½ ë° ì •ë³´ ì¶”ì¶œ
                ai_summary = await self._analyze_with_ai(org_name, page_data)
                
                # ì¶”ì¶œëœ ì—°ë½ì²˜ ì •ë³´ ì €ì¥
                contact_info = page_data.get('contact_info', {})
                if contact_info.get('phones'):
                    context.extracted_data['phone'] = contact_info['phones'][0]
                    self.update_confidence(context, 'phone', 0.9)
                
                if contact_info.get('emails'):
                    context.extracted_data['email'] = contact_info['emails'][0]
                    self.update_confidence(context, 'email', 0.9)
                
                if contact_info.get('addresses'):
                    context.extracted_data['address'] = contact_info['addresses'][0]
                    self.update_confidence(context, 'address', 0.8)
                
                # AI ë¶„ì„ ê²°ê³¼ ì €ì¥
                context.ai_insights['homepage_analysis'] = ai_summary
                context.extracted_data['page_title'] = page_data.get('title', '')
                
                self.logger.info(f"âœ… í™ˆí˜ì´ì§€ ë¶„ì„ ì™„ë£Œ: {len(contact_info)} ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ")
            
            context.current_stage = CrawlingStage.CONTACT_EXTRACTION
            return context
            
        except Exception as e:
            context.error_log.append(f"HomepageAnalysisAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _extract_page_content(self, url: str) -> Dict[str, Any]:
        """í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ (urlextractor_2.pyì˜ HomepageParser ë¡œì§)"""
        driver = None
        try:
            driver = self.crawler_utils.setup_driver(headless=True)
            if not driver:
                return {}
            
            # í˜ì´ì§€ ë¡œë“œ
            driver.get(url)
            time.sleep(3)  # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
            
            # ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸
            if not self._is_page_accessible(driver):
                return {'accessible': False}
            
            # ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
            page_text = driver.find_element(By.TAG_NAME, "body").text
            contact_info = self._extract_contact_info(page_text)
            
            return {
                'accessible': True,
                'title': driver.title,
                'url': url,
                'text_content': page_text[:5000],  # ì²˜ìŒ 5000ìë§Œ
                'contact_info': contact_info
            }
            
        except Exception as e:
            self.logger.error(f"í˜ì´ì§€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {'accessible': False, 'error': str(e)}
        finally:
            if driver:
                self.crawler_utils.safe_close_driver(driver)
    
    def _is_page_accessible(self, driver) -> bool:
        """í˜ì´ì§€ ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            title = driver.title.lower()
            if any(keyword in title for keyword in ['404', 'not found', 'error']):
                return False
            
            page_source = driver.page_source
            return len(page_source) > 1000
            
        except:
            return False
    
    def _extract_contact_info(self, text: str) -> Dict[str, List[str]]:
        """ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        contact_info = {
            "phones": [],
            "faxes": [],
            "emails": [],
            "addresses": []
        }
        
        try:
            # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in PHONE_EXTRACTION_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    phone = PhoneUtils.format_phone_number(match)
                    if phone and phone not in contact_info["phones"]:
                        contact_info["phones"].append(phone)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in FAX_EXTRACTION_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    fax = PhoneUtils.format_phone_number(match)
                    if fax and fax not in contact_info["faxes"]:
                        contact_info["faxes"].append(fax)
            
            # ì´ë©”ì¼ ì¶”ì¶œ
            for pattern in EMAIL_EXTRACTION_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if self._is_valid_email(match) and match not in contact_info["emails"]:
                        contact_info["emails"].append(match)
            
            # ì£¼ì†Œ ì¶”ì¶œ
            for pattern in ADDRESS_EXTRACTION_PATTERNS:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    clean_address = self._clean_address(match)
                    if clean_address and clean_address not in contact_info["addresses"]:
                        contact_info["addresses"].append(clean_address)
        
        except Exception as e:
            self.logger.warning(f"ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return contact_info
    
    def _is_valid_email(self, email: str) -> bool:
        """ì´ë©”ì¼ ìœ íš¨ì„± í™•ì¸"""
        import re
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return re.match(email_pattern, email) is not None
    
    def _clean_address(self, address: str) -> Optional[str]:
        """ì£¼ì†Œ ì •ë¦¬"""
        if not address or len(address) < 10:
            return None
        return re.sub(r'\s+', ' ', address.strip())
    
    async def _analyze_with_ai(self, org_name: str, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """AIë¡œ í˜ì´ì§€ ë‚´ìš© ë¶„ì„ (urlextractor_2.pyì˜ summarize_with_ai ë¡œì§)"""
        try:
            prompt = f"""
            '{org_name}' ê¸°ê´€ì˜ í™ˆí˜ì´ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

            **ê¸°ë³¸ ì •ë³´:**
            - ì œëª©: {page_data.get('title', '')}
            - URL: {page_data.get('url', '')}

            **í™ˆí˜ì´ì§€ ë‚´ìš©:**
            {page_data.get('text_content', '')[:2000]}

            **ì¶”ì¶œëœ ì—°ë½ì²˜:**
            - ì „í™”ë²ˆí˜¸: {', '.join(page_data.get('contact_info', {}).get('phones', []))}
            - íŒ©ìŠ¤ë²ˆí˜¸: {', '.join(page_data.get('contact_info', {}).get('faxes', []))}
            - ì´ë©”ì¼: {', '.join(page_data.get('contact_info', {}).get('emails', []))}
            - ì£¼ì†Œ: {', '.join(page_data.get('contact_info', {}).get('addresses', []))}

            **ë¶„ì„ ìš”ì²­:**
            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
            
            SUMMARY: [ê¸°ê´€ ìš”ì•½ 3-4ë¬¸ì¥]
            CATEGORY: [ê¸°ê´€ ìœ í˜• - êµíšŒ, ë³‘ì›, í•™êµ, ì •ë¶€ê¸°ê´€, ê¸°ì—…, ë‹¨ì²´ ë“±]
            SERVICES: [ì£¼ìš” ì„œë¹„ìŠ¤ 3ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„]
            LOCATION: [ìœ„ì¹˜ì •ë³´]
            CONTACT_QUALITY: [ìƒ/ì¤‘/í•˜]
            WEBSITE_QUALITY: [ìƒ/ì¤‘/í•˜]
            FEATURES: [íŠ¹ë³„í•œ íŠ¹ì§• 2ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„]
            """
            
            response = await self.ai_manager.extract_with_gemini(page_data.get('text_content', ''), prompt)
            return self._parse_ai_response(response)
            
        except Exception as e:
            self.logger.error(f"AI ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'summary': f'AI ë¶„ì„ ì˜¤ë¥˜: {str(e)}'}
    
    def _parse_ai_response(self, response: str) -> Dict[str, Any]:
        """AI ì‘ë‹µ íŒŒì‹±"""
        result = {
            "summary": "",
            "category": "ê¸°íƒ€",
            "services": [],
            "location": "",
            "contact_quality": "ì¤‘",
            "website_quality": "ì¤‘",
            "features": []
        }
        
        try:
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('SUMMARY:'):
                    result["summary"] = line.replace('SUMMARY:', '').strip()
                elif line.startswith('CATEGORY:'):
                    result["category"] = line.replace('CATEGORY:', '').strip()
                elif line.startswith('SERVICES:'):
                    services_text = line.replace('SERVICES:', '').strip()
                    result["services"] = [s.strip() for s in services_text.split(',')]
                elif line.startswith('LOCATION:'):
                    result["location"] = line.replace('LOCATION:', '').strip()
                elif line.startswith('CONTACT_QUALITY:'):
                    result["contact_quality"] = line.replace('CONTACT_QUALITY:', '').strip()
                elif line.startswith('WEBSITE_QUALITY:'):
                    result["website_quality"] = line.replace('WEBSITE_QUALITY:', '').strip()
                elif line.startswith('FEATURES:'):
                    features_text = line.replace('FEATURES:', '').strip()
                    result["features"] = [f.strip() for f in features_text.split(',')]
        
        except Exception as e:
            self.logger.warning(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return result

class FaxSearchAgent(AIAgent):
    """íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ AI ì—ì´ì „íŠ¸ (faxextractor.py ë¡œì§ í†µí•©)"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger):
        super().__init__("FaxSearchAgent", ai_manager, logger)
        self.crawler_utils = CrawlerUtils()
    
    async def should_execute(self, context: CrawlingContext) -> bool:
        """íŒ©ìŠ¤ë²ˆí˜¸ê°€ ì—†ì„ ë•Œë§Œ ì‹¤í–‰"""
        return not context.extracted_data.get('fax')
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ íŒ©ìŠ¤ë²ˆí˜¸ ì°¾ê¸°"""
        try:
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ“  [{self.name}] íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰: {org_name}")
            
            fax_number = await self._search_fax_number(org_name)
            if fax_number:
                # ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µ í™•ì¸
                phone = context.extracted_data.get('phone', '')
                if fax_number != phone:
                    context.extracted_data['fax'] = fax_number
                    self.update_confidence(context, 'fax', 0.7)
                    self.logger.info(f"âœ… íŒ©ìŠ¤ë²ˆí˜¸ ë°œê²¬: {fax_number}")
                else:
                    self.logger.info(f"âš ï¸ ì „í™”ë²ˆí˜¸ì™€ ë™ì¼í•œ íŒ©ìŠ¤ë²ˆí˜¸: {fax_number}")
            
            context.current_stage = CrawlingStage.AI_VERIFICATION
            return context
            
        except Exception as e:
            context.error_log.append(f"FaxSearchAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _search_fax_number(self, org_name: str) -> Optional[str]:
        """êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ íŒ©ìŠ¤ë²ˆí˜¸ ì°¾ê¸° (faxextractor.py ë¡œì§)"""
        driver = None
        try:
            driver = self.crawler_utils.setup_driver(headless=True)
            if not driver:
                return None
            
            search_query = f"{org_name} íŒ©ìŠ¤ë²ˆí˜¸"
            success = self.crawler_utils.search_google(driver, search_query)
            
            if success:
                page_text = driver.find_element(By.TAG_NAME, "body").text
                fax_numbers = self._extract_fax_numbers(page_text)
                
                if fax_numbers:
                    return fax_numbers[0]  # ì²« ë²ˆì§¸ ë°œê²¬ëœ íŒ©ìŠ¤ë²ˆí˜¸
            
            return None
            
        except Exception as e:
            self.logger.error(f"íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
        finally:
            if driver:
                self.crawler_utils.safe_close_driver(driver)
    
    def _extract_fax_numbers(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ"""
        fax_numbers = []
        
        for pattern in FAX_EXTRACTION_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                fax = PhoneUtils.format_phone_number(match)
                if fax and fax not in fax_numbers:
                    fax_numbers.append(fax)
        
        return fax_numbers

class AIVerificationAgent(AIAgent):
    """AI ê²€ì¦ ì—ì´ì „íŠ¸"""
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì¶”ì¶œëœ ë°ì´í„°ë¥¼ AIë¡œ ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ¤– [{self.name}] AI ê²€ì¦: {org_name}")
            
            # ì¶”ì¶œëœ ë°ì´í„° ê²€ì¦
            verification_result = await self._verify_extracted_data(context)
            context.ai_insights['verification'] = verification_result
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ì¡°ì •
            self._adjust_confidence_scores(context, verification_result)
            
            context.current_stage = CrawlingStage.DATA_VALIDATION
            return context
            
        except Exception as e:
            context.error_log.append(f"AIVerificationAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _verify_extracted_data(self, context: CrawlingContext) -> Dict[str, Any]:
        """ì¶”ì¶œëœ ë°ì´í„° AI ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            extracted = context.extracted_data
            
            prompt = f"""
            ê¸°ê´€ëª…: {org_name}
            ì¶”ì¶œëœ ì •ë³´ë¥¼ ê²€ì¦í•´ì£¼ì„¸ìš”:
            
            - í™ˆí˜ì´ì§€: {extracted.get('homepage', 'ì—†ìŒ')}
            - ì „í™”ë²ˆí˜¸: {extracted.get('phone', 'ì—†ìŒ')}
            - íŒ©ìŠ¤ë²ˆí˜¸: {extracted.get('fax', 'ì—†ìŒ')}
            - ì´ë©”ì¼: {extracted.get('email', 'ì—†ìŒ')}
            - ì£¼ì†Œ: {extracted.get('address', 'ì—†ìŒ')}
            
            ê° ì •ë³´ê°€ í•´ë‹¹ ê¸°ê´€ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨í•˜ê³ , 
            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
            
            HOMEPAGE_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            PHONE_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            FAX_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            EMAIL_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            ADDRESS_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            OVERALL_CONFIDENCE: [ë†’ìŒ/ë³´í†µ/ë‚®ìŒ]
            ISSUES: [ë°œê²¬ëœ ë¬¸ì œì ë“¤]
            """
            
            response = await self.ai_manager.extract_with_gemini("", prompt)
            return self._parse_verification_response(response)
            
        except Exception as e:
            self.logger.error(f"AI ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {'overall_confidence': 'ë‚®ìŒ', 'issues': [str(e)]}
    
    def _parse_verification_response(self, response: str) -> Dict[str, Any]:
        """AI ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        result = {
            'homepage_valid': False,
            'phone_valid': False,
            'fax_valid': False,
            'email_valid': False,
            'address_valid': False,
            'overall_confidence': 'ë³´í†µ',
            'issues': []
        }
        
        try:
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('HOMEPAGE_VALID:'):
                    result['homepage_valid'] = 'ì˜ˆ' in line
                elif line.startswith('PHONE_VALID:'):
                    result['phone_valid'] = 'ì˜ˆ' in line
                elif line.startswith('FAX_VALID:'):
                    result['fax_valid'] = 'ì˜ˆ' in line
                elif line.startswith('EMAIL_VALID:'):
                    result['email_valid'] = 'ì˜ˆ' in line
                elif line.startswith('ADDRESS_VALID:'):
                    result['address_valid'] = 'ì˜ˆ' in line
                elif line.startswith('OVERALL_CONFIDENCE:'):
                    confidence = line.replace('OVERALL_CONFIDENCE:', '').strip()
                    result['overall_confidence'] = confidence
                elif line.startswith('ISSUES:'):
                    issues_text = line.replace('ISSUES:', '').strip()
                    if issues_text and issues_text != 'ì—†ìŒ':
                        result['issues'] = [issues_text]
        
        except Exception as e:
            self.logger.warning(f"ê²€ì¦ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return result
    
    def _adjust_confidence_scores(self, context: CrawlingContext, verification: Dict[str, Any]):
        """ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ì‹ ë¢°ë„ ì ìˆ˜ ì¡°ì •"""
        adjustments = {
            'homepage': verification.get('homepage_valid', False),
            'phone': verification.get('phone_valid', False),
            'fax': verification.get('fax_valid', False),
            'email': verification.get('email_valid', False),
            'address': verification.get('address_valid', False)
        }
        
        for field, is_valid in adjustments.items():
            if field in context.confidence_scores:
                if is_valid:
                    context.confidence_scores[field] = min(1.0, context.confidence_scores[field] + 0.1)
                else:
                    context.confidence_scores[field] = max(0.0, context.confidence_scores[field] - 0.2)

class DataValidationAgent(AIAgent):
    """ë°ì´í„° ê²€ì¦ ì—ì´ì „íŠ¸ (phone_utils.py ë¡œì§ í™œìš©)"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger):
        super().__init__("DataValidationAgent", ai_manager, logger)
        self.phone_utils = PhoneUtils()
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ìµœì¢… ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬"""
        try:
            self.logger.info(f"âœ… [{self.name}] ë°ì´í„° ê²€ì¦ ì‹œì‘")
            
            # ì „í™”ë²ˆí˜¸ ê²€ì¦
            phone = context.extracted_data.get('phone')
            if phone:
                if self.phone_utils.validate_korean_phone(phone):
                    context.extracted_data['phone'] = self.phone_utils.format_phone_number(phone)
                else:
                    context.error_log.append(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì „í™”ë²ˆí˜¸: {phone}")
                    context.extracted_data.pop('phone', None)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦
            fax = context.extracted_data.get('fax')
            if fax:
                if self.phone_utils.validate_korean_phone(fax):
                    context.extracted_data['fax'] = self.phone_utils.format_phone_number(fax)
                else:
                    context.error_log.append(f"ìœ íš¨í•˜ì§€ ì•Šì€ íŒ©ìŠ¤ë²ˆí˜¸: {fax}")
                    context.extracted_data.pop('fax', None)
            
            # ì „í™”ë²ˆí˜¸-íŒ©ìŠ¤ë²ˆí˜¸ ì¤‘ë³µ í™•ì¸
            if phone and fax and self.phone_utils.is_phone_fax_duplicate(phone, fax):
                context.error_log.append("ì „í™”ë²ˆí˜¸ì™€ íŒ©ìŠ¤ë²ˆí˜¸ê°€ ì¤‘ë³µë¨")
                context.extracted_data.pop('fax', None)
            
            # ì´ë©”ì¼ ê²€ì¦
            email = context.extracted_data.get('email')
            if email and not self._is_valid_email(email):
                context.error_log.append(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë©”ì¼: {email}")
                context.extracted_data.pop('email', None)
            
            context.current_stage = CrawlingStage.COMPLETION
            return context
            
        except Exception as e:
            context.error_log.append(f"DataValidationAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    def _is_valid_email(self, email: str) -> bool:
        """ì´ë©”ì¼ ìœ íš¨ì„± í™•ì¸"""
        import re
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return re.match(email_pattern, email) is not None

class EnhancedUnifiedCrawler:
    """AI Agentic Workflow ê¸°ë°˜ ê°•í™”ëœ í†µí•© í¬ë¡¤ëŸ¬"""
    
    def __init__(self, config_override=None):
        """ì´ˆê¸°í™”"""
        self.config = config_override or CRAWLING_CONFIG
        self.logger = LoggerUtils.setup_crawler_logger()
        self.ai_manager = AIModelManager()
        
        # AI ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™”
        self.agents = [
            HomepageSearchAgent(self.ai_manager, self.logger),
            HomepageAnalysisAgent(self.ai_manager, self.logger),
            FaxSearchAgent(self.ai_manager, self.logger),
            AIVerificationAgent(self.ai_manager, self.logger),
            DataValidationAgent(self.ai_manager, self.logger)
        ]
        
        # í†µê³„ ì •ë³´
        self.stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "start_time": None,
            "end_time": None,
            "agent_stats": {agent.name: {"executed": 0, "success": 0} for agent in self.agents}
        }
        
        self.logger.info("ğŸš€ AI Agentic Workflow í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def process_organizations_with_callback(self, organizations: List[Dict]) -> List[Dict]:
        """AI Agentic Workflowë¡œ ì¡°ì§ ì²˜ë¦¬"""
        if not organizations:
            return []
        
        self.stats["start_time"] = datetime.now()
        self.stats["total_processed"] = len(organizations)
        
        results = []
        
        for i, org in enumerate(organizations, 1):
            try:
                # í¬ë¡¤ë§ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
                context = CrawlingContext(
                    organization=org,
                    current_stage=CrawlingStage.INITIALIZATION,
                    extracted_data={},
                    ai_insights={},
                    error_log=[],
                    processing_time=0,
                    confidence_scores={}
                )
                
                start_time = time.time()
                
                # AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                context = await self._execute_agent_workflow(context, i)
                
                processing_time = time.time() - start_time
                context.processing_time = processing_time
                
                # ê²°ê³¼ ì¡°í•©
                processed_org = self._combine_results(org, context)
                results.append(processed_org)
                
                # ì½œë°± í˜¸ì¶œ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
                if hasattr(self, 'progress_callback') and self.progress_callback:
                    self._call_progress_callback(processed_org, i, len(organizations), processing_time)
                
                self.stats["successful"] += 1
                
                # ë”œë ˆì´
                await asyncio.sleep(self.config.get("default_delay", 2))
                
            except Exception as e:
                self.logger.error(f"âŒ ì¡°ì§ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {org.get('name', 'Unknown')} - {e}")
                self.stats["failed"] += 1
                results.append(org)
        
        self.stats["end_time"] = datetime.now()
        self._print_agent_statistics()
        
        return results
    
    async def _execute_agent_workflow(self, context: CrawlingContext, index: int) -> CrawlingContext:
        """AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        org_name = context.organization.get('name', 'Unknown')
        self.logger.info(f"ğŸ¤– AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹œì‘ [{index}]: {org_name}")
        
        for agent in self.agents:
            try:
                # ì‹¤í–‰ ì¡°ê±´ í™•ì¸
                if await agent.should_execute(context):
                    self.logger.info(f"ğŸ”„ ì—ì´ì „íŠ¸ ì‹¤í–‰: {agent.name}")
                    
                    # ì—ì´ì „íŠ¸ ì‹¤í–‰
                    context = await agent.execute(context)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.stats["agent_stats"][agent.name]["executed"] += 1
                    if not context.error_log or len(context.error_log) == 0:
                        self.stats["agent_stats"][agent.name]["success"] += 1
                    
                    self.logger.info(f"âœ… ì—ì´ì „íŠ¸ ì™„ë£Œ: {agent.name}")
                else:
                    self.logger.info(f"â­ï¸ ì—ì´ì „íŠ¸ ìŠ¤í‚µ: {agent.name}")
                
            except Exception as e:
                self.logger.error(f"âŒ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜ {agent.name}: {e}")
                context.error_log.append(f"{agent.name} ì˜¤ë¥˜: {str(e)}")
        
        self.logger.info(f"ğŸ‰ AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: {org_name}")
        return context
    
    def _combine_results(self, original_org: Dict, context: CrawlingContext) -> Dict:
        """ì›ë³¸ ì¡°ì§ ë°ì´í„°ì™€ ì¶”ì¶œëœ ë°ì´í„° ê²°í•©"""
        result = original_org.copy()
        
        # ì¶”ì¶œëœ ë°ì´í„° ì¶”ê°€
        result.update(context.extracted_data)
        
        # AI ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
        result['ai_insights'] = context.ai_insights
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ì¶”ê°€
        result['confidence_scores'] = context.confidence_scores
        
        # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result['processing_metadata'] = {
            'processing_time': context.processing_time,
            'final_stage': context.current_stage.value,
            'error_count': len(context.error_log),
            'errors': context.error_log,
            'extraction_method': 'ai_agentic_workflow',
            'timestamp': datetime.now().isoformat()
        }
        
        return result
    
    def _call_progress_callback(self, processed_org: Dict, current: int, total: int, processing_time: float):
        """ì§„í–‰ ìƒí™© ì½œë°± í˜¸ì¶œ"""
        try:
            callback_data = {
                'name': processed_org.get('name', 'Unknown'),
                'category': processed_org.get('category', ''),
                'homepage_url': processed_org.get('homepage', ''),
                'status': 'completed',
                'current_step': f'{current}/{total}',
                'processing_time': processing_time,
                'phone': processed_org.get('phone', ''),
                'fax': processed_org.get('fax', ''),
                'email': processed_org.get('email', ''),
                'address': processed_org.get('address', ''),
                'extraction_method': 'ai_agentic_workflow',
                'confidence_scores': processed_org.get('confidence_scores', {}),
                'ai_insights': processed_org.get('ai_insights', {})
            }
            self.progress_callback(callback_data)
            
        except Exception as e:
            self.logger.error(f"ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    def _print_agent_statistics(self):
        """ì—ì´ì „íŠ¸ í†µê³„ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ¤– AI Agentic Workflow í†µê³„")
        print("="*80)
        
        for agent_name, stats in self.stats["agent_stats"].items():
            executed = stats["executed"]
            success = stats["success"]
            success_rate = (success / executed * 100) if executed > 0 else 0
            print(f"ğŸ”§ {agent_name:<25} ì‹¤í–‰: {executed:>3}íšŒ, ì„±ê³µ: {success:>3}íšŒ ({success_rate:>5.1f}%)")
        
        duration = self.stats["end_time"] - self.stats["start_time"]
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  ğŸ“‹ ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"  âœ… ì„±ê³µ: {self.stats['successful']}ê°œ")
        print(f"  âŒ ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
        print(f"  ğŸ“ˆ ì„±ê³µë¥ : {(self.stats['successful']/self.stats['total_processed']*100):.1f}%")
        print(f"  â±ï¸ ì†Œìš”ì‹œê°„: {duration}")
        print("="*80)

    # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œë“¤
    async def process_json_file_async(self, json_file_path: str, test_mode: bool = False, test_count: int = 10, progress_callback=None) -> List[Dict]:
        """app.py í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ"""
        try:
            # JSON íŒŒì¼ ë¡œë“œ
            data = FileUtils.load_json(json_file_path)
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            organizations = []
            if isinstance(data, dict):
                for category, orgs in data.items():
                    if isinstance(orgs, list):
                        for org in orgs:
                            if isinstance(org, dict):
                                org["category"] = category
                                organizations.append(org)
            elif isinstance(data, list):
                organizations = [org for org in data if isinstance(org, dict)]
            
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
            if test_mode and test_count:
                organizations = organizations[:test_count]
            
            # progress_callback ì €ì¥
            self.progress_callback = progress_callback
            
            # AI Agentic Workflowë¡œ ì²˜ë¦¬
            results = await self.process_organizations_with_callback(organizations)
            
            return results
            
        except Exception as e:
            self.logger.error(f"JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
UnifiedCrawler = EnhancedUnifiedCrawler

# í¸ì˜ í•¨ìˆ˜ë“¤
async def crawl_with_ai_agents(input_file: str, options: Dict = None) -> List[Dict]:
    """AI ì—ì´ì „íŠ¸ë¡œ í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        data = FileUtils.load_json(input_file)
        if not data:
            raise ValueError(f"íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        
        crawler = EnhancedUnifiedCrawler()
        results = await crawler.process_organizations_with_callback(data)
        
        return results
        
    except Exception as e:
        logging.error(f"AI ì—ì´ì „íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¤– AI Agentic Workflow í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*80)
    
    try:
        # í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
        initialize_project()
        
        # AI ì—ì´ì „íŠ¸ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        latest_file = get_latest_input_file()
        if latest_file:
            results = await crawl_with_ai_agents(str(latest_file))
            
            if results:
                print(f"\nâœ… AI ì—ì´ì „íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ì¡°ì§ ì²˜ë¦¬")
            else:
                print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
        else:
            print("\nâŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main())