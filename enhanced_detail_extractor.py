#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Detail Extractor Library
JSON íŒŒì¼ì˜ ê¸°ì¡´ ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ê³ , í•„ìš”ì‹œì—ë§Œ í™ˆí˜ì´ì§€ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
TPM ê´€ë¦¬, ë§ˆí¬ë‹¤ìš´ íŒŒì‹±, ë°ì´í„° ë³‘í•©, ì¢…í•© ë¦¬í¬íŠ¸ ê¸°ëŠ¥ í¬í•¨
"""

import json
import re
import time
import asyncio
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import os
import sys
from urllib.parse import urljoin, urlparse
import warnings
warnings.filterwarnings('ignore')

import logging
import random

import ssl
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Selenium imports ì¶”ê°€
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException

# enhanced_detail_extractor.pyì—ì„œ ì‚¬ìš©
from utils.phone_utils import PhoneUtils
from utils.file_utils import FileUtils
from utils.crawler_utils import CrawlerUtils
from utils.logger_utils import LoggerUtils

# constants.pyì—ì„œ ì„í¬íŠ¸
from constants import (
    PHONE_EXTRACTION_PATTERNS,
    FAX_EXTRACTION_PATTERNS,
    EMAIL_EXTRACTION_PATTERNS,
    ADDRESS_EXTRACTION_PATTERNS,
    LOGGER_NAMES,
    LOG_FORMAT,
    AI_MODEL_CONFIG,
    TPM_LIMITS,
    EXCLUDE_DOMAINS
)

from ai_helpers import AIModelManager, extract_with_gemini_text

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# validator.pyì˜ ContactValidator ì„í¬íŠ¸
from validator import ContactValidator

# Gemini API ì„¤ì •
import google.generativeai as genai

class TPMManager:
    """TPM(Tokens Per Minute) ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, requests_per_minute=TPM_LIMITS["requests_per_minute"]):
        """TPM ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.requests_per_minute = requests_per_minute
        self.request_times = []
        self.max_wait_time = TPM_LIMITS["max_wait_time"]
    
    async def wait_if_needed(self):
        """ê°œì„ ëœ TPM ì œí•œ ëŒ€ê¸° (ìµœëŒ€ ëŒ€ê¸°ì‹œê°„ ì œí•œ)"""
        now = datetime.now()
        
        # 1ë¶„ ì´ë‚´ì˜ ìš”ì²­ë§Œ ìœ ì§€
        self.request_times = [t for t in self.request_times if now - t < timedelta(minutes=1)]
        
        if len(self.request_times) >= self.requests_per_minute:
            wait_time = 60 - (now - self.request_times[0]).total_seconds()
            # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ ì œí•œ
            wait_time = min(wait_time, self.max_wait_time)
            
            if wait_time > 0:
                print(f"â³ TPM ì œí•œìœ¼ë¡œ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                await asyncio.sleep(wait_time)
        
        self.request_times.append(now)
        
        # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ì„ 5ì´ˆë¡œ ë‹¨ì¶•
        print(f"â±ï¸ API í˜¸ì¶œ ì•ˆì „ ëŒ€ê¸°: 5ì´ˆ")
        await asyncio.sleep(5)

    

class EnhancedDetailExtractor:
    """Gemini 1.5-flash ì œì•½ì‚¬í•­ì„ ê³ ë ¤í•œ ì§€ëŠ¥í˜• ì›¹ ë¶„ì„ ì¶”ì¶œê¸°"""
    
    def __init__(self, api_key=None, use_selenium=True, headless=False, progress_callback=None):
        """ì´ˆê¸°í™” (AI ì œì•½ì‚¬í•­ ê³ ë ¤)"""
        # ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œ...
        self.session = CrawlerUtils.setup_requests_session()
        self.validator = ContactValidator()
        
        # url_extractor.pyì˜ ì„¤ì • ì¶”ê°€
        self.max_search_results = 5
        self.max_retries = 3
        self.delay_range = (3, 6)
        
        # ğŸ”§ ê°œì„ : constants.pyì˜ TPM ì„¤ì • ì‚¬ìš©
        self.tpm_manager = TPMManager(requests_per_minute=TPM_LIMITS["requests_per_minute"])
        
        # ğŸ”§ ê°œì„ : ai_helpers.pyì˜ AIModelManager í™œìš©
        self.ai_model_manager = None
        if api_key or os.getenv('GEMINI_API_KEY'):
            try:
                self.ai_model_manager = AIModelManager()
                self.use_ai = True
                self.use_ai_web_analysis = True
                print("ğŸ¤– AI ëª¨ë¸ ê´€ë¦¬ì ì´ˆê¸°í™” ì„±ê³µ (ai_helpers.py ê¸°ë°˜)")
            except Exception as e:
                print(f"âŒ AI ëª¨ë¸ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.ai_model_manager = None
                self.use_ai = False
                self.use_ai_web_analysis = False
        else:
            self.use_ai = False
            self.use_ai_web_analysis = False
            print("ğŸ”§ ì •ê·œì‹ ì „ìš© ëª¨ë“œ (API í‚¤ ì—†ìŒ)")
        
        # ğŸ†• Gemini 1.5-flash ì œì•½ì‚¬í•­ ê³ ë ¤í•œ ì„¤ì •
        self.max_input_tokens = 30000  # 32K í† í° ì œí•œì˜ ì•ˆì „ ë§ˆì§„
        self.max_html_chars = 60000    # ëŒ€ëµ 30K í† í°ì— í•´ë‹¹ (í•œê¸€ ê¸°ì¤€)
        self.max_text_chars = 15000    # ì—°ë½ì²˜ ì¶”ì¶œìš© í…ìŠ¤íŠ¸ ì œí•œ
        
        # Selenium ê´€ë ¨ ì„¤ì •
        self.use_selenium = use_selenium
        self.headless = headless
        self.driver = None
        self.exclude_domains = EXCLUDE_DOMAINS
        
        # ê¸°ì¡´ í†µê³„ ì •ë³´ì— AI ê´€ë ¨ ì¶”ê°€
        self.stats = {
            'total_processed': 0,
            'json_info_used': 0,
            'crawling_performed': 0,
            'url_search_performed': 0,
            'url_found': 0,
            'phone_found': 0,
            'fax_found': 0,
            'email_found': 0,
            'address_found': 0,
            'postal_code_found': 0,
            'ai_enhanced': 0,
            'duplicates_removed': 0,
            'api_calls_made': 0,
            'token_limit_exceeded': 0,  # ğŸ†• í† í° ì œí•œ ì´ˆê³¼ íšŸìˆ˜
            'tpm_wait_count': 0,        # ğŸ†• TPM ëŒ€ê¸° íšŸìˆ˜
        }
        
        self.ai_logger = LoggerUtils.setup_ai_logger()
        self.patterns = {
            'phone': PHONE_EXTRACTION_PATTERNS,
            'fax': FAX_EXTRACTION_PATTERNS,
            'email': EMAIL_EXTRACTION_PATTERNS,
            'address': ADDRESS_EXTRACTION_PATTERNS
        }
        
        self.progress_callback = progress_callback
        
        self.merged_results = []

    # ğŸ”§ ê°œì„ : í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ í•¨ìˆ˜
    def estimate_token_count(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •"""
        # í•œê¸€/í•œì: 1ê¸€ì â‰ˆ 1.5í† í°, ì˜ë¬¸: 1ë‹¨ì–´ â‰ˆ 1í† í°, íŠ¹ìˆ˜ë¬¸ì ê³ ë ¤
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        other_chars = len(text) - korean_chars - sum(len(word) for word in re.findall(r'[a-zA-Z]+', text))
        
        estimated_tokens = int(korean_chars * 1.5 + english_words + other_chars * 0.5)
        return estimated_tokens

    def truncate_text_by_tokens(self, text: str, max_tokens: int = None) -> str:
        """í† í° ìˆ˜ ì œí•œì— ë§ì¶° í…ìŠ¤íŠ¸ ìë¥´ê¸°"""
        if max_tokens is None:
            max_tokens = self.max_input_tokens
        
        current_tokens = self.estimate_token_count(text)
        
        if current_tokens <= max_tokens:
            return text
        
        # í† í° ìˆ˜ê°€ ì´ˆê³¼í•˜ë©´ ë¹„ìœ¨ë¡œ ìë¥´ê¸°
        ratio = max_tokens / current_tokens * 0.9  # ì•ˆì „ ë§ˆì§„ 10%
        target_length = int(len(text) * ratio)
        
        # ì•ë¶€ë¶„ 70%, ë’·ë¶€ë¶„ 30%ë¡œ ë‚˜ëˆ„ì–´ ì¤‘ìš” ì •ë³´ ë³´ì¡´
        front_length = int(target_length * 0.7)
        back_length = target_length - front_length
        
        if back_length > 0:
            truncated = text[:front_length] + "\n... (ì¤‘ê°„ ìƒëµ) ...\n" + text[-back_length:]
        else:
            truncated = text[:target_length]
        
        self.stats['token_limit_exceeded'] += 1
        print(f"âš ï¸ í† í° ì œí•œìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶•ì†Œ: {current_tokens} â†’ {self.estimate_token_count(truncated)} í† í°")
        
        return truncated

    # ğŸ†• URL ê²€ìƒ‰ ê´€ë ¨ ë©”ì„œë“œë“¤ ì¶”ê°€
    def setup_selenium_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        if not self.use_selenium or self.driver:
            return
        
        try:
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument('--headless')
            
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print(f"ğŸŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ (headless: {self.headless})")
            
        except Exception as e:
            print(f"âŒ ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.use_selenium = False
    
    def close_selenium_driver(self):
        """Selenium WebDriver ì¢…ë£Œ"""
        if self.driver:
            try:
                self.driver.quit()
                self.driver = None
                print("ğŸŒ ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")
            except:
                pass

    # ğŸ†• url_extractor.pyì˜ ì •ì œ ë¡œì§ ì™„ì „ ì´ì‹
    def clean_search_query_advanced(self, organization_name: str, location: str = "") -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ì •ì œ (url_extractor.pyì™€ ë™ì¼í•œ ë¡œì§)"""
        # ê¸°ë³¸ ì •ì œ
        cleaned = re.sub(r'[^\w\sê°€-í£]', ' ', organization_name)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # ì§€ì—­ ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ê²€ìƒ‰ ê²°ê³¼ ì œí•œ ë°©ì§€)
        
        # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ì „ëµ
        if any(keyword in cleaned for keyword in ['êµíšŒ', 'ì„±ë‹¹', 'ì ˆ']):
            return f'{cleaned} í™ˆí˜ì´ì§€'
        elif any(keyword in cleaned for keyword in ['ì„¼í„°', 'ê´€', 'ì›']):
            return f'{cleaned} ê³µì‹í™ˆí˜ì´ì§€'
        else:
            return f'{cleaned} í™ˆí˜ì´ì§€'

    # ğŸ†• url_extractor.pyì˜ ë„¤ì´ë²„ ê²€ìƒ‰ ë¡œì§ ì™„ì „ ì´ì‹
    def search_homepage_url_with_naver_advanced(self, organization_name: str) -> str:
        """ê¸°ê´€ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (url_extractor.py ë¡œì§ ì™„ì „ ì´ì‹)"""
        if not self.use_selenium:
            print(f"âš ï¸ Selenium ë¹„í™œì„±í™”ë¨: {organization_name}")
            return ""
        
        if not self.driver:
            self.setup_selenium_driver()
        
        if not self.driver:
            print(f"âš ï¸ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {organization_name}")
            return ""
        
        try:
            # url_extractor.pyì™€ ë™ì¼í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤
            search_queries = [
                self.clean_search_query_advanced(organization_name),
                f'{organization_name} í™ˆí˜ì´ì§€',
                f'{organization_name} ê³µì‹ì‚¬ì´íŠ¸',
                f'{organization_name} site:or.kr',
                f'{organization_name} site:org',
            ]
            
            for i, query in enumerate(search_queries, 1):
                print(f"ğŸ” ë„¤ì´ë²„ ê²€ìƒ‰ {i}/{len(search_queries)}: {query}")
                result = self._perform_naver_search_advanced(query, organization_name)
                if result:
                    self.stats['url_found'] += 1
                    return result
                
                # url_extractor.pyì™€ ë™ì¼í•œ ì§€ì—°
                self.add_delay_advanced()
            
            return ""
            
        except Exception as e:
            print(f"âŒ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹¤íŒ¨ ({organization_name}): {e}")
            return ""

    # ğŸ†• url_extractor.pyì˜ ì§€ì—° ë¡œì§ ì™„ì „ ì´ì‹
    def add_delay_advanced(self):
        """ìš”ì²­ ê°„ ì§€ì—° (url_extractor.pyì™€ ë™ì¼)"""
        delay = random.uniform(*self.delay_range)
        print(f"â±ï¸ ì§€ì—° ì‹œê°„: {delay:.1f}ì´ˆ")
        time.sleep(delay)

    # ğŸ†• url_extractor.pyì˜ ë„¤ì´ë²„ ê²€ìƒ‰ ë¡œì§ ì™„ì „ ì´ì‹
    def _perform_naver_search_advanced(self, query: str, organization_name: str) -> str:
        """ì‹¤ì œ ë„¤ì´ë²„ ê²€ìƒ‰ ìˆ˜í–‰ (url_extractor.py ë¡œì§ ì™„ì „ ì´ì‹)"""
        try:
            # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
            self.driver.get('https://www.naver.com')
            time.sleep(2)
            
            # ê²€ìƒ‰ì°½ ì°¾ê¸° ë° ê²€ìƒ‰ì–´ ì…ë ¥
            search_box = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "query"))
            )
            
            # ê²€ìƒ‰ì°½ í´ë¦¬ì–´ í›„ ê²€ìƒ‰ì–´ ì…ë ¥
            search_box.clear()
            search_box.send_keys(query)
            search_box.send_keys(Keys.RETURN)
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œ ëŒ€ê¸°
            time.sleep(3)
            
            print(f"âœ… ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤í–‰: {query}")
            
            # ğŸ”§ url_extractor.pyì™€ ë™ì¼í•œ ì„ íƒìë“¤ (ë” ì •êµí•¨)
            link_selectors = [
                # í†µí•©ê²€ìƒ‰ ê²°ê³¼
                'h3.title a[href*="http"]',
                '.total_wrap a[href*="http"]',
                '.result_area a[href*="http"]',
                '.data_area a[href*="http"]',
                
                # ì›¹ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼
                '.web_page a[href*="http"]',
                '.site_area a[href*="http"]',
                
                # ì¼ë°˜ ë§í¬ë“¤
                'a[href*="http"]:not([href*="naver.com"])',
                '.info_area a[href*="http"]',
                '.source_area a[href*="http"]'
            ]
            
            found_urls = []
            
            for selector in link_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    print(f"ğŸ“ ì„ íƒì '{selector}'ë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                    
                    for element in elements[:15]:  # ê° ì„ íƒìë‹¹ 15ê°œì”©
                        try:
                            href = element.get_attribute('href')
                            
                            if not href or href.startswith('#') or href.startswith('javascript:'):
                                continue
                            
                            # ë„¤ì´ë²„ ë‚´ë¶€ ë§í¬ í•„í„°ë§
                            if 'naver.com' in href and not self._extract_redirect_url_advanced(href):
                                continue
                            
                            # ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬
                            if 'naver.com' in href:
                                actual_url = self._extract_redirect_url_advanced(href)
                                if actual_url:
                                    url = actual_url
                                else:
                                    continue
                            else:
                                url = href
                            
                            # URL ìœ íš¨ì„± ê²€ì‚¬ (url_extractor.py ë¡œì§)
                            if self.is_valid_homepage_url_advanced(url, organization_name):
                                found_urls.append(url)
                                print(f"ğŸ“‹ ìœ íš¨í•œ URL ë°œê²¬: {url}")
                                
                                if len(found_urls) >= self.max_search_results:
                                    break
                                    
                        except Exception:
                            continue
                    
                    if found_urls:
                        break
                        
                except Exception as e:
                    print(f"âš ï¸ ì„ íƒì '{selector}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            # ğŸ†• url_extractor.pyì˜ ì œëª© í…ìŠ¤íŠ¸ ì¶”ê°€ ê²€ìƒ‰
            if not found_urls:
                print(f"ğŸ” ì œëª© í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ê°€ ê²€ìƒ‰")
                found_urls = self._search_by_title_text_advanced(organization_name)
            
            # ê°€ì¥ ì í•©í•œ URL ì„ íƒ (url_extractor.py ë¡œì§)
            if found_urls:
                best_url = self.select_best_homepage_advanced(found_urls, organization_name)
                print(f"ğŸ¯ ìµœì¢… ì„ íƒëœ URL: {best_url}")
                return best_url
            
            print(f"âŒ '{query}' ê²€ìƒ‰ì—ì„œ ìœ íš¨í•œ URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            return ""
            
        except Exception as e:
            print(f"âš ï¸ ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return ""

    # ğŸ†• url_extractor.pyì˜ ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬ ë¡œì§ ì™„ì „ ì´ì‹
    def _extract_redirect_url_advanced(self, naver_url: str) -> str:
        """ë„¤ì´ë²„ ë¦¬ë‹¤ì´ë ‰íŠ¸ URLì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ (url_extractor.py ë¡œì§)"""
        try:
            if 'url=' in naver_url:
                from urllib.parse import parse_qs, urlparse, unquote
                parsed_url = urlparse(naver_url)
                query_params = parse_qs(parsed_url.query)
                if 'url' in query_params:
                    actual_url = query_params['url'][0]
                    return unquote(actual_url)
            return ""
        except:
            return ""

    # ğŸ†• url_extractor.pyì˜ ì œëª© í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë¡œì§ ì™„ì „ ì´ì‹
    def _search_by_title_text_advanced(self, organization_name: str) -> list:
        """ì œëª© í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€ ê²€ìƒ‰ (url_extractor.py ë¡œì§ ì™„ì „ ì´ì‹)"""
        found_urls = []
        try:
            # í™ˆí˜ì´ì§€ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë§í¬ ì°¾ê¸°
            homepage_keywords = ['í™ˆí˜ì´ì§€', 'ê³µì‹', 'official', 'home', 'www', 'ì‚¬ì´íŠ¸']
            
            all_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*="http"]')
            
            for link in all_links[:50]:
                try:
                    text = link.text.strip().lower()
                    href = link.get_attribute('href')
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ì— í™ˆí˜ì´ì§€ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                    if any(keyword in text for keyword in homepage_keywords):
                        if href and self.is_valid_homepage_url_advanced(href, organization_name):
                            found_urls.append(href)
                            print(f"ğŸ“ ì œëª© í…ìŠ¤íŠ¸ë¡œ ë°œê²¬: {href}")
                            
                            if len(found_urls) >= self.max_search_results:
                                break
                                
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ ì œëª© í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return found_urls

    # ğŸ†• url_extractor.pyì˜ URL ìœ íš¨ì„± ê²€ì‚¬ ë¡œì§ ì™„ì „ ì´ì‹
    def is_valid_homepage_url_advanced(self, url: str, organization_name: str) -> bool:
        """ìœ íš¨í•œ í™ˆí˜ì´ì§€ URLì¸ì§€ í™•ì¸ (url_extractor.py ë¡œì§ ì™„ì „ ì´ì‹)"""
        try:
            if not url or len(url) < 10:
                return False
            
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return False
            
            domain = parsed.netloc.lower()
            
            # ì œì™¸í•  ë„ë©”ì¸ ì²´í¬ (EXCLUDE_DOMAINS ì‚¬ìš©)
            for exclude_domain in EXCLUDE_DOMAINS:
                if exclude_domain in domain:
                    return False
            
            # ì¶”ê°€ ì œì™¸ íŒ¨í„´
            exclude_patterns = [
                r'\.pdf$', r'\.jpg$', r'\.png$', r'\.gif$',
                r'/board/', r'/bbs/', r'/community/',
                r'blog\.', r'cafe\.', r'post\.',
                r'search\.', r'maps\.', r'news\.'
            ]
            
            for pattern in exclude_patterns:
                if re.search(pattern, url.lower()):
                    return False
            
            # ë„ˆë¬´ ê¸´ URL ì œì™¸
            if len(url) > 200:
                return False
            
            # ê³µì‹ ë„ë©”ì¸ ìš°ì„ ìˆœìœ„
            official_tlds = ['.or.kr', '.go.kr', '.ac.kr', '.org', '.church']
            if any(tld in domain for tld in official_tlds):
                print(f"ğŸ›ï¸ ê³µì‹ ë„ë©”ì¸ ë°œê²¬: {domain}")
                return True
            
            # ê¸°ê´€ëª…ì´ ë„ë©”ì¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            name_parts = re.findall(r'[ê°€-í£a-zA-Z]+', organization_name.lower())
            for part in name_parts:
                if len(part) > 2 and part in domain:
                    print(f"ğŸ¯ ê¸°ê´€ëª…ì´ ë„ë©”ì¸ì— í¬í•¨ë¨: {part} in {domain}")
                    return True
            
            return True
            
        except Exception:
            return False

    # ğŸ†• url_extractor.pyì˜ ì ìˆ˜ ê¸°ë°˜ URL ì„ íƒ ë¡œì§ ì™„ì „ ì´ì‹
    def select_best_homepage_advanced(self, urls: list, organization_name: str) -> str:
        """ê°€ì¥ ì í•©í•œ í™ˆí˜ì´ì§€ URL ì„ íƒ (url_extractor.py ë¡œì§ ì™„ì „ ì´ì‹)"""
        scored_urls = []
        
        for url in urls:
            score = 0
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # ê³µì‹ ë„ë©”ì¸ ë†’ì€ ì ìˆ˜
            if any(tld in domain for tld in ['.or.kr', '.go.kr', '.ac.kr']):
                score += 10
            elif '.org' in domain:
                score += 8
            elif '.church' in domain:
                score += 7
            elif '.com' in domain:
                score += 3
            
            # ê¸°ê´€ëª… ë§¤ì¹­ ì ìˆ˜
            name_parts = re.findall(r'[ê°€-í£a-zA-Z]+', organization_name.lower())
            for part in name_parts:
                if len(part) > 1 and part in domain:
                    score += 5
            
            # ğŸ”§ ì¤‘ìš”: url_extractor.pyì˜ ê²½ë¡œ ì ìˆ˜ ë¡œì§ ì¶”ê°€
            path_depth = len(parsed.path.strip('/').split('/')) if parsed.path != '/' else 0
            score += max(0, 3 - path_depth)
            
            scored_urls.append((url, score))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_urls.sort(key=lambda x: x[1], reverse=True)
        
        best_url = scored_urls[0][0]
        print(f"ğŸ† ìµœê³  ì ìˆ˜ URL ì„ íƒ: {best_url} (ì ìˆ˜: {scored_urls[0][1]})")
        
        return best_url

    # ğŸ†• url_extractor.pyì˜ URL ê²€ì¦ ë¡œì§ ì™„ì „ ì´ì‹
    def verify_homepage_url_advanced(self, url: str) -> bool:
        """í™ˆí˜ì´ì§€ URLì´ ì‹¤ì œë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸ (url_extractor.py ë¡œì§ ì™„ì „ ì´ì‹)"""
        try:
            original_window = self.driver.current_window_handle
            
            # ìƒˆ íƒ­ì—ì„œ URL ê²€ì¦
            self.driver.execute_script("window.open('');")
            self.driver.switch_to.window(self.driver.window_handles[-1])
            
            self.driver.get(url)
            time.sleep(3)
            
            # í˜ì´ì§€ ë¡œë“œ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            is_accessible = "404" not in self.driver.title.lower() and \
                           "error" not in self.driver.title.lower() and \
                           len(self.driver.page_source) > 1000
            
            # ì›ë˜ ì°½ìœ¼ë¡œ ëŒì•„ê°€ê¸°
            self.driver.close()
            self.driver.switch_to.window(original_window)
            
            if is_accessible:
                print(f"âœ… URL ì ‘ê·¼ ì„±ê³µ: {url}")
            else:
                print(f"âš ï¸ URL ì ‘ê·¼ ì‹¤íŒ¨: {url}")
            
            return is_accessible
            
        except Exception as e:
            print(f"âŒ URL ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {url} - {e}")
            try:
                # ì˜¤ë¥˜ ì‹œ ì›ë˜ ì°½ìœ¼ë¡œ ëŒì•„ê°€ê¸°
                if self.driver and len(self.driver.window_handles) > 1:
                    self.driver.close()
                    self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass
            return False
    
    async def process_single_organization_enhanced(self, org_data):
        """ì™„ì „íˆ ê°œì„ ëœ ë‹¨ì¼ ê¸°ê´€ ì²˜ë¦¬ (ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì½œë°± í¬í•¨)"""
        org_name = org_data.get('name', 'Unknown')
        homepage_url = org_data.get('homepage', '')
        
        # ğŸ†• ì¶”ê°€: ì´ˆê¸° ìƒíƒœ ì½œë°± í˜¸ì¶œ
        if self.progress_callback:
            initial_result = {
                "name": org_name,
                "category": org_data.get('category', ''),
                "phone": org_data.get('phone', ''),
                "fax": org_data.get('fax', ''),
                "email": org_data.get('email', ''),
                "postal_code": org_data.get('postal_code', ''),
                "address": org_data.get('address', ''),
                "homepage_url": homepage_url,
                "status": "processing",
                "processed_at": datetime.now().isoformat(),
                "error_message": "",
                "current_step": "ì‹œì‘"
            }
            self.progress_callback(initial_result)
        
        try:
            start_time = time.time()
            
            # ğŸ†• 1ë‹¨ê³„: í™ˆí˜ì´ì§€ URL í™•ì¸/ê²€ìƒ‰ ì‹œì‘ ì•Œë¦¼
            if self.progress_callback:
                step_result = initial_result.copy()
                step_result.update({
                    "current_step": "í™ˆí˜ì´ì§€ URL ê²€ìƒ‰ ì¤‘...",
                    "status": "processing"
                })
                self.progress_callback(step_result)
            
            # 1. í™ˆí˜ì´ì§€ URL í™•ì¸/ê²€ìƒ‰ (url_extractor.py ë¡œì§ ì‚¬ìš©)
            if not homepage_url or homepage_url == 'ì •ë³´í™•ì¸ ì•ˆë¨' or not homepage_url.startswith(('http://', 'https://')):
                print(f"ğŸ” ê³ ê¸‰ í™ˆí˜ì´ì§€ URL ê²€ìƒ‰: {org_name}")
                homepage_url = self.search_homepage_url_with_naver_advanced(org_name)
                self.stats['url_search_performed'] += 1
                
                if homepage_url:
                    # ğŸ†• URL ë°œê²¬ ì‹œ ì½œë°± í˜¸ì¶œ
                    if self.progress_callback:
                        step_result = initial_result.copy()
                        step_result.update({
                            "homepage_url": homepage_url,
                            "current_step": "í™ˆí˜ì´ì§€ URL ê²€ì¦ ì¤‘...",
                            "status": "processing"
                        })
                        self.progress_callback(step_result)
                    
                    # ğŸ†• ì‹¤ì œ ì ‘ê·¼ì„± ê²€ì¦ (url_extractor.py í•µì‹¬ ê¸°ëŠ¥)
                    if self.verify_homepage_url_advanced(homepage_url):
                        org_data['homepage'] = homepage_url
                        print(f"âœ… ê²€ì¦ëœ í™ˆí˜ì´ì§€ ë°œê²¬: {org_name} -> {homepage_url}")
                        
                        # ğŸ†• ê²€ì¦ ì™„ë£Œ ì½œë°±
                        if self.progress_callback:
                            step_result = initial_result.copy()
                            step_result.update({
                                "homepage_url": homepage_url,
                                "current_step": "í™ˆí˜ì´ì§€ URL ê²€ì¦ ì™„ë£Œ",
                                "status": "processing"
                            })
                            self.progress_callback(step_result)
                    else:
                        homepage_url = ""
                        print(f"âŒ URL ì ‘ê·¼ ë¶ˆê°€, ì œì™¸: {org_name}")
                        
                        # ğŸ†• ê²€ì¦ ì‹¤íŒ¨ ì½œë°±
                        if self.progress_callback:
                            step_result = initial_result.copy()
                            step_result.update({
                                "current_step": "í™ˆí˜ì´ì§€ URL ì ‘ê·¼ ë¶ˆê°€",
                                "status": "processing"
                            })
                            self.progress_callback(step_result)
                else:
                    print(f"âŒ í™ˆí˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {org_name}")
                    
                    # ğŸ†• URL ì°¾ê¸° ì‹¤íŒ¨ ì½œë°±
                    if self.progress_callback:
                        step_result = initial_result.copy()
                        step_result.update({
                            "current_step": "í™ˆí˜ì´ì§€ URL ì°¾ê¸° ì‹¤íŒ¨",
                            "status": "processing"
                        })
                        self.progress_callback(step_result)
            else:
                print(f"ğŸ“‹ ê¸°ì¡´ í™ˆí˜ì´ì§€ ì‚¬ìš©: {org_name} -> {homepage_url}")
                
                # ğŸ†• ê¸°ì¡´ URL ì‚¬ìš© ì½œë°±
                if self.progress_callback:
                    step_result = initial_result.copy()
                    step_result.update({
                        "homepage_url": homepage_url,
                        "current_step": "ê¸°ì¡´ í™ˆí˜ì´ì§€ URL ì‚¬ìš©",
                        "status": "processing"
                    })
                    self.progress_callback(step_result)
            
            # ğŸ†• 2ë‹¨ê³„: JSON ë°ì´í„° ì¶”ì¶œ ì‹œì‘ ì•Œë¦¼
            if self.progress_callback:
                step_result = initial_result.copy()
                step_result.update({
                    "homepage_url": homepage_url,
                    "current_step": "JSON ë°ì´í„° ì¶”ì¶œ ì¤‘...",
                    "status": "processing"
                })
                self.progress_callback(step_result)
            
            # 2. JSON ë°ì´í„° ì¶”ì¶œ
            json_info = self.extract_from_json_data(org_data)
            
            # ğŸ†• JSON ì¶”ì¶œ ì™„ë£Œ ì½œë°± (ê¸°ë³¸ ì—°ë½ì²˜ ì •ë³´ ì—…ë°ì´íŠ¸)
            if self.progress_callback:
                step_result = initial_result.copy()
                step_result.update({
                    "phone": json_info.get('ì „í™”ë²ˆí˜¸', org_data.get('phone', '')),
                    "fax": json_info.get('íŒ©ìŠ¤ë²ˆí˜¸', org_data.get('fax', '')),
                    "email": json_info.get('ì´ë©”ì¼', org_data.get('email', '')),
                    "postal_code": json_info.get('ìš°í¸ë²ˆí˜¸', org_data.get('postal_code', '')),
                    "address": json_info.get('ì£¼ì†Œ', org_data.get('address', '')),
                    "homepage_url": homepage_url,
                    "current_step": "JSON ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ",
                    "status": "processing"
                })
                self.progress_callback(step_result)
            
            # 3. í™ˆí˜ì´ì§€ê°€ ìˆìœ¼ë©´ í¬ë¡¤ë§ ìˆ˜í–‰
            ai_info = {}
            if homepage_url and homepage_url.startswith(('http://', 'https://')):
                if self.needs_additional_crawling(json_info):
                    # ğŸ†• 3ë‹¨ê³„: í™ˆí˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘ ì•Œë¦¼
                    if self.progress_callback:
                        step_result = initial_result.copy()
                        step_result.update({
                            "phone": json_info.get('ì „í™”ë²ˆí˜¸', org_data.get('phone', '')),
                            "fax": json_info.get('íŒ©ìŠ¤ë²ˆí˜¸', org_data.get('fax', '')),
                            "email": json_info.get('ì´ë©”ì¼', org_data.get('email', '')),
                            "postal_code": json_info.get('ìš°í¸ë²ˆí˜¸', org_data.get('postal_code', '')),
                            "address": json_info.get('ì£¼ì†Œ', org_data.get('address', '')),
                            "homepage_url": homepage_url,
                            "current_step": "í™ˆí˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...",
                            "status": "processing"
                        })
                        self.progress_callback(step_result)
                    
                    print(f"ğŸŒ í¬ë¡¤ë§ ìˆ˜í–‰: {org_name}")
                    
                    try:
                        ai_info = await asyncio.wait_for(
                            self.crawl_and_extract_async(org_data), 
                            timeout=120
                        )
                        
                        # ğŸ†• í¬ë¡¤ë§ ì™„ë£Œ ì½œë°±
                        if self.progress_callback:
                            step_result = initial_result.copy()
                            step_result.update({
                                "phone": json_info.get('ì „í™”ë²ˆí˜¸', org_data.get('phone', '')),
                                "fax": json_info.get('íŒ©ìŠ¤ë²ˆí˜¸', org_data.get('fax', '')),
                                "email": json_info.get('ì´ë©”ì¼', org_data.get('email', '')),
                                "postal_code": json_info.get('ìš°í¸ë²ˆí˜¸', org_data.get('postal_code', '')),
                                "address": json_info.get('ì£¼ì†Œ', org_data.get('address', '')),
                                "homepage_url": homepage_url,
                                "current_step": "í™ˆí˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ",
                                "status": "processing"
                            })
                            self.progress_callback(step_result)
                            
                    except asyncio.TimeoutError:
                        print(f"â° í¬ë¡¤ë§ ì‹œê°„ ì´ˆê³¼: {org_name}")
                        ai_info = {}
                        
                        # ğŸ†• í¬ë¡¤ë§ ì‹œê°„ ì´ˆê³¼ ì½œë°±
                        if self.progress_callback:
                            step_result = initial_result.copy()
                            step_result.update({
                                "phone": json_info.get('ì „í™”ë²ˆí˜¸', org_data.get('phone', '')),
                                "fax": json_info.get('íŒ©ìŠ¤ë²ˆí˜¸', org_data.get('fax', '')),
                                "email": json_info.get('ì´ë©”ì¼', org_data.get('email', '')),
                                "postal_code": json_info.get('ìš°í¸ë²ˆí˜¸', org_data.get('postal_code', '')),
                                "address": json_info.get('ì£¼ì†Œ', org_data.get('address', '')),
                                "homepage_url": homepage_url,
                                "current_step": "í¬ë¡¤ë§ ì‹œê°„ ì´ˆê³¼",
                                "status": "processing"
                            })
                            self.progress_callback(step_result)
                else:
                    print(f"ğŸ“‹ JSON ì •ë³´ ì¶©ë¶„: {org_name}")
                    
                    # ğŸ†• í¬ë¡¤ë§ ìŠ¤í‚µ ì½œë°±
                    if self.progress_callback:
                        step_result = initial_result.copy()
                        step_result.update({
                            "phone": json_info.get('ì „í™”ë²ˆí˜¸', org_data.get('phone', '')),
                            "fax": json_info.get('íŒ©ìŠ¤ë²ˆí˜¸', org_data.get('fax', '')),
                            "email": json_info.get('ì´ë©”ì¼', org_data.get('email', '')),
                            "postal_code": json_info.get('ìš°í¸ë²ˆí˜¸', org_data.get('postal_code', '')),
                            "address": json_info.get('ì£¼ì†Œ', org_data.get('address', '')),
                            "homepage_url": homepage_url,
                            "current_step": "JSON ì •ë³´ ì¶©ë¶„ (í¬ë¡¤ë§ ìŠ¤í‚µ)",
                            "status": "processing"
                        })
                        self.progress_callback(step_result)
            
            # ğŸ†• 4ë‹¨ê³„: ë°ì´í„° ë³‘í•© ì‹œì‘ ì•Œë¦¼
            if self.progress_callback:
                step_result = initial_result.copy()
                step_result.update({
                    "current_step": "ë°ì´í„° ë³‘í•© ì¤‘...",
                    "status": "processing"
                })
                self.progress_callback(step_result)
            
            # 4. ë°ì´í„° ë³‘í•©
            merged_data = self.merge_contact_data(json_info, ai_info)
            
            # 5. ìµœì¢… ê²°ê³¼ í¬ë§·íŒ…
            final_result = self.format_final_result(org_data, merged_data)
            final_result['ì¶”ì¶œë°©ë²•'] = 'JSON+ê³ ê¸‰ê²€ìƒ‰'  # ê³ ê¸‰ ê²€ìƒ‰ í‘œì‹œ
            
            # ì²˜ë¦¬ ì‹œê°„ ë¡œê¹…
            elapsed_time = time.time() - start_time
            print(f"âœ… ê³ ê¸‰ ì²˜ë¦¬ ì™„ë£Œ: {org_name} ({final_result['ì¶”ì¶œë°©ë²•']}) - {elapsed_time:.1f}ì´ˆ")
            
            # ğŸ†• ìµœì¢… ì™„ë£Œ ì½œë°± (ëª¨ë“  ë°ì´í„° í¬í•¨)
            if self.progress_callback:
                completion_result = {
                    "name": org_name,
                    "category": org_data.get('category', ''),
                    "phone": merged_data.get('ì „í™”ë²ˆí˜¸', ''),
                    "fax": merged_data.get('íŒ©ìŠ¤ë²ˆí˜¸', ''),
                    "email": merged_data.get('ì´ë©”ì¼', ''),
                    "postal_code": merged_data.get('ìš°í¸ë²ˆí˜¸', ''),
                    "address": merged_data.get('ì£¼ì†Œ', ''),
                    "homepage_url": homepage_url,
                    "status": "completed",
                    "processed_at": datetime.now().isoformat(),
                    "error_message": "",
                    "current_step": f"ì²˜ë¦¬ ì™„ë£Œ ({elapsed_time:.1f}ì´ˆ)",
                    "processing_time": f"{elapsed_time:.1f}ì´ˆ",
                    "extraction_method": final_result.get('ì¶”ì¶œë°©ë²•', 'JSON+ê³ ê¸‰ê²€ìƒ‰')
                }
                self.progress_callback(completion_result)
            
            return final_result
            
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ ({org_name}): {str(e)[:100]}...")
            
            # ğŸ†• ì—ëŸ¬ ë°œìƒ ì‹œ ì½œë°±
            if self.progress_callback:
                error_result = {
                    "name": org_name,
                    "category": org_data.get('category', ''),
                    "phone": org_data.get('phone', ''),
                    "fax": org_data.get('fax', ''),
                    "email": org_data.get('email', ''),
                    "postal_code": org_data.get('postal_code', ''),
                    "address": org_data.get('address', ''),
                    "homepage_url": homepage_url,
                    "status": "failed",
                    "processed_at": datetime.now().isoformat(),
                    "error_message": str(e)[:200],  # ì—ëŸ¬ ë©”ì‹œì§€ ì œí•œ
                    "current_step": "ì²˜ë¦¬ ì‹¤íŒ¨",
                    "extraction_method": "ì‹¤íŒ¨"
                }
                self.progress_callback(error_result)
            
            return {
                'ê¸°ê´€ëª…': org_name,
                'í™ˆí˜ì´ì§€': homepage_url,
                'ì£¼ì†Œ': 'ì •ë³´í™•ì¸ ì•ˆë¨',
                'ìš°í¸ë²ˆí˜¸': 'ì •ë³´í™•ì¸ ì•ˆë¨',
                'ì „í™”ë²ˆí˜¸': 'ì •ë³´í™•ì¸ ì•ˆë¨',
                'íŒ©ìŠ¤ë²ˆí˜¸': 'ì •ë³´í™•ì¸ ì•ˆë¨',
                'ì´ë©”ì¼': 'ì •ë³´í™•ì¸ ì•ˆë¨',
                'ì¶”ì¶œë°©ë²•': 'ì‹¤íŒ¨',
                'ì¶”ì¶œìƒíƒœ': 'ì‹¤íŒ¨'
            }

    def validate_and_clean_contacts(self, phone_list, fax_list):
        """ê°œì„ ëœ ì „í™”ë²ˆí˜¸/íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦ ë° ì¤‘ë³µ ì œê±°"""
        cleaned_phones = []
        cleaned_faxes = []
        
        # âœ… ìˆ˜ì •: PhoneUtils í™œìš©
        # ì „í™”ë²ˆí˜¸ ê²€ì¦
        for phone in phone_list:
            if PhoneUtils.validate_korean_phone(phone):  # validator.validate_phone_number ëŒ€ì²´
                formatted = PhoneUtils.format_phone_number(phone)
                if formatted and formatted not in cleaned_phones:
                    cleaned_phones.append(formatted)
        
        # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦
        for fax in fax_list:
            if PhoneUtils.validate_korean_phone(fax):  # validator.validate_fax_number ëŒ€ì²´
                formatted = PhoneUtils.format_phone_number(fax)
                if formatted and formatted not in cleaned_faxes:
                    cleaned_faxes.append(formatted)
        
        # ì „í™”ë²ˆí˜¸-íŒ©ìŠ¤ë²ˆí˜¸ ì¤‘ë³µ ê²€ì‚¬ ê°œì„ 
        final_phones = cleaned_phones.copy()
        final_faxes = []
        
        for fax in cleaned_faxes:
            # âœ… ìˆ˜ì •: PhoneUtils í™œìš©
            # ì „í™”ë²ˆí˜¸ì™€ ì™„ì „íˆ ë™ì¼í•œ íŒ©ìŠ¤ë²ˆí˜¸ëŠ” ì œì™¸
            if fax in final_phones:
                self.stats['duplicates_removed'] += 1
                print(f"ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {fax} (ì „í™”ë²ˆí˜¸ì™€ ë™ì¼)")
                continue
            
            # ìœ ì‚¬í•œ ë²ˆí˜¸ ê²€ì‚¬ (ë” ì •êµí•œ ë¡œì§)
            is_duplicate = False
            for phone in final_phones:
                if PhoneUtils.is_phone_fax_duplicate(phone, fax):  # validator ë©”ì„œë“œ ëŒ€ì²´
                    is_duplicate = True
                    self.stats['duplicates_removed'] += 1
                    print(f"ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {fax} (ì „í™”ë²ˆí˜¸ {phone}ì™€ ìœ ì‚¬)")
                    break
            
            if not is_duplicate:
                final_faxes.append(fax)
        
        return final_phones, final_faxes
    
    def extract_from_json_data(self, org_data):
        """JSON ë°ì´í„°ì—ì„œ ê¸°ì¡´ ì •ë³´ ì¶”ì¶œ (jsontocsv.py ë¡œì§ í™œìš©)"""
        result = {
            'phone': [],
            'fax': [],
            'email': [],
            'address': [],
            'postal_code': []
        }
        
        # ê¸°ë³¸ í•„ë“œì—ì„œ ì¶”ì¶œ
        if org_data.get("phone"):
            result['phone'].append(org_data["phone"])
        if org_data.get("fax"):
            result['fax'].append(org_data["fax"])
        
        # í™ˆí˜ì´ì§€ íŒŒì‹± ê²°ê³¼ì—ì„œ ì¶”ì¶œ
        homepage_content = org_data.get("homepage_content", {})
        if homepage_content:
            parsed_contact = homepage_content.get("parsed_contact", {})
            
            # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (íŒŒì‹±ëœ ê²°ê³¼ ìš°ì„ )
            if parsed_contact.get("phones"):
                for phone in parsed_contact["phones"]:
                    if phone not in result['phone']:
                        result['phone'].append(phone)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ (íŒŒì‹±ëœ ê²°ê³¼ ìš°ì„ )
            if parsed_contact.get("faxes"):
                for fax in parsed_contact["faxes"]:
                    if fax not in result['fax']:
                        result['fax'].append(fax)
            
            # ì´ë©”ì¼ ì¶”ì¶œ
            if parsed_contact.get("emails"):
                for email in parsed_contact["emails"]:
                    if email not in result['email']:
                        result['email'].append(email)
            
            # ì£¼ì†Œ ì¶”ì¶œ
            if parsed_contact.get("addresses"):
                for address in parsed_contact["addresses"]:
                    if address not in result['address']:
                        result['address'].append(address)
            
            # contact_infoì—ì„œë„ ì¶”ì¶œ ì‹œë„
            contact_info = homepage_content.get("contact_info", "")
            if contact_info:
                # ê°„ë‹¨í•œ ì •ê·œì‹ìœ¼ë¡œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                phone_matches = re.findall(r'ì „í™”[:\s]*([0-9\-\.\(\)\s]{8,20})', contact_info)
                fax_matches = re.findall(r'íŒ©ìŠ¤[:\s]*([0-9\-\.\(\)\s]{8,20})', contact_info)
                email_matches = re.findall(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', contact_info)
                postal_matches = re.findall(r'(\d{5})', contact_info)
                
                for phone in phone_matches:
                    clean_phone = phone.strip()
                    if clean_phone and clean_phone not in result['phone']:
                        result['phone'].append(clean_phone)
                
                for fax in fax_matches:
                    clean_fax = fax.strip()
                    if clean_fax and clean_fax not in result['fax']:
                        result['fax'].append(clean_fax)
                
                for email in email_matches:
                    clean_email = email.strip()
                    if clean_email and clean_email not in result['email']:
                        result['email'].append(clean_email)
                
                for postal in postal_matches:
                    clean_postal = postal.strip()
                    if clean_postal and clean_postal not in result['postal_code']:
                        result['postal_code'].append(clean_postal)
        
        return result
    
    def extract_with_regex(self, text, pattern_type):
        """ì •ê·œì‹ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ"""
        results = []
        if pattern_type in self.patterns:
            for pattern in self.patterns[pattern_type]:
                matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    if isinstance(match, tuple):
                        match = match[0] if match[0] else (match[1] if len(match) > 1 else "")
                    if match and len(match.strip()) > 3:
                        results.append(match.strip())
        
        # ì¤‘ë³µ ì œê±° ë° ì •ì œ
        unique_results = []
        for result in results:
            cleaned = re.sub(r'[^\w\s\-\.\@\(\)]', '', result)
            if cleaned and cleaned not in unique_results:
                unique_results.append(cleaned)
        
        return unique_results[:3]  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ
        
    def extract_contact_info_sync(self, text, organization_name):
        """ì—°ë½ì²˜ ì •ë³´ ì¢…í•© ì¶”ì¶œ"""
        result = {
            'phone': [],
            'fax': [],
            'email': [],
            'address': []
        }
        
        # 1. ì •ê·œì‹ìœ¼ë¡œ 1ì°¨ ì¶”ì¶œ
        for info_type in result.keys():
            regex_results = self.extract_with_regex(text, info_type)
            result[info_type].extend(regex_results)
        
        # 2. AIë¡œ 2ì°¨ ë³´ì™„ (í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬)
        if self.use_ai and text:
            chunk_size = 3000
            overlap = 300
            
            for i in range(0, len(text), chunk_size - overlap):
                chunk = text[i:i + chunk_size]
                if len(chunk) < 100:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ëŠ” ìŠ¤í‚µ
                    continue
                
                ai_result = self.extract_with_ai(chunk, organization_name)
                
                # AI ê²°ê³¼ ë³‘í•©
                for info_type, values in ai_result.items():
                    if info_type in result and isinstance(values, list):
                        for value in values:
                            if value and value != "ì •ë³´í™•ì¸ ì•ˆë¨" and value not in result[info_type]:
                                result[info_type].append(value)
        
        # 3. ê²°ê³¼ ì •ì œ ë° ì œí•œ
        for info_type in result.keys():
            # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            unique_items = []
            for item in result[info_type]:
                if item and item not in unique_items:
                    unique_items.append(item)
            result[info_type] = unique_items[:2]  # ìµœëŒ€ 2ê°œê¹Œì§€
        
        return result
    
    def extract_with_ai(self, text_chunk, organization_name):
        """Gemini AIë¡œ ì •ë³´ ì¶”ì¶œ (ë™ê¸°ì‹)"""
        if not self.use_ai or not self.ai_model_manager:  # ğŸ”§ ìˆ˜ì •
            return {}
        
        try:
            prompt = self.create_structured_prompt(text_chunk, organization_name)
            response_text = self.ai_model_manager.generate_text(prompt)  # ğŸ”§ ìˆ˜ì •
            
            # ë§ˆí¬ë‹¤ìš´ íŒŒì‹±
            result = self.parse_markdown_to_dict(response_text)
            self.stats['ai_enhanced'] += 1
            return result
            
        except Exception as e:
            print(f"âŒ AI ì¶”ì¶œ ì˜¤ë¥˜ ({organization_name}): {str(e)}")
            return {}
        
    def create_structured_prompt(self, text_chunk, organization_name):
        """êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.

            ê¸°ê´€ëª…: {organization_name}

            ì•„ë˜ í…ìŠ¤íŠ¸ì—ì„œ ì •í™•í•œ ì—°ë½ì²˜ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

            ì „í™”ë²ˆí˜¸: 
            íŒ©ìŠ¤ë²ˆí˜¸: 
            ì´ë©”ì¼: 
            ìš°í¸ë²ˆí˜¸: 
            ì£¼ì†Œ: 

            **ì¶”ì¶œ ê·œì¹™:**
            - ì „í™”ë²ˆí˜¸: í•œêµ­ í˜•ì‹ë§Œ (ì˜ˆ: 02-1234-5678, 031-123-4567)
            - íŒ©ìŠ¤ë²ˆí˜¸: í•œêµ­ í˜•ì‹ë§Œ (ì˜ˆ: 02-1234-5679)
            - ì´ë©”ì¼: ìœ íš¨í•œ ì´ë©”ì¼ í˜•ì‹ë§Œ (ì˜ˆ: info@example.com)
            - ìš°í¸ë²ˆí˜¸: 5ìë¦¬ ìˆ«ì (ì˜ˆ: 12345)
            - ì£¼ì†Œ: ì‹œ/ë„ë¶€í„° ì‹œì‘í•˜ëŠ” ì™„ì „í•œ ì£¼ì†Œ
            - ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì •ë³´í™•ì¸ ì•ˆë¨"ìœ¼ë¡œ í‘œì‹œ
            - ê° í•­ëª©ë‹¹ ìµœëŒ€ 1ê°œë§Œ ì¶”ì¶œ

            **ë¶„ì„í•  í…ìŠ¤íŠ¸:**
            {text_chunk[:2000]}

            ìœ„ í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

        # í”„ë¡¬í”„íŠ¸ ë¡œê¹…
        self.ai_logger.info(f"=== í”„ë¡¬í”„íŠ¸ ìƒì„± [{organization_name}] ===")
        self.ai_logger.info(f"í”„ë¡¬í”„íŠ¸ ë‚´ìš©:\n{prompt[:500]}...")
        self.ai_logger.info(f"í…ìŠ¤íŠ¸ ì²­í¬ ê¸¸ì´: {len(text_chunk)} ë¬¸ì")
        
        return prompt
    
    def parse_markdown_to_dict(self, markdown_text):
        """ë§ˆí¬ë‹¤ìš´ ì‘ë‹µì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        result = {}
        lines = markdown_text.split('\n')
        
        for line in lines:
            line = line.strip()
            if ':' in line and not line.startswith('#'):
                key, value = line.split(':', 1)
                key = key.strip().replace('**', '').replace('*', '')
                value = value.strip()
                
                if value and value != "ì •ë³´í™•ì¸ ì•ˆë¨":
                    # í‚¤ ë§¤í•‘
                    if 'ì „í™”ë²ˆí˜¸' in key or 'phone' in key.lower():
                        result['phone'] = [value]
                    elif 'íŒ©ìŠ¤' in key or 'fax' in key.lower():
                        result['fax'] = [value]
                    elif 'ì´ë©”ì¼' in key or 'email' in key.lower():
                        result['email'] = [value]
                    elif 'ì£¼ì†Œ' in key or 'address' in key.lower():
                        result['address'] = [value]
                    elif 'ìš°í¸ë²ˆí˜¸' in key or 'postal' in key.lower():
                        result['postal_code'] = [value]
        
        return result
    
    def merge_contact_data(self, json_data, ai_data):
        """JSON ë°ì´í„°ì™€ AI ì¶”ì¶œ ë°ì´í„° ë³‘í•©"""
        merged = {}
        
        # ëª¨ë“  í‚¤ì— ëŒ€í•´ ë³‘í•© ìˆ˜í–‰
        all_keys = set(json_data.keys()) | set(ai_data.keys())
        
        for key in all_keys:
            merged[key] = []
            
            # JSON ë°ì´í„° ì¶”ê°€
            if key in json_data and json_data[key]:
                if isinstance(json_data[key], list):
                    merged[key].extend(json_data[key])
                else:
                    merged[key].append(json_data[key])
            
            # AI ë°ì´í„° ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
            if key in ai_data and ai_data[key]:
                if isinstance(ai_data[key], list):
                    for item in ai_data[key]:
                        if item not in merged[key]:
                            merged[key].append(item)
                else:
                    if ai_data[key] not in merged[key]:
                        merged[key].append(ai_data[key])
        
        return merged
    
    async def extract_with_ai_structured_async(self, text_chunk, organization_name):
        """ë¹„ë™ê¸° AI ì¶”ì¶œ (TPM ê³ ë ¤)"""
        if not self.use_ai or not self.ai_model_manager:  # ğŸ”§ ìˆ˜ì •
            return {}
        
        try:
            # TPM ì œí•œ ëŒ€ê¸°
            await self.tpm_manager.wait_if_needed()
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.create_structured_prompt(text_chunk, organization_name)
            
            # AI í˜¸ì¶œ (ë¹„ë™ê¸°) - ai_helpers.py í™œìš©
            response_text = await extract_with_gemini_text(text_chunk, prompt)  # ğŸ”§ ìˆ˜ì •
            
            self.stats['api_calls_made'] += 1
            
            # ë§ˆí¬ë‹¤ìš´ íŒŒì‹±
            ai_data = self.parse_markdown_to_dict(response_text)
            
            if ai_data:
                self.stats['ai_enhanced'] += 1
                print(f"ğŸ¤– AI ì¶”ì¶œ ì„±ê³µ: {organization_name}")
            
            return ai_data
            
        except Exception as e:
            print(f"âŒ ë¹„ë™ê¸° AI ì¶”ì¶œ ì˜¤ë¥˜ ({organization_name}): {e}")
            return {}
    
    def crawl_homepage_if_needed(self, url, organization_name):
        """âœ… ìˆ˜ì •: í´ë˜ìŠ¤ ë‚´ë¶€ë¡œ ì´ë™, SSL ë¬¸ì œ í•´ê²°ì´ í¬í•¨ëœ í¬ë¡¤ë§"""
        try:
            print(f"ğŸ” ì¶”ê°€ í¬ë¡¤ë§ ì‹œì‘: {organization_name} ({url})")
            
            # SSL ê²€ì¦ ë¹„í™œì„±í™”ë¡œ ìš”ì²­ ì‹œë„
            try:
                response = self.session.get(url, timeout=10, verify=True)
            except requests.exceptions.SSLError:
                print(f"âš ï¸ SSL ì¸ì¦ì„œ ë¬¸ì œ ê°ì§€, ê²€ì¦ ì—†ì´ ì¬ì‹œë„: {organization_name}")
                response = self.session.get(url, timeout=10, verify=False)
            
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup(['script', 'style', 'nav', 'header', 'aside']):
                tag.decompose()
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = soup.get_text(separator=' ', strip=True)
            
            self.stats['crawling_performed'] += 1
            print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {organization_name} ({len(full_text)} ë¬¸ì)")
            return full_text
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨ ({organization_name}): {str(e)[:100]}...")
            return ""
    
    async def crawl_and_extract_async(self, org_data):
        """ê°œì„ ëœ ë¹„ë™ê¸° í¬ë¡¤ë§ ë° AI ì¶”ì¶œ"""
        org_name = org_data.get('name', 'Unknown')
        homepage_url = org_data.get('homepage', '')
        
        if not homepage_url or homepage_url == 'ì •ë³´í™•ì¸ ì•ˆë¨':
            return {}
        
        # í™ˆí˜ì´ì§€ í¬ë¡¤ë§
        homepage_text = self.crawl_homepage_if_needed(homepage_url, org_name)
        
        if not homepage_text:
            return {}
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ì²˜ë¦¬)
        max_text_length = 10000  # 10KBë¡œ ì œí•œ
        if len(homepage_text) > max_text_length:
            homepage_text = homepage_text[:max_text_length]
            print(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ: {org_name} ({max_text_length} ë¬¸ìë¡œ ì œí•œ)")
        
        # ì²­í¬ ì²˜ë¦¬ ê°œì„ 
        chunk_size = 2000
        overlap = 200
        max_chunks = 5  # ìµœëŒ€ ì²­í¬ ìˆ˜ ì œí•œ
        ai_results = {}
        processed_chunks = 0
        
        for i in range(0, len(homepage_text), chunk_size - overlap):
            if processed_chunks >= max_chunks:
                print(f"âš ï¸ ìµœëŒ€ ì²­í¬ ìˆ˜ ë„ë‹¬: {org_name} (5ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ)")
                break
                
            chunk = homepage_text[i:i + chunk_size]
            if len(chunk) < 100:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ëŠ” ìŠ¤í‚µ
                continue
            
            print(f"ğŸ“ ì²­í¬ {processed_chunks + 1}/{max_chunks} ì²˜ë¦¬ ì¤‘: {org_name}")
            
            chunk_result = await self.extract_with_ai_structured_async(chunk, org_name)
            processed_chunks += 1
            
            # ê²°ê³¼ ë³‘í•©
            for key, value in chunk_result.items():
                if key not in ai_results:
                    ai_results[key] = []
                
                if isinstance(value, list):
                    for item in value:
                        if item not in ai_results[key]:
                            ai_results[key].append(item)
                else:
                    if value not in ai_results[key]:
                        ai_results[key].append(value)
            
            # ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨ (íš¨ìœ¨ì„± ê°œì„ )
            essential_fields = ['phone', 'email', 'address']
            found_fields = sum(1 for field in essential_fields if ai_results.get(field))
            
            if found_fields >= 2:  # 3ê°œ ì¤‘ 2ê°œ ì´ìƒ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
                print(f"âœ… ì¶©ë¶„í•œ ì •ë³´ ë°œê²¬: {org_name} ({found_fields}/{len(essential_fields)} í•„ë“œ)")
                break
        
        print(f"ğŸ“Š {org_name}: {processed_chunks}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
        return ai_results
    
    def needs_additional_crawling(self, json_info):
        """ì¶”ê°€ í¬ë¡¤ë§ì´ í•„ìš”í•œì§€ íŒë‹¨"""
        missing_count = 0
        
        if not json_info.get('phone'):
            missing_count += 1
        if not json_info.get('fax'):
            missing_count += 1
        if not json_info.get('email'):
            missing_count += 1
        if not json_info.get('address'):
            missing_count += 1
        
        # 2ê°œ ì´ìƒ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ í¬ë¡¤ë§ ìˆ˜í–‰
        return missing_count >= 2
    
    def format_final_result(self, org_data, merged_data):
        """ìµœì¢… ê²°ê³¼ í¬ë§·íŒ…"""
        org_name = org_data.get('name', 'Unknown')
        homepage_url = org_data.get('homepage', '')
        
        result = {
            'ê¸°ê´€ëª…': org_name,
            'í™ˆí˜ì´ì§€': homepage_url,
            'ì£¼ì†Œ': 'ì •ë³´í™•ì¸ ì•ˆë¨',
            'ìš°í¸ë²ˆí˜¸': 'ì •ë³´í™•ì¸ ì•ˆë¨',
            'ì „í™”ë²ˆí˜¸': 'ì •ë³´í™•ì¸ ì•ˆë¨',
            'íŒ©ìŠ¤ë²ˆí˜¸': 'ì •ë³´í™•ì¸ ì•ˆë¨',
            'ì´ë©”ì¼': 'ì •ë³´í™•ì¸ ì•ˆë¨',
            'ì¶”ì¶œë°©ë²•': 'JSON',
            'ì¶”ì¶œìƒíƒœ': 'ì„±ê³µ'
        }
        
        # ê²€ì¦ ë° ì •ì œ
        validated_phones, validated_faxes = self.validate_and_clean_contacts(
            merged_data.get('phone', []), merged_data.get('fax', [])
        )
        
        # ê²°ê³¼ ì ìš©
        if validated_phones:
            result['ì „í™”ë²ˆí˜¸'] = validated_phones[0]
            self.stats['phone_found'] += 1
        
        if validated_faxes:
            result['íŒ©ìŠ¤ë²ˆí˜¸'] = validated_faxes[0]
            self.stats['fax_found'] += 1
        
        if merged_data.get('email'):
            result['ì´ë©”ì¼'] = merged_data['email'][0]
            self.stats['email_found'] += 1
        
        if merged_data.get('address'):
            result['ì£¼ì†Œ'] = merged_data['address'][0]
            self.stats['address_found'] += 1
        
        if merged_data.get('postal_code'):
            result['ìš°í¸ë²ˆí˜¸'] = merged_data['postal_code'][0]
            self.stats['postal_code_found'] += 1
        
        # ì¶”ì¶œë°©ë²• ê²°ì •
        json_only = not self.needs_additional_crawling(self.extract_from_json_data(org_data))
        if json_only:
            result['ì¶”ì¶œë°©ë²•'] = 'JSONë§Œ'
            self.stats['json_info_used'] += 1
        else:
            result['ì¶”ì¶œë°©ë²•'] = 'JSON+í¬ë¡¤ë§'
        
        return result

    def save_to_excel_simple(self, results, output_filename=None):
        """ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥"""
        if not results:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        if not output_filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"homepage_details_{timestamp}.xlsx"
        
        try:
            df = pd.DataFrame(results)
            
            # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
            column_order = [
                'ê¸°ê´€ëª…', 'í™ˆí˜ì´ì§€', 'ì£¼ì†Œ', 
                'ì „í™”ë²ˆí˜¸1', 'ì „í™”ë²ˆí˜¸2', 
                'íŒ©ìŠ¤ë²ˆí˜¸1', 'íŒ©ìŠ¤ë²ˆí˜¸2',
                'ì´ë©”ì¼1', 'ì´ë©”ì¼2', 'ì¶”ì¶œìƒíƒœ'
            ]
            
            df = df[column_order]
            df.to_excel(output_filename, index=False, engine='openpyxl')
            
            print(f"ğŸ’¾ Excel íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_filename}")
            print(f"ğŸ“Š ì´ {len(results)}ê°œ ê¸°ê´€ ë°ì´í„° ì €ì¥")
            
            return output_filename
            
        except Exception as e:
            print(f"âŒ Excel ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return None
    
    # ğŸ”„ ê¸°ì¡´ process_json_file_async ë©”ì„œë“œì—ì„œ ë“œë¼ì´ë²„ ì •ë¦¬ ì¶”ê°€
    async def process_json_file_async(self, json_file_path, test_mode=False, test_count=10):
        """JSON íŒŒì¼ ì „ì²´ ì²˜ë¦¬ (URL ê²€ìƒ‰ í¬í•¨)"""
        print(f"ğŸ“‚ JSON íŒŒì¼ ë¡œë“œ: {json_file_path}")
        
        try:
            data = FileUtils.load_json(json_file_path)
            if not data:
                print("âŒ JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
                return None
            
            # ë°ì´í„° êµ¬ì¡° í™•ì¸ ë° ë³€í™˜ (ê¸°ì¡´ ë¡œì§)
            all_organizations = []
            
            if isinstance(data, dict):
                print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° êµ¬ì¡° ê°ì§€:")
                for category, organizations in data.items():
                    if isinstance(organizations, list):
                        print(f"   ğŸ“‚ {category}: {len(organizations)}ê°œ ê¸°ê´€")
                        for org in organizations:
                            org['category'] = category
                        all_organizations.extend(organizations)
                    else:
                        print(f"   âš ï¸ {category}: ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ë°ì´í„° íƒ€ì… ({type(organizations)})")
            elif isinstance(data, list):
                print(f"ğŸ“Š ë¦¬ìŠ¤íŠ¸ ë°ì´í„° êµ¬ì¡° ê°ì§€")
                all_organizations = data
            else:
                print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° êµ¬ì¡°: {type(data)}")
                return None
            
            total_count = len(all_organizations)
            process_count = min(test_count, total_count) if test_mode else total_count
            
            print(f"ğŸ“Š ì´ {total_count}ê°œ ê¸°ê´€ ì¤‘ {process_count}ê°œ ì²˜ë¦¬ ì˜ˆì •")
            print(f"ğŸŒ URL ê²€ìƒ‰ ê¸°ëŠ¥: {'í™œì„±í™”' if self.use_selenium else 'ë¹„í™œì„±í™”'}")
            if test_mode:
                print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì²˜ìŒ {process_count}ê°œë§Œ ì²˜ë¦¬")
            
            results = []
            
            for i, org_data in enumerate(all_organizations[:process_count]):
                print(f"\nğŸ”„ ì§„í–‰ë¥ : {i+1}/{process_count} ({((i+1)/process_count)*100:.1f}%)")
                
                result = await self.process_single_organization_enhanced(org_data)
                results.append(result)
                
                self.stats['total_processed'] += 1
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if (i + 1) % 5 == 0:
                    self.print_progress_stats()
            
            self.merged_results = results
            return results
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return None
        finally:
            # ğŸ†• ë“œë¼ì´ë²„ ì •ë¦¬
            self.close_selenium_driver()
    
    def categorize_results(self, results):
        """ê²°ê³¼ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        categorized = {
            'churches': [],
            'taekwondo_centers': [],
            'youth_centers': [],
            'educational_institutions': [],
            'other_organizations': []
        }
        
        for result in results:
            category = result.get('category', 'other_organizations')
            if category in categorized:
                # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì œê±° í›„ ì¶”ê°€
                result_copy = result.copy()
                result_copy.pop('category', None)
                categorized[category].append(result_copy)
            else:
                categorized['other_organizations'].append(result)
        
        return categorized
    
    def save_merged_results_to_json(self, results, filename=None):
        """ë³‘í•©ëœ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        if not filename:
            # âœ… ìˆ˜ì •: FileUtils í™œìš©
            filename = FileUtils.create_timestamped_filename("enhanced_contact_data")
        
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¬ë¶„ë¥˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            categorized_data = self.categorize_results(results)
            
            # âœ… ìˆ˜ì •: FileUtils í™œìš©
            success = FileUtils.save_json(categorized_data, filename, backup=False)
            
            if success:
                print(f"ğŸ’¾ ë³‘í•©ëœ JSON íŒŒì¼ ì €ì¥: {filename}")
                return filename
            else:
                print(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨")
                return None
                
        except Exception as e:
            print(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def create_final_excel_report(self, results, output_filename=None):
        """ìµœì¢… ì¢…í•© Excel ë¦¬í¬íŠ¸ ìƒì„±"""
        if not output_filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"enhanced_contact_report_{timestamp}.xlsx"
        
        try:
            with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:
                # ë©”ì¸ ë°ì´í„° ì‹œíŠ¸
                df_main = pd.DataFrame(results)
                
                # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
                column_order = [
                    'ê¸°ê´€ëª…', 'í™ˆí˜ì´ì§€', 'ì£¼ì†Œ', 'ìš°í¸ë²ˆí˜¸',
                    'ì „í™”ë²ˆí˜¸', 'íŒ©ìŠ¤ë²ˆí˜¸', 'ì´ë©”ì¼', 
                    'ì¶”ì¶œë°©ë²•', 'ì¶”ì¶œìƒíƒœ'
                ]
                
                # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
                available_columns = [col for col in column_order if col in df_main.columns]
                df_main = df_main[available_columns]
                
                df_main.to_excel(writer, sheet_name='ì—°ë½ì²˜ì •ë³´', index=False)
                
                # í†µê³„ ì‹œíŠ¸
                total = max(self.stats['total_processed'], 1)
                stats_data = [
                    ['í•­ëª©', 'ê°œìˆ˜', 'ë¹„ìœ¨(%)'],
                    ['ì´ ê¸°ê´€ ìˆ˜', self.stats['total_processed'], 100.0],
                    ['ì „í™”ë²ˆí˜¸ ë³´ìœ ', self.stats['phone_found'], 
                     round(self.stats['phone_found']/total*100, 1)],
                    ['íŒ©ìŠ¤ë²ˆí˜¸ ë³´ìœ ', self.stats['fax_found'], 
                     round(self.stats['fax_found']/total*100, 1)],
                    ['ì´ë©”ì¼ ë³´ìœ ', self.stats['email_found'], 
                     round(self.stats['email_found']/total*100, 1)],
                    ['ì£¼ì†Œ ë³´ìœ ', self.stats['address_found'], 
                     round(self.stats['address_found']/total*100, 1)],
                    ['ìš°í¸ë²ˆí˜¸ ë³´ìœ ', self.stats['postal_code_found'], 
                     round(self.stats['postal_code_found']/total*100, 1)],
                    ['JSONë§Œ ì‚¬ìš©', self.stats['json_info_used'], 
                     round(self.stats['json_info_used']/total*100, 1)],
                    ['í¬ë¡¤ë§ ìˆ˜í–‰', self.stats['crawling_performed'], 
                     round(self.stats['crawling_performed']/total*100, 1)],
                    ['AI í˜¸ì¶œ íšŸìˆ˜', self.stats['api_calls_made'], '-'],
                    ['ì¤‘ë³µ ì œê±°', self.stats['duplicates_removed'], '-']
                ]
                
                df_stats = pd.DataFrame(stats_data[1:], columns=stats_data[0])
                df_stats.to_excel(writer, sheet_name='í†µê³„', index=False)
                
                # ì¶”ì¶œë°©ë²•ë³„ ë¶„ë¥˜ ì‹œíŠ¸
                method_summary = {}
                for result in results:
                    method = result.get('ì¶”ì¶œë°©ë²•', 'Unknown')
                    method_summary[method] = method_summary.get(method, 0) + 1
                
                df_methods = pd.DataFrame(list(method_summary.items()), 
                                        columns=['ì¶”ì¶œë°©ë²•', 'ê¸°ê´€ìˆ˜'])
                df_methods.to_excel(writer, sheet_name='ì¶”ì¶œë°©ë²•ë³„', index=False)
            
            print(f"ğŸ“Š ìµœì¢… Excel ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_filename}")
            return output_filename
            
        except Exception as e:
            print(f"âŒ Excel ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def create_comprehensive_report(self, results):
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± (JSON + Excel)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print(f"\nğŸ“‹ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # 1. JSON íŒŒì¼ ì €ì¥
        json_file = self.save_merged_results_to_json(results, 
                                                   f"enhanced_final_data_{timestamp}.json")
        
        # 2. Excel ë¦¬í¬íŠ¸ ìƒì„±
        excel_file = self.create_final_excel_report(results, 
                                                   f"enhanced_final_report_{timestamp}.xlsx")
        
        return json_file, excel_file
    
    # ğŸ”„ í†µê³„ ì¶œë ¥ ë©”ì„œë“œ ìˆ˜ì •
    def print_progress_stats(self):
        """ì§„í–‰ ìƒí™© í†µê³„ ì¶œë ¥ (URL ê²€ìƒ‰ í¬í•¨)"""
        print(f"\nğŸ“ˆ í˜„ì¬ ì§„í–‰ ìƒí™©:")
        print(f"   ğŸ”„ ì²˜ë¦¬ ì™„ë£Œ: {self.stats['total_processed']}ê°œ")
        print(f"   ğŸ” URL ê²€ìƒ‰ ìˆ˜í–‰: {self.stats['url_search_performed']}ê°œ")
        print(f"   ğŸŒ URL ë°œê²¬: {self.stats['url_found']}ê°œ")
        print(f"   ğŸ“‹ JSON ì •ë³´ í™œìš©: {self.stats['json_info_used']}ê°œ")
        print(f"   ğŸŒ ì¶”ê°€ í¬ë¡¤ë§: {self.stats['crawling_performed']}ê°œ")
        print(f"   ğŸ“ ì „í™”ë²ˆí˜¸ ë°œê²¬: {self.stats['phone_found']}ê°œ")
        print(f"   ğŸ“  íŒ©ìŠ¤ë²ˆí˜¸ ë°œê²¬: {self.stats['fax_found']}ê°œ")
        print(f"   ğŸ“§ ì´ë©”ì¼ ë°œê²¬: {self.stats['email_found']}ê°œ")
        print(f"   ğŸ  ì£¼ì†Œ ë°œê²¬: {self.stats['address_found']}ê°œ")
        print(f"   ğŸ“® ìš°í¸ë²ˆí˜¸ ë°œê²¬: {self.stats['postal_code_found']}ê°œ")
        print(f"   ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {self.stats['duplicates_removed']}ê°œ")
        if self.use_ai:
            print(f"   ğŸ¤– AI í˜¸ì¶œ: {self.stats['api_calls_made']}íšŒ")
            print(f"   ğŸ¤– AI ë³´ì™„: {self.stats['ai_enhanced']}íšŒ")
    
    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥ (URL ê²€ìƒ‰ í¬í•¨)"""
        print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! ìµœì¢… í†µê³„:")
        print(f"=" * 60)
        total = max(self.stats['total_processed'], 1)
        print(f"ğŸ“Š ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ ê¸°ê´€")
        print(f"ğŸ” URL ê²€ìƒ‰ ìˆ˜í–‰: {self.stats['url_search_performed']}ê°œ ({(self.stats['url_search_performed']/total)*100:.1f}%)")
        print(f"ğŸŒ URL ë°œê²¬ë¥ : {(self.stats['url_found']/max(self.stats['url_search_performed'], 1))*100:.1f}%")
        print(f"ğŸ“‹ JSON ì •ë³´ë§Œ ì‚¬ìš©: {self.stats['json_info_used']}ê°œ ({(self.stats['json_info_used']/total)*100:.1f}%)")
        print(f"ğŸŒ ì¶”ê°€ í¬ë¡¤ë§ ìˆ˜í–‰: {self.stats['crawling_performed']}ê°œ ({(self.stats['crawling_performed']/total)*100:.1f}%)")
        print(f"ğŸ“ ì „í™”ë²ˆí˜¸ ë°œê²¬ë¥ : {(self.stats['phone_found']/total)*100:.1f}%")
        print(f"ğŸ“  íŒ©ìŠ¤ë²ˆí˜¸ ë°œê²¬ë¥ : {(self.stats['fax_found']/total)*100:.1f}%")
        print(f"ğŸ“§ ì´ë©”ì¼ ë°œê²¬ë¥ : {(self.stats['email_found']/total)*100:.1f}%")
        print(f"ğŸ  ì£¼ì†Œ ë°œê²¬ë¥ : {(self.stats['address_found']/total)*100:.1f}%")
        print(f"ğŸ“® ìš°í¸ë²ˆí˜¸ ë°œê²¬ë¥ : {(self.stats['postal_code_found']/total)*100:.1f}%")
        print(f"ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {self.stats['duplicates_removed']}ê°œ")
        if self.use_ai:
            print(f"ğŸ¤– ì´ AI í˜¸ì¶œ: {self.stats['api_calls_made']}íšŒ")
            print(f"ğŸ¤– AI ë³´ì™„ ì„±ê³µ: {self.stats['ai_enhanced']}íšŒ")
        print(f"=" * 60)

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
# ğŸ”„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜ ìˆ˜ì •
async def extract_enhanced_details_async(json_file_path, api_key=None, test_mode=False, test_count=10, use_selenium=True, headless=False):
    """
    í–¥ìƒëœ ìƒì„¸ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜ (URL ê²€ìƒ‰ ê¸°ëŠ¥ í¬í•¨)
    
    Args:
        json_file_path: JSON íŒŒì¼ ê²½ë¡œ
        api_key: Gemini API í‚¤ (ì„ íƒì‚¬í•­)
        test_mode: í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì—¬ë¶€
        test_count: í…ŒìŠ¤íŠ¸ ëª¨ë“œì‹œ ì²˜ë¦¬í•  ê¸°ê´€ ìˆ˜
        use_selenium: Selenium ì‚¬ìš© ì—¬ë¶€ (URL ê²€ìƒ‰ìš©)
        headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
    
    Returns:
        tuple: (ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, JSON íŒŒì¼ëª…, Excel íŒŒì¼ëª…, í†µê³„ ì •ë³´)
    """
    print("ğŸš€ Enhanced Detail Extractor ì‹œì‘ (URL ê²€ìƒ‰ + í¬ë¡¤ë§)")
    print("=" * 60)
    
    # ì¶”ì¶œê¸° ì´ˆê¸°í™” (URL ê²€ìƒ‰ ê¸°ëŠ¥ í¬í•¨)
    extractor = EnhancedDetailExtractor(api_key=api_key, use_selenium=use_selenium, headless=headless)
    
    start_time = time.time()
    
    # ì²˜ë¦¬ ì‹¤í–‰
    results = await extractor.process_json_file_async(json_file_path, test_mode=test_mode, test_count=test_count)
    
    if results:
        # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        json_file, excel_file = extractor.create_comprehensive_report(results)
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        extractor.print_final_stats()
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        
        return results, json_file, excel_file, extractor.stats
    else:
        print("âŒ ì²˜ë¦¬ ì‹¤íŒ¨")
        return None, None, None, extractor.stats

def extract_enhanced_details(json_file_path, api_key=None, test_mode=False, test_count=10, use_selenium=True, headless=False):
    """
    í–¥ìƒëœ ìƒì„¸ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜ (URL ê²€ìƒ‰ + í¬ë¡¤ë§)
    """
    return asyncio.run(extract_enhanced_details_async(
        json_file_path, api_key, test_mode, test_count, use_selenium, headless
    ))

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë…ë¦½ ì‹¤í–‰ìš©)"""
    print("ğŸ  Enhanced Detail Extractor")
    print("=" * 60)
    
    # .envì—ì„œ API í‚¤ í™•ì¸
    api_key_from_env = os.getenv('GEMINI_API_KEY')
    if api_key_from_env:
        print(f"ğŸ”‘ .envì—ì„œ API í‚¤ ë°œê²¬: {api_key_from_env[:10]}...")
        use_env_key = input("ğŸ¤– .envì˜ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").strip().lower()
        if use_env_key in ['', 'y', 'yes']:
            api_key = None  # Noneì´ë©´ __init__ì—ì„œ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
        else:
            api_key = input("Gemini API í‚¤ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”: ").strip() or None
    else:
        print("âš ï¸ .envì—ì„œ GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        api_key = input("Gemini API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì—”í„° ì‹œ ì •ê·œì‹ ì „ìš© ëª¨ë“œ): ").strip() or None
    
    # JSON íŒŒì¼ ê²½ë¡œ ì…ë ¥
    json_file = input("JSON íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    if not os.path.exists(json_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file}")
        return
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ
    test_mode = input("í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower() == 'y'
    test_count = 10
    if test_mode:
        try:
            test_count = int(input(f"í…ŒìŠ¤íŠ¸í•  ê¸°ê´€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: {test_count}): ").strip() or test_count)
        except ValueError:
            test_count = 10
    
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
    results, json_file, excel_file, stats = extract_enhanced_details(
        json_file, api_key=api_key, test_mode=test_mode, test_count=test_count
    )
    
    if json_file and excel_file:
        print(f"\nğŸ“ ìµœì¢… ê²°ê³¼ íŒŒì¼ë“¤:")
        print(f"   ğŸ“„ JSON: {json_file}")
        print(f"   ğŸ“Š Excel: {excel_file}")
    else:
        print(f"\nâŒ íŒŒì¼ ìƒì„± ì‹¤íŒ¨")

if __name__ == "__main__":
    main() 


# # // ... existing imports ...

# # ai_helpers.pyì˜ AIModelManager í™œìš©
# from ai_helpers import AIModelManager, extract_with_gemini_text

# # constants.pyì—ì„œ AI ê´€ë ¨ ì„¤ì • import
# from constants import (
#     AI_MODEL_CONFIG, 
#     TPM_LIMITS, 
#     CRAWLING_CONFIG,
#     EXCLUDE_DOMAINS,
#     EXCLUDE_URL_PATTERNS
# )

# class EnhancedDetailExtractor:
#     """Gemini 1.5-flash ì œì•½ì‚¬í•­ì„ ê³ ë ¤í•œ ì§€ëŠ¥í˜• ì›¹ ë¶„ì„ ì¶”ì¶œê¸°"""
    
#     def __init__(self, api_key=None, use_selenium=True, headless=False):
#         """ì´ˆê¸°í™” (AI ì œì•½ì‚¬í•­ ê³ ë ¤)"""
#         # ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œ...
#         self.session = CrawlerUtils.setup_requests_session()
#         self.validator = ContactValidator()
        
#         # ğŸ”§ ê°œì„ : constants.pyì˜ TPM ì„¤ì • ì‚¬ìš©
#         self.tpm_manager = TPMManager(requests_per_minute=TPM_LIMITS["requests_per_minute"])
        
#         # ğŸ”§ ê°œì„ : ai_helpers.pyì˜ AIModelManager í™œìš©
#         self.ai_model_manager = None
#         if api_key or os.getenv('GEMINI_API_KEY'):
#             try:
#                 self.ai_model_manager = AIModelManager()
#                 self.use_ai = True
#                 self.use_ai_web_analysis = True
#                 print("ğŸ¤– AI ëª¨ë¸ ê´€ë¦¬ì ì´ˆê¸°í™” ì„±ê³µ (ai_helpers.py ê¸°ë°˜)")
#             except Exception as e:
#                 print(f"âŒ AI ëª¨ë¸ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
#                 self.ai_model_manager = None
#                 self.use_ai = False
#                 self.use_ai_web_analysis = False
#         else:
#             self.use_ai = False
#             self.use_ai_web_analysis = False
#             print("ğŸ”§ ì •ê·œì‹ ì „ìš© ëª¨ë“œ (API í‚¤ ì—†ìŒ)")
        
#         # ğŸ†• Gemini 1.5-flash ì œì•½ì‚¬í•­ ê³ ë ¤í•œ ì„¤ì •
#         self.max_input_tokens = 30000  # 32K í† í° ì œí•œì˜ ì•ˆì „ ë§ˆì§„
#         self.max_html_chars = 60000    # ëŒ€ëµ 30K í† í°ì— í•´ë‹¹ (í•œê¸€ ê¸°ì¤€)
#         self.max_text_chars = 15000    # ì—°ë½ì²˜ ì¶”ì¶œìš© í…ìŠ¤íŠ¸ ì œí•œ
        
#         # Selenium ê´€ë ¨ ì„¤ì •
#         self.use_selenium = use_selenium
#         self.headless = headless
#         self.driver = None
#         self.exclude_domains = EXCLUDE_DOMAINS
        
#         # ê¸°ì¡´ í†µê³„ ì •ë³´ì— AI ê´€ë ¨ ì¶”ê°€
#         self.stats = {
#             'total_processed': 0,
#             'json_info_used': 0,
#             'crawling_performed': 0,
#             'url_search_performed': 0,
#             'url_found': 0,
#             'phone_found': 0,
#             'fax_found': 0,
#             'email_found': 0,
#             'address_found': 0,
#             'postal_code_found': 0,
#             'ai_enhanced': 0,
#             'duplicates_removed': 0,
#             'api_calls_made': 0,
#             'token_limit_exceeded': 0,  # ğŸ†• í† í° ì œí•œ ì´ˆê³¼ íšŸìˆ˜
#             'tpm_wait_count': 0,        # ğŸ†• TPM ëŒ€ê¸° íšŸìˆ˜
#         }
        
#         self.ai_logger = LoggerUtils.setup_ai_logger()
#         self.patterns = {
#             'phone': PHONE_EXTRACTION_PATTERNS,
#             'fax': FAX_EXTRACTION_PATTERNS,
#             'email': EMAIL_EXTRACTION_PATTERNS,
#             'address': ADDRESS_EXTRACTION_PATTERNS
#         }
        
#         self.merged_results = []

#     # ğŸ”§ ê°œì„ : í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ í•¨ìˆ˜
#     def estimate_token_count(self, text: str) -> int:
#         """í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •"""
#         # í•œê¸€/í•œì: 1ê¸€ì â‰ˆ 1.5í† í°, ì˜ë¬¸: 1ë‹¨ì–´ â‰ˆ 1í† í°, íŠ¹ìˆ˜ë¬¸ì ê³ ë ¤
#         korean_chars = len(re.findall(r'[ê°€-í£]', text))
#         english_words = len(re.findall(r'[a-zA-Z]+', text))
#         other_chars = len(text) - korean_chars - sum(len(word) for word in re.findall(r'[a-zA-Z]+', text))
        
#         estimated_tokens = int(korean_chars * 1.5 + english_words + other_chars * 0.5)
#         return estimated_tokens

#     def truncate_text_by_tokens(self, text: str, max_tokens: int = None) -> str:
#         """í† í° ìˆ˜ ì œí•œì— ë§ì¶° í…ìŠ¤íŠ¸ ìë¥´ê¸°"""
#         if max_tokens is None:
#             max_tokens = self.max_input_tokens
        
#         current_tokens = self.estimate_token_count(text)
        
#         if current_tokens <= max_tokens:
#             return text
        
#         # í† í° ìˆ˜ê°€ ì´ˆê³¼í•˜ë©´ ë¹„ìœ¨ë¡œ ìë¥´ê¸°
#         ratio = max_tokens / current_tokens * 0.9  # ì•ˆì „ ë§ˆì§„ 10%
#         target_length = int(len(text) * ratio)
        
#         # ì•ë¶€ë¶„ 70%, ë’·ë¶€ë¶„ 30%ë¡œ ë‚˜ëˆ„ì–´ ì¤‘ìš” ì •ë³´ ë³´ì¡´
#         front_length = int(target_length * 0.7)
#         back_length = target_length - front_length
        
#         if back_length > 0:
#             truncated = text[:front_length] + "\n... (ì¤‘ê°„ ìƒëµ) ...\n" + text[-back_length:]
#         else:
#             truncated = text[:target_length]
        
#         self.stats['token_limit_exceeded'] += 1
#         print(f"âš ï¸ í† í° ì œí•œìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶•ì†Œ: {current_tokens} â†’ {self.estimate_token_count(truncated)} í† í°")
        
#         return truncated

#     # ğŸ”§ ê°œì„ : TPM ê´€ë¦¬ í´ë˜ìŠ¤ (constants.py ì„¤ì • ì‚¬ìš©)
#     async def wait_for_tpm_limit(self):
#         """TPM ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•œ ëŒ€ê¸°"""
#         await self.tpm_manager.wait_if_needed()
#         self.stats['tpm_wait_count'] += 1

#     # ğŸ”§ ê°œì„ : AI ê¸°ë°˜ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (í† í° ì œí•œ ê³ ë ¤)
#     async def search_homepage_url_with_ai_analysis(self, organization_name: str) -> str:
#         """Gemini AI ê¸°ë°˜ ì§€ëŠ¥í˜• í™ˆí˜ì´ì§€ ê²€ìƒ‰ (ì œì•½ì‚¬í•­ ê³ ë ¤)"""
#         if not self.use_selenium or not self.ai_model_manager:
#             print(f"âš ï¸ Selenium ë˜ëŠ” AI ë¹„í™œì„±í™”ë¨: {organization_name}")
#             return ""
        
#         if not self.driver:
#             self.setup_selenium_driver()
        
#         if not self.driver:
#             print(f"âš ï¸ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {organization_name}")
#             return ""
        
#         try:
#             # ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ (ë” ì œí•œì ìœ¼ë¡œ)
#             search_queries = [
#                 f'{organization_name} í™ˆí˜ì´ì§€',
#                 f'{organization_name} site:or.kr',
#                 f'{organization_name} ê³µì‹ì‚¬ì´íŠ¸'
#             ]
            
#             for i, query in enumerate(search_queries, 1):
#                 print(f"ğŸ” AI ê²€ìƒ‰ {i}/{len(search_queries)}: {query}")
                
#                 result_url = await self._perform_ai_guided_search_optimized(query, organization_name)
#                 if result_url:
#                     self.stats['url_found'] += 1
#                     return result_url
                
#                 # TPM ì œí•œ ê³ ë ¤í•œ ì§€ì—°
#                 if i < len(search_queries):
#                     await asyncio.sleep(3)
            
#             return ""
            
#         except Exception as e:
#             print(f"âŒ AI ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨ ({organization_name}): {e}")
#             return ""

#     async def _perform_ai_guided_search_optimized(self, query: str, organization_name: str) -> str:
#         """ìµœì í™”ëœ AI ê°€ì´ë“œ ê²€ìƒ‰ (í† í° ì œí•œ ê³ ë ¤)"""
#         try:
#             # 1. ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤í–‰
#             self.driver.get('https://www.naver.com')
#             time.sleep(2)
            
#             search_box = WebDriverWait(self.driver, 10).until(
#                 EC.presence_of_element_located((By.ID, "query"))
#             )
            
#             search_box.clear()
#             search_box.send_keys(query)
#             search_box.send_keys(Keys.RETURN)
#             time.sleep(4)
            
#             # 2. í˜ì´ì§€ ì†ŒìŠ¤ íšë“ ë° ì „ì²˜ë¦¬
#             page_source = self.driver.page_source
            
#             # 3. HTMLì„ í† í° ì œí•œì— ë§ê²Œ ì „ì²˜ë¦¬
#             processed_html = self._preprocess_html_for_ai(page_source)
            
#             # 4. AI ë¶„ì„ (í† í° ì œí•œ ê³ ë ¤)
#             if self.use_ai_web_analysis and processed_html:
#                 homepage_url = await self._analyze_search_results_optimized(
#                     processed_html, organization_name, query
#                 )
#                 if homepage_url:
#                     return homepage_url
            
#             # 5. í´ë°±: ê¸°ì¡´ ë°©ì‹
#             print(f"âš ï¸ AI ë¶„ì„ ì‹¤íŒ¨, í´ë°± ì‹¤í–‰: {organization_name}")
#             return self._fallback_search_method(organization_name)
            
#         except Exception as e:
#             print(f"âŒ ìµœì í™”ëœ AI ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
#             return ""

#     def _preprocess_html_for_ai(self, html_content: str) -> str:
#         """AI ë¶„ì„ì„ ìœ„í•œ HTML ì „ì²˜ë¦¬ (í† í° ì œí•œ ê³ ë ¤)"""
#         try:
#             soup = BeautifulSoup(html_content, 'html.parser')
            
#             # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
#             for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'comment']):
#                 tag.decompose()
            
#             # ê²€ìƒ‰ ê²°ê³¼ ì˜ì—­ë§Œ ì¶”ì¶œ
#             main_content = ""
#             for selector in ['.main_pack', '.content_area', '.result_area', '.search_area', '.api_subject_bx']:
#                 element = soup.select_one(selector)
#                 if element:
#                     main_content = str(element)
#                     break
            
#             if not main_content:
#                 # ë©”ì¸ ì˜ì—­ì„ ì°¾ì§€ ëª»í•˜ë©´ body ì „ì²´ ì‚¬ìš©
#                 body = soup.find('body')
#                 main_content = str(body) if body else html_content
            
#             # í† í° ì œí•œì— ë§ì¶° í…ìŠ¤íŠ¸ ì¶•ì†Œ
#             processed_content = self.truncate_text_by_tokens(main_content, self.max_input_tokens - 1000)  # í”„ë¡¬í”„íŠ¸ ì—¬ìœ ë¶„
            
#             return processed_content
            
#         except Exception as e:
#             print(f"âš ï¸ HTML ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
#             # ì˜¤ë¥˜ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ìë¥´ê¸°
#             return self.truncate_text_by_tokens(html_content, self.max_input_tokens - 1000)

#     async def _analyze_search_results_optimized(self, html_content: str, organization_name: str, query: str) -> str:
#         """ìµœì í™”ëœ AI HTML ë¶„ì„ (ai_helpers.py í™œìš©)"""
#         if not self.ai_model_manager:
#             return ""
        
#         try:
#             # TPM ì œí•œ ëŒ€ê¸°
#             await self.wait_for_tpm_limit()
            
#             # ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
#             prompt_template = self._create_optimized_html_analysis_prompt(organization_name, query)
            
#             # ai_helpers.pyì˜ extract_with_gemini_text í™œìš©
#             result_text = await extract_with_gemini_text(html_content, prompt_template)
            
#             self.stats['api_calls_made'] += 1
            
#             if result_text:
#                 # ì‘ë‹µ íŒŒì‹±
#                 homepage_url = self._parse_ai_html_response_optimized(result_text, organization_name)
                
#                 if homepage_url:
#                     print(f"ğŸ¤– AIê°€ ë°œê²¬í•œ í™ˆí˜ì´ì§€: {organization_name} -> {homepage_url}")
#                     return homepage_url
            
#             return ""
            
#         except Exception as e:
#             print(f"âŒ ìµœì í™”ëœ AI HTML ë¶„ì„ ì˜¤ë¥˜ ({organization_name}): {e}")
#             return ""

#     def _create_optimized_html_analysis_prompt(self, organization_name: str, query: str) -> str:
#         """ìµœì í™”ëœ HTML ë¶„ì„ í”„ë¡¬í”„íŠ¸ (í† í° ì ˆì•½)"""
#         prompt = f"""ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ '{organization_name}'ì˜ ê³µì‹ í™ˆí˜ì´ì§€ URLì„ ì°¾ì•„ì£¼ì„¸ìš”.

# ê²€ìƒ‰ì–´: {query}
# ê¸°ê´€ëª…: {organization_name}

# ê·œì¹™:
# 1. ê³µì‹ í™ˆí˜ì´ì§€ë§Œ ì„ íƒ (.or.kr, .go.kr, .ac.kr, .org, .church ìš°ì„ )
# 2. ë¸”ë¡œê·¸, ì¹´í˜, SNS ì œì™¸
# 3. ê¸°ê´€ëª… í¬í•¨ ë„ë©”ì¸ ìš°ì„ 

# ì‘ë‹µ í˜•ì‹:
# URL: [í™ˆí˜ì´ì§€ URL ë˜ëŠ” "ì—†ìŒ"]

# HTML:
# {{text_content}}

# ìœ„ HTMLì—ì„œ '{organization_name}'ì˜ ê³µì‹ í™ˆí˜ì´ì§€ URLì„ ì°¾ì•„ì£¼ì„¸ìš”."""

#         return prompt

#     def _parse_ai_html_response_optimized(self, ai_response: str, organization_name: str) -> str:
#         """ìµœì í™”ëœ AI HTML ë¶„ì„ ì‘ë‹µ íŒŒì‹±"""
#         try:
#             lines = ai_response.split('\n')
            
#             for line in lines:
#                 line = line.strip()
                
#                 # URL ë¼ì¸ ì°¾ê¸°
#                 if line.startswith('URL:') or line.startswith('url:'):
#                     url_part = line.split(':', 1)[1].strip()
#                     if url_part and url_part != "ì—†ìŒ" and url_part.startswith(('http://', 'https://')):
#                         if self._is_valid_homepage_url(url_part, organization_name):
#                             return url_part
                
#                 # ì§ì ‘ URL íŒ¨í„´ ì°¾ê¸°
#                 elif line.startswith(('http://', 'https://')):
#                     if self._is_valid_homepage_url(line, organization_name):
#                         return line
            
#             return ""
            
#         except Exception as e:
#             print(f"âŒ ìµœì í™”ëœ AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ({organization_name}): {e}")
#             return ""

#     # ğŸ”§ ê°œì„ : ì—°ë½ì²˜ ì¶”ì¶œë„ í† í° ì œí•œ ê³ ë ¤
#     async def crawl_and_extract_with_ai_optimized(self, org_data):
#         """ìµœì í™”ëœ AI ê¸°ë°˜ í¬ë¡¤ë§ ë° ì¶”ì¶œ"""
#         org_name = org_data.get('name', 'Unknown')
#         homepage_url = org_data.get('homepage', '')
        
#         if not homepage_url or homepage_url == 'ì •ë³´í™•ì¸ ì•ˆë¨':
#             return {}
        
#         # í™ˆí˜ì´ì§€ í¬ë¡¤ë§
#         homepage_text = self.crawl_homepage_if_needed(homepage_url, org_name)
        
#         if not homepage_text:
#             return {}
        
#         # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ì—°ë½ì²˜ ì¶”ì¶œìš©)
#         if len(homepage_text) > self.max_text_chars:
#             # Footer ìš°ì„  ì¶”ì¶œ ì‹œë„
#             soup = BeautifulSoup(homepage_text, 'html.parser')
#             footer = soup.find('footer')
#             contact_section = soup.find(['section', 'div'], class_=re.compile(r'contact|ì—°ë½ì²˜', re.I))
            
#             if footer:
#                 homepage_text = footer.get_text(separator=' ', strip=True)
#             elif contact_section:
#                 homepage_text = contact_section.get_text(separator=' ', strip=True)
#             else:
#                 homepage_text = homepage_text[:self.max_text_chars]
            
#             print(f"ğŸ“ ì—°ë½ì²˜ ì¶”ì¶œìš© í…ìŠ¤íŠ¸ ì œí•œ: {org_name} ({len(homepage_text)} ë¬¸ì)")
        
#         # AI ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
#         if self.use_ai and self.ai_model_manager:
#             ai_results = await self._extract_contact_with_optimized_ai(
#                 homepage_text, org_name, homepage_url
#             )
#             return ai_results
#         else:
#             # AI ì—†ìœ¼ë©´ ê¸°ì¡´ ì •ê·œì‹ ë°©ì‹
#             return self.extract_contact_info_sync(homepage_text, org_name)

#     async def _extract_contact_with_optimized_ai(self, text: str, org_name: str, homepage_url: str):
#         """ìµœì í™”ëœ AI ê¸°ë°˜ ì—°ë½ì²˜ ì¶”ì¶œ"""
#         try:
#             # TPM ì œí•œ ëŒ€ê¸°
#             await self.wait_for_tpm_limit()
            
#             # í† í° ì œí•œ ê³ ë ¤í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
#             processed_text = self.truncate_text_by_tokens(text, self.max_input_tokens - 800)  # í”„ë¡¬í”„íŠ¸ ì—¬ìœ ë¶„
            
#             # ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
#             prompt_template = f"""'{org_name}' ì—°ë½ì²˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

# ê¸°ê´€ëª…: {org_name}
# í™ˆí˜ì´ì§€: {homepage_url}

# ì¶”ì¶œ ê·œì¹™:
# - í•œêµ­ ì „í™”ë²ˆí˜¸ë§Œ (02-1234-5678 í˜•ì‹)
# - ìœ íš¨í•œ ì´ë©”ì¼ë§Œ
# - ì™„ì „í•œ ì£¼ì†Œë§Œ
# - ì •ë³´ ì—†ìœ¼ë©´ "ì—†ìŒ"

# í˜•ì‹:
# ì „í™”ë²ˆí˜¸: [ë²ˆí˜¸ ë˜ëŠ” "ì—†ìŒ"]
# íŒ©ìŠ¤ë²ˆí˜¸: [ë²ˆí˜¸ ë˜ëŠ” "ì—†ìŒ"]  
# ì´ë©”ì¼: [ì´ë©”ì¼ ë˜ëŠ” "ì—†ìŒ"]
# ì£¼ì†Œ: [ì£¼ì†Œ ë˜ëŠ” "ì—†ìŒ"]
# ìš°í¸ë²ˆí˜¸: [ìš°í¸ë²ˆí˜¸ ë˜ëŠ” "ì—†ìŒ"]

# í…ìŠ¤íŠ¸:
# {{text_content}}

# ì •í™•í•œ ì—°ë½ì²˜ë§Œ ì¶”ì¶œí•˜ì„¸ìš”."""

#             # ai_helpers.py í™œìš©
#             result_text = await extract_with_gemini_text(processed_text, prompt_template)
            
#             self.stats['api_calls_made'] += 1
            
#             if result_text:
#                 contact_info = self._parse_advanced_ai_response(result_text)
                
#                 if contact_info:
#                     self.stats['ai_enhanced'] += 1
#                     print(f"ğŸ¤– AI ì—°ë½ì²˜ ì¶”ì¶œ ì„±ê³µ: {org_name}")
                
#                 return contact_info
            
#             return {}
            
#         except Exception as e:
#             print(f"âŒ ìµœì í™”ëœ AI ì¶”ì¶œ ì˜¤ë¥˜ ({org_name}): {e}")
#             return {}

#     # ğŸ”§ ê¸°ì¡´ ë©”ì„œë“œë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€...
#     def _parse_advanced_ai_response(self, ai_response: str) -> dict:
#         """ê³ ê¸‰ AI ì‘ë‹µ íŒŒì‹±"""
#         result = {
#             'phone': [],
#             'fax': [],
#             'email': [],
#             'address': [],
#             'postal_code': []
#         }
        
#         try:
#             lines = ai_response.split('\n')
            
#             for line in lines:
#                 line = line.strip()
#                 if ':' in line:
#                     key, value = line.split(':', 1)
#                     key = key.strip().lower()
#                     value = value.strip()
                    
#                     if value and value != "ì—†ìŒ":
#                         if 'ì „í™”ë²ˆí˜¸' in key or 'phone' in key:
#                             result['phone'].append(value)
#                         elif 'íŒ©ìŠ¤' in key or 'fax' in key:
#                             result['fax'].append(value)
#                         elif 'ì´ë©”ì¼' in key or 'email' in key:
#                             result['email'].append(value)
#                         elif 'ì£¼ì†Œ' in key or 'address' in key:
#                             result['address'].append(value)
#                         elif 'ìš°í¸ë²ˆí˜¸' in key or 'postal' in key:
#                             result['postal_code'].append(value)
            
#             return result
            
#         except Exception as e:
#             print(f"âŒ ê³ ê¸‰ AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
#             return result

#     # ğŸ”§ ê°œì„ : ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ ìˆ˜ì •
#     async def process_single_organization_enhanced(self, org_data):
#         """ì™„ì „íˆ ìµœì í™”ëœ ë‹¨ì¼ ê¸°ê´€ ì²˜ë¦¬"""
#         org_name = org_data.get('name', 'Unknown')
#         homepage_url = org_data.get('homepage', '')
        
#         try:
#             start_time = time.time()
            
#             # 1. í™ˆí˜ì´ì§€ URL í™•ì¸/AI ê²€ìƒ‰
#             if not homepage_url or homepage_url == 'ì •ë³´í™•ì¸ ì•ˆë¨' or not homepage_url.startswith(('http://', 'https://')):
#                 print(f"ğŸ¤– AI í™ˆí˜ì´ì§€ ê²€ìƒ‰: {org_name}")
#                 homepage_url = await self.search_homepage_url_with_ai_analysis(org_name)
#                 self.stats['url_search_performed'] += 1
                
#                 if homepage_url:
#                     org_data['homepage'] = homepage_url
#                     print(f"âœ… AI ë°œê²¬: {org_name} -> {homepage_url}")
#                 else:
#                     print(f"âŒ í™ˆí˜ì´ì§€ ì—†ìŒ: {org_name}")
#             else:
#                 print(f"ğŸ“‹ ê¸°ì¡´ í™ˆí˜ì´ì§€: {org_name}")
            
#             # 2. JSON ë°ì´í„° ì¶”ì¶œ
#             json_info = self.extract_from_json_data(org_data)
            
#             # 3. í•„ìš”ì‹œ AI í¬ë¡¤ë§
#             ai_info = {}
#             if homepage_url and homepage_url.startswith(('http://', 'https://')):
#                 if self.needs_additional_crawling(json_info):
#                     print(f"ğŸŒ AI í¬ë¡¤ë§: {org_name}")
                    
#                     try:
#                         ai_info = await asyncio.wait_for(
#                             self.crawl_and_extract_with_ai_optimized(org_data), 
#                             timeout=CRAWLING_CONFIG["async_timeout"]
#                         )
#                     except asyncio.TimeoutError:
#                         print(f"â° AI í¬ë¡¤ë§ ì‹œê°„ ì´ˆê³¼: {org_name}")
#                         ai_info = {}
#                 else:
#                     print(f"ğŸ“‹ JSON ì¶©ë¶„: {org_name}")
            
#             # 4. ë°ì´í„° ë³‘í•© ë° ê²°ê³¼ í¬ë§·íŒ…
#             merged_data = self.merge_contact_data(json_info, ai_info)
#             final_result = self.format_final_result(org_data, merged_data)
            
#             elapsed_time = time.time() - start_time
#             print(f"âœ… AI ì²˜ë¦¬ ì™„ë£Œ: {org_name} - {elapsed_time:.1f}ì´ˆ")
            
#             return final_result
            
#         except Exception as e:
#             print(f"âŒ AI ì²˜ë¦¬ ì‹¤íŒ¨ ({org_name}): {str(e)[:100]}...")
#             return {
#                 'ê¸°ê´€ëª…': org_name,
#                 'í™ˆí˜ì´ì§€': homepage_url,
#                 'ì£¼ì†Œ': 'ì •ë³´í™•ì¸ ì•ˆë¨',
#                 'ìš°í¸ë²ˆí˜¸': 'ì •ë³´í™•ì¸ ì•ˆë¨',
#                 'ì „í™”ë²ˆí˜¸': 'ì •ë³´í™•ì¸ ì•ˆë¨',
#                 'íŒ©ìŠ¤ë²ˆí˜¸': 'ì •ë³´í™•ì¸ ì•ˆë¨',
#                 'ì´ë©”ì¼': 'ì •ë³´í™•ì¸ ì•ˆë¨',
#                 'ì¶”ì¶œë°©ë²•': 'ì‹¤íŒ¨',
#                 'ì¶”ì¶œìƒíƒœ': 'ì‹¤íŒ¨'
#             }

#     # ğŸ”§ ê°œì„ : í†µê³„ ì¶œë ¥ì— í† í°/TPM ì •ë³´ ì¶”ê°€
#     def print_final_stats(self):
#         """ìµœì¢… í†µê³„ ì¶œë ¥ (í† í°/TPM ì œí•œ ì •ë³´ í¬í•¨)"""
#         print(f"\nğŸ‰ AI ê¸°ë°˜ ì²˜ë¦¬ ì™„ë£Œ! ìµœì¢… í†µê³„:")
#         print(f"=" * 60)
#         total = max(self.stats['total_processed'], 1)
#         print(f"ğŸ“Š ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ ê¸°ê´€")
#         print(f"ğŸ¤– AI ì›¹ ë¶„ì„: {'í™œì„±í™”' if self.use_ai_web_analysis else 'ë¹„í™œì„±í™”'}")
#         print(f"ğŸ” URL ê²€ìƒ‰ ìˆ˜í–‰: {self.stats['url_search_performed']}ê°œ")
#         print(f"ğŸŒ URL ë°œê²¬ë¥ : {(self.stats['url_found']/max(self.stats['url_search_performed'], 1))*100:.1f}%")
#         print(f"ğŸ“ ì „í™”ë²ˆí˜¸ ë°œê²¬ë¥ : {(self.stats['phone_found']/total)*100:.1f}%")
#         print(f"ğŸ“  íŒ©ìŠ¤ë²ˆí˜¸ ë°œê²¬ë¥ : {(self.stats['fax_found']/total)*100:.1f}%")
#         print(f"ğŸ“§ ì´ë©”ì¼ ë°œê²¬ë¥ : {(self.stats['email_found']/total)*100:.1f}%")
#         print(f"ğŸ  ì£¼ì†Œ ë°œê²¬ë¥ : {(self.stats['address_found']/total)*100:.1f}%")
#         print(f"ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {self.stats['duplicates_removed']}ê°œ")
#         if self.use_ai:
#             print(f"ğŸ¤– ì´ AI í˜¸ì¶œ: {self.stats['api_calls_made']}íšŒ")
#             print(f"ğŸ¤– AI ë³´ì™„ ì„±ê³µ: {self.stats['ai_enhanced']}íšŒ")
#             print(f"âš ï¸ í† í° ì œí•œ ì´ˆê³¼: {self.stats['token_limit_exceeded']}íšŒ")
#             print(f"â° TPM ëŒ€ê¸°: {self.stats['tpm_wait_count']}íšŒ")
#         print(f"=" * 60)

# // ... existing code ...