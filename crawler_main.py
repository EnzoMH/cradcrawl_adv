"""
AI ê°•í™” í†µí•© í¬ë¡¤ë§ ì—”ì§„ v4.0
âœ… ê¸°ì¡´ ì „ë¬¸ ëª¨ë“ˆ + AI Agentic Workflow í†µí•©
âœ… ìµœê³  í’ˆì§ˆì˜ AI ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ
âœ… ì‹ ë¢°ë„ ì ìˆ˜ ë° ë‹¤ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'test'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'cralwer'))

import asyncio
import json
import time
import re
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

# í”„ë¡œì íŠ¸ ì„¤ì • import
from utils.settings import *
from utils.logger_utils import LoggerUtils
from utils.file_utils import FileUtils
from utils.phone_utils import PhoneUtils
from utils.crawler_utils import CrawlerUtils
from utils.ai_helpers import AIModelManager

# ì „ë¬¸ ëª¨ë“ˆë“¤ import (ê¸°ì¡´ ìœ ì§€)
try:
    from cralwer.fax_extractor import GoogleContactCrawler as FaxExtractor
    FAX_EXTRACTOR_AVAILABLE = True
    print("âœ… fax_extractor.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ fax_extractor.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    FAX_EXTRACTOR_AVAILABLE = False

try:
    from cralwer.phone_extractor import extract_phone_numbers, search_phone_number, setup_driver
    PHONE_EXTRACTOR_AVAILABLE = True
    print("âœ… phone_extractor.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ phone_extractor.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    PHONE_EXTRACTOR_AVAILABLE = False

try:
    from cralwer.url_extractor import HomepageParser
    URL_EXTRACTOR_AVAILABLE = True
    print("âœ… url_extractor.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ url_extractor.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    URL_EXTRACTOR_AVAILABLE = False

try:
    from utils.validator import ContactValidator, AIValidator
    VALIDATOR_AVAILABLE = True
    print("âœ… validator.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ validator.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    VALIDATOR_AVAILABLE = False

try:
    from database.database import get_database
    DATABASE_AVAILABLE = True
    print("âœ… database.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ database.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    DATABASE_AVAILABLE = False

# ==================== AI Agentic Workflow ì‹œìŠ¤í…œ í†µí•© ====================

class CrawlingStage(Enum):
    """í¬ë¡¤ë§ ë‹¨ê³„ ì •ì˜"""
    INITIALIZATION = "ì´ˆê¸°í™”"
    HOMEPAGE_SEARCH = "í™ˆí˜ì´ì§€_ê²€ìƒ‰"
    HOMEPAGE_ANALYSIS = "í™ˆí˜ì´ì§€_ë¶„ì„"
    CONTACT_PAGE_SEARCH = "ì—°ë½ì²˜í˜ì´ì§€_ê²€ìƒ‰"
    CONTACT_EXTRACTION = "ì—°ë½ì²˜_ì¶”ì¶œ"
    FAX_SEARCH = "íŒ©ìŠ¤_ê²€ìƒ‰"
    AI_VERIFICATION = "AI_ê²€ì¦"
    DATA_VALIDATION = "ë°ì´í„°_ê²€ì¦"
    COMPLETION = "ì™„ë£Œ"

@dataclass
class CrawlingContext:
    """í¬ë¡¤ë§ ì»¨í…ìŠ¤íŠ¸ (AI Agent ê°„ ê³µìœ  ë°ì´í„°)"""
    organization: Dict[str, Any]
    current_stage: CrawlingStage
    extracted_data: Dict[str, Any]
    ai_insights: Dict[str, Any]
    error_log: List[str]
    processing_time: float
    confidence_scores: Dict[str, float]

class AIAgent:
    """AI ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler=None):
        self.name = name
        self.ai_manager = ai_manager
        self.logger = logger
        self.parent_crawler = parent_crawler  # Optionalë¡œ ë³€ê²½
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    async def should_execute(self, context: CrawlingContext) -> bool:
        """ì‹¤í–‰ ì¡°ê±´ í™•ì¸"""
        return True
    
    def update_confidence(self, context: CrawlingContext, field: str, score: float):
        """ì‹ ë¢°ë„ ì ìˆ˜ ì—…ë°ì´íŠ¸"""
        context.confidence_scores[field] = score

class EnhancedHomepageSearchAgent(AIAgent):
    """AI ê°•í™” í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì—ì´ì „íŠ¸"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler):
        super().__init__("EnhancedHomepageSearchAgent", ai_manager, logger, parent_crawler)
        self.crawler_utils = CrawlerUtils()
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰"""
        try:
            org_name = context.organization.get('name', '')
            category = context.organization.get('category', '')
            self.logger.info(f"ğŸ” [{self.name}] AI í™ˆí˜ì´ì§€ ê²€ìƒ‰: {org_name}")
            
            # ê¸°ì¡´ í™ˆí˜ì´ì§€ê°€ ìˆìœ¼ë©´ AIë¡œ ê²€ì¦
            existing_homepage = context.organization.get('homepage', '')
            if existing_homepage:
                verification_result = await self._verify_homepage_with_ai(existing_homepage, org_name, category)
                if verification_result['is_valid']:
                    context.extracted_data['homepage'] = existing_homepage
                    context.extracted_data['homepage_type'] = verification_result['type']
                    self.update_confidence(context, 'homepage', verification_result['confidence'])
                    context.current_stage = CrawlingStage.HOMEPAGE_ANALYSIS
                    return context
            
            # AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰
            search_results = await self._ai_comprehensive_homepage_search(org_name, category)
            if search_results:
                best_result = search_results[0]
                context.extracted_data['homepage'] = best_result['url']
                context.extracted_data['homepage_type'] = best_result['type']
                context.extracted_data['homepage_confidence'] = best_result['confidence']
                self.update_confidence(context, 'homepage', best_result['confidence'])
                self.logger.info(f"âœ… AI í™ˆí˜ì´ì§€ ë°œê²¬: {best_result['url']} ({best_result['type']})")
            
            context.current_stage = CrawlingStage.HOMEPAGE_ANALYSIS
            return context
            
        except Exception as e:
            context.error_log.append(f"EnhancedHomepageSearchAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _ai_comprehensive_homepage_search(self, org_name: str, category: str) -> List[Dict]:
        """AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰"""
        # ê¸°ì¡´ ëª¨ë“ˆì˜ homepage_parser í™œìš© + AI ê°•í™”
        if self.parent_crawler.homepage_parser:
            try:
                # url_extractorì˜ AI ê¸°ëŠ¥ í™œìš©
                ai_search_results = await self.parent_crawler.homepage_parser.ai_search_homepage(org_name, category)
                return ai_search_results
            except Exception as e:
                self.logger.warning(f"AI í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return []
    
    async def _verify_homepage_with_ai(self, url: str, org_name: str, category: str) -> Dict:
        """AIë¡œ í™ˆí˜ì´ì§€ ê´€ë ¨ì„± ê²€ì¦"""
        try:
            prompt = f"""
            ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™ˆí˜ì´ì§€ì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”:

            **ê¸°ê´€ ì •ë³´:**
            - ê¸°ê´€ëª…: {org_name}
            - ì¹´í…Œê³ ë¦¬: {category}
            - í™ˆí˜ì´ì§€ URL: {url}

            **íŒë‹¨ ê¸°ì¤€:**
            1. ë„ë©”ì¸ëª…ì´ ê¸°ê´€ëª…ê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€?
            2. URLì´ í•´ë‹¹ ê¸°ê´€ì˜ ê³µì‹ ë˜ëŠ” ê´€ë ¨ í˜ì´ì§€ì¸ê°€?
            3. ì†Œê·œëª¨ ê¸°ê´€ì˜ ê²½ìš° ë¸”ë¡œê·¸/ì¹´í˜/SNSë„ ê³µì‹ í˜ì´ì§€ë¡œ ì¸ì •
            4. ê¸°ê´€ì˜ ì„±ê²©ê³¼ URL íƒ€ì…ì´ ì í•©í•œê°€?

            **ì‘ë‹µ í˜•ì‹:**
            VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            TYPE: [ê³µì‹ì‚¬ì´íŠ¸/ë„¤ì´ë²„ë¸”ë¡œê·¸/ë„¤ì´ë²„ì¹´í˜/í˜ì´ìŠ¤ë¶/ì¸ìŠ¤íƒ€ê·¸ë¨/ìœ íŠœë¸Œ/ê¸°íƒ€]
            CONFIDENCE: [0.1-1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„ ì ìˆ˜]
            REASON: [íŒë‹¨ ì´ìœ  1-2ë¬¸ì¥]
            """
            
            response = await self.ai_manager.extract_with_gemini(url, prompt)
            return self._parse_verification_response(response)
            
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {'is_valid': False, 'type': 'ì•Œìˆ˜ì—†ìŒ', 'confidence': 0.0}
    
    def _parse_verification_response(self, response: str) -> Dict:
        """AI ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        result = {
            'is_valid': False,
            'type': 'ì•Œìˆ˜ì—†ìŒ',
            'confidence': 0.0,
            'reason': ''
        }
        
        try:
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('VALID:'):
                    result['is_valid'] = 'ì˜ˆ' in line
                elif line.startswith('TYPE:'):
                    result['type'] = line.replace('TYPE:', '').strip()
                elif line.startswith('CONFIDENCE:'):
                    confidence_text = line.replace('CONFIDENCE:', '').strip()
                    try:
                        result['confidence'] = float(confidence_text)
                    except:
                        result['confidence'] = 0.5
                elif line.startswith('REASON:'):
                    result['reason'] = line.replace('REASON:', '').strip()
        
        except Exception as e:
            self.logger.warning(f"ê²€ì¦ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return result

class EnhancedHomepageAnalysisAgent(AIAgent):
    """AI ê°•í™” í™ˆí˜ì´ì§€ ë¶„ì„ ì—ì´ì „íŠ¸"""
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """AI ê¸°ë°˜ í™ˆí˜ì´ì§€ ì¢…í•© ë¶„ì„"""
        try:
            homepage_url = context.extracted_data.get('homepage')
            if not homepage_url:
                context.current_stage = CrawlingStage.CONTACT_PAGE_SEARCH
                return context
            
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸŒ [{self.name}] AI í™ˆí˜ì´ì§€ ë¶„ì„: {homepage_url}")
            
            # ê¸°ì¡´ í™ˆí˜ì´ì§€ íŒŒì„œ + AI ê°•í™” ë¶„ì„
            if self.parent_crawler.homepage_parser:
                page_data = self.parent_crawler.homepage_parser.extract_page_content(homepage_url)
                
                if page_data["status"] == "success" and page_data["accessible"]:
                    # AI ìš”ì•½ ìƒì„± (ê¸°ì¡´ ê¸°ëŠ¥ í™œìš©)
                    ai_summary = self.parent_crawler.homepage_parser.summarize_with_ai(org_name, page_data)
                    
                    # ì—°ë½ì²˜ ì •ë³´ ì €ì¥
                    contact_info = page_data.get("contact_info", {})
                    self._store_enhanced_contact_info(context, contact_info)
                    
                    # AI ì¸ì‚¬ì´íŠ¸ ì €ì¥
                    context.ai_insights['homepage_analysis'] = ai_summary
                    context.ai_insights['homepage_parsing_details'] = page_data["parsing_details"]
                    
                    self.logger.info(f"âœ… AI í™ˆí˜ì´ì§€ ë¶„ì„ ì™„ë£Œ: {org_name}")
            
            context.current_stage = CrawlingStage.CONTACT_PAGE_SEARCH
            return context
            
        except Exception as e:
            context.error_log.append(f"EnhancedHomepageAnalysisAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    def _store_enhanced_contact_info(self, context: CrawlingContext, contact_info: Dict):
        """ê°•í™”ëœ ì—°ë½ì²˜ ì •ë³´ ì €ì¥"""
        if contact_info.get('phone'):
            context.extracted_data['phone'] = contact_info['phone'][0] if isinstance(contact_info['phone'], list) else contact_info['phone']
            self.update_confidence(context, 'phone', 0.9)
        
        if contact_info.get('fax'):
            context.extracted_data['fax'] = contact_info['fax'][0] if isinstance(contact_info['fax'], list) else contact_info['fax']
            self.update_confidence(context, 'fax', 0.9)
        
        if contact_info.get('email'):
            context.extracted_data['email'] = contact_info['email'][0] if isinstance(contact_info['email'], list) else contact_info['email']
            self.update_confidence(context, 'email', 0.9)
        
        if contact_info.get('address'):
            context.extracted_data['address'] = contact_info['address'][0] if isinstance(contact_info['address'], list) else contact_info['address']
            self.update_confidence(context, 'address', 0.8)

class EnhancedContactExtractionAgent(AIAgent):
    """AI ê°•í™” ì—°ë½ì²˜ ì¶”ì¶œ ì—ì´ì „íŠ¸"""
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì „ë¬¸ ëª¨ë“ˆ + AIë¥¼ í™œìš©í•œ ì—°ë½ì²˜ ì¶”ì¶œ"""
        try:
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ“ [{self.name}] AI ì—°ë½ì²˜ ì¶”ì¶œ: {org_name}")
            
            # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (ê¸°ì¡´ ëª¨ë“ˆ ì‚¬ìš©)
            if not context.extracted_data.get('phone'):
                phone_result = await self._extract_phone_basic(org_name, context)
                if phone_result:
                    context.extracted_data.update(phone_result)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ (ê¸°ì¡´ ëª¨ë“ˆ ì‚¬ìš©)
            if not context.extracted_data.get('fax'):
                fax_result = await self._extract_fax_basic(org_name, context)
                if fax_result:
                    context.extracted_data.update(fax_result)
            
            context.current_stage = CrawlingStage.AI_VERIFICATION
            return context
            
        except Exception as e:
            context.error_log.append(f"EnhancedContactExtractionAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _extract_phone_basic(self, org_name: str, context: CrawlingContext) -> Optional[Dict]:
        """ê¸°ë³¸ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ"""
        try:
            # ê¸°ì¡´ phone_extractor ëª¨ë“ˆ ì‚¬ìš©
            if self.parent_crawler and self.parent_crawler.phone_driver:
                found_phones = search_phone_number(self.parent_crawler.phone_driver, org_name)
                
                if found_phones:
                    verified_phone = found_phones[0]  # ì²« ë²ˆì§¸ ì „í™”ë²ˆí˜¸ ì‚¬ìš©
                    self.update_confidence(context, 'phone', 0.8)
                    return {
                        "phone": verified_phone,
                        "phone_extraction_method": "basic_phone_extractor",
                        "phone_extraction_timestamp": datetime.now().isoformat()
                    }
            
            return None
            
        except Exception as e:
            self.logger.error(f"ê¸°ë³¸ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    async def _extract_fax_basic(self, org_name: str, context: CrawlingContext) -> Optional[Dict]:
        """ê¸°ë³¸ íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ"""
        try:
            # ê¸°ì¡´ fax_extractor ëª¨ë“ˆ ì‚¬ìš©
            if self.parent_crawler and self.parent_crawler.fax_extractor:
                found_faxes = self.parent_crawler.fax_extractor.search_fax_number(org_name)
                
                if found_faxes:
                    verified_fax = found_faxes[0]  # ì²« ë²ˆì§¸ íŒ©ìŠ¤ë²ˆí˜¸ ì‚¬ìš©
                    
                    # ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µ í™•ì¸
                    phone = context.extracted_data.get('phone', '')
                    if verified_fax != phone:
                        self.update_confidence(context, 'fax', 0.7)
                        return {
                            "fax": verified_fax,
                            "fax_extraction_method": "basic_fax_extractor",
                            "fax_extraction_timestamp": datetime.now().isoformat()
                        }
                    else:
                        self.logger.info(f"íŒ©ìŠ¤ë²ˆí˜¸ê°€ ì „í™”ë²ˆí˜¸ì™€ ë™ì¼: {verified_fax}")
            
            return None
            
        except Exception as e:
            self.logger.error(f"ê¸°ë³¸ íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

class AIVerificationAgent(AIAgent):
    """AI ì¢…í•© ê²€ì¦ ì—ì´ì „íŠ¸"""
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """AI ê¸°ë°˜ ì¢…í•© ë°ì´í„° ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ¤– [{self.name}] AI ì¢…í•© ê²€ì¦: {org_name}")
            
            # ì¶”ì¶œëœ ë°ì´í„° ì¢…í•© ê²€ì¦
            verification_result = await self._ai_comprehensive_verification(context)
            context.ai_insights['verification'] = verification_result
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ì¡°ì •
            self._adjust_confidence_scores(context, verification_result)
            
            context.current_stage = CrawlingStage.DATA_VALIDATION
            return context
            
        except Exception as e:
            context.error_log.append(f"AIVerificationAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _ai_comprehensive_verification(self, context: CrawlingContext) -> Dict[str, Any]:
        """AI ì¢…í•© ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            extracted = context.extracted_data
            
            prompt = f"""
            ê¸°ê´€ëª…: {org_name}
            ì¶”ì¶œëœ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€ì¦í•´ì£¼ì„¸ìš”:
            
            - í™ˆí˜ì´ì§€: {extracted.get('homepage', 'ì—†ìŒ')}
            - ì „í™”ë²ˆí˜¸: {extracted.get('phone', 'ì—†ìŒ')}
            - íŒ©ìŠ¤ë²ˆí˜¸: {extracted.get('fax', 'ì—†ìŒ')}
            - ì´ë©”ì¼: {extracted.get('email', 'ì—†ìŒ')}
            - ì£¼ì†Œ: {extracted.get('address', 'ì—†ìŒ')}
            
            ê° ì •ë³´ê°€ í•´ë‹¹ ê¸°ê´€ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨í•˜ê³ ,
            ì „ì²´ì ì¸ ë°ì´í„° í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.
            
            ì‘ë‹µ í˜•ì‹:
            HOMEPAGE_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            PHONE_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            FAX_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            EMAIL_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            ADDRESS_VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            OVERALL_QUALITY: [ìµœê³ /ë†’ìŒ/ë³´í†µ/ë‚®ìŒ]
            CONFIDENCE_SCORE: [0.0-1.0]
            ISSUES: [ë°œê²¬ëœ ë¬¸ì œì ë“¤]
            RECOMMENDATIONS: [ê°œì„  ì œì•ˆì‚¬í•­]
            """
            
            response = await self.ai_manager.extract_with_gemini("", prompt)
            return self._parse_verification_response(response)
            
        except Exception as e:
            self.logger.error(f"AI ì¢…í•© ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {'overall_quality': 'ë‚®ìŒ', 'confidence_score': 0.0, 'issues': [str(e)]}
    
    def _parse_verification_response(self, response: str) -> Dict[str, Any]:
        """AI ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        result = {
            'homepage_valid': False,
            'phone_valid': False,
            'fax_valid': False,
            'email_valid': False,
            'address_valid': False,
            'overall_quality': 'ë³´í†µ',
            'confidence_score': 0.5,
            'issues': [],
            'recommendations': []
        }
        
        try:
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('HOMEPAGE_VALID:'):
                    result['homepage_valid'] = 'ì˜ˆ' in line
                elif line.startswith('PHONE_VALID:'):
                    result['phone_valid'] = 'ì˜ˆ' in line
                elif line.startswith('FAX_VALID:'):
                    result['fax_valid'] = 'ì˜ˆ' in line
                elif line.startswith('EMAIL_VALID:'):
                    result['email_valid'] = 'ì˜ˆ' in line
                elif line.startswith('ADDRESS_VALID:'):
                    result['address_valid'] = 'ì˜ˆ' in line
                elif line.startswith('OVERALL_QUALITY:'):
                    result['overall_quality'] = line.replace('OVERALL_QUALITY:', '').strip()
                elif line.startswith('CONFIDENCE_SCORE:'):
                    try:
                        score_text = line.replace('CONFIDENCE_SCORE:', '').strip()
                        result['confidence_score'] = float(score_text)
                    except:
                        result['confidence_score'] = 0.5
                elif line.startswith('ISSUES:'):
                    issues_text = line.replace('ISSUES:', '').strip()
                    if issues_text and issues_text != 'ì—†ìŒ':
                        result['issues'] = [issues_text]
                elif line.startswith('RECOMMENDATIONS:'):
                    rec_text = line.replace('RECOMMENDATIONS:', '').strip()
                    if rec_text and rec_text != 'ì—†ìŒ':
                        result['recommendations'] = [rec_text]
        
        except Exception as e:
            self.logger.warning(f"ê²€ì¦ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return result
    
    def _adjust_confidence_scores(self, context: CrawlingContext, verification: Dict[str, Any]):
        """ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ì‹ ë¢°ë„ ì ìˆ˜ ì¡°ì •"""
        adjustments = {
            'homepage': verification.get('homepage_valid', False),
            'phone': verification.get('phone_valid', False),
            'fax': verification.get('fax_valid', False),
            'email': verification.get('email_valid', False),
            'address': verification.get('address_valid', False)
        }
        
        for field, is_valid in adjustments.items():
            if field in context.confidence_scores:
                if is_valid:
                    context.confidence_scores[field] = min(1.0, context.confidence_scores[field] + 0.1)
                else:
                    context.confidence_scores[field] = max(0.0, context.confidence_scores[field] - 0.2)

# ==================== AI ê°•í™” ModularUnifiedCrawler ====================

class AIEnhancedModularUnifiedCrawler:
    """AI ê°•í™” ëª¨ë“ˆëŸ¬ í†µí•© í¬ë¡¤ëŸ¬"""
    
    def __init__(self, config_override=None, api_key=None, progress_callback=None):
        """ì´ˆê¸°í™”"""
        self.config = config_override or CRAWLING_CONFIG
        self.logger = LoggerUtils.setup_crawler_logger("ai_enhanced_modular_crawler")
        self.progress_callback = progress_callback
        
        # AI ë§¤ë‹ˆì € ì´ˆê¸°í™”
        try:
            self.ai_manager = AIModelManager()
        except Exception as e:
            self.logger.warning(f"AI ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ai_manager = None
        
        # ì „ë¬¸ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ë“¤ (ê¸°ì¡´ ìœ ì§€)
        self.fax_extractor = None
        self.phone_driver = None
        self.homepage_parser = None
        self.contact_validator = None
        self.ai_validator = None
        self.database = None
        
        # AI ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™” (ìˆ˜ì •: parent_crawler ì „ë‹¬)
        self.ai_agents = []
        if self.ai_manager:
            try:
                self.ai_agents = [
                    EnhancedHomepageSearchAgent(self.ai_manager, self.logger, self),
                    EnhancedHomepageAnalysisAgent(self.ai_manager, self.logger, self),
                    EnhancedContactExtractionAgent(self.ai_manager, self.logger, self),
                    AIVerificationAgent(self.ai_manager, self.logger, self)
                ]
                self.logger.info("âœ… AI ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                self.logger.error(f"âŒ AI ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.ai_agents = []
        else:
            self.logger.warning("âš ï¸ AI ë§¤ë‹ˆì € ì—†ìŒ - ê¸°ë³¸ ëª¨ë“ˆë§Œ ì‚¬ìš©")
        
        # í†µê³„ ì •ë³´
        self.stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "phone_extracted": 0,
            "fax_extracted": 0,
            "homepage_parsed": 0,
            "contacts_validated": 0,
            "saved_to_db": 0,
            "ai_enhanced": 0,
            "start_time": None,
            "end_time": None,
            "agent_stats": {agent.name: {"executed": 0, "success": 0} for agent in self.ai_agents}
        }
        
        self.logger.info("ğŸš€ AI ê°•í™” ëª¨ë“ˆëŸ¬ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def initialize_modules(self):
        """ì „ë¬¸ ëª¨ë“ˆë“¤ ì´ˆê¸°í™” (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        try:
            self.logger.info("ğŸ”§ ì „ë¬¸ ëª¨ë“ˆë“¤ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. íŒ©ìŠ¤ ì¶”ì¶œê¸° ì´ˆê¸°í™”
            if FAX_EXTRACTOR_AVAILABLE:
                try:
                    self.fax_extractor = FaxExtractor()
                    self.logger.info("âœ… íŒ©ìŠ¤ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ íŒ©ìŠ¤ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.fax_extractor = None
            
            # 2. ì „í™”ë²ˆí˜¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” (Selenium ë“œë¼ì´ë²„)
            if PHONE_EXTRACTOR_AVAILABLE:
                try:
                    self.phone_driver = setup_driver()
                    self.logger.info("âœ… ì „í™”ë²ˆí˜¸ ì¶”ì¶œê¸° ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ ì „í™”ë²ˆí˜¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.phone_driver = None
            
            # 3. í™ˆí˜ì´ì§€ íŒŒì„œ ì´ˆê¸°í™”
            if URL_EXTRACTOR_AVAILABLE:
                try:
                    self.homepage_parser = HomepageParser(headless=True)
                    self.logger.info("âœ… í™ˆí˜ì´ì§€ íŒŒì„œ ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ í™ˆí˜ì´ì§€ íŒŒì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.homepage_parser = None
            
            # 4. ê²€ì¦ê¸° ì´ˆê¸°í™”
            if VALIDATOR_AVAILABLE:
                try:
                    self.contact_validator = ContactValidator()
                    self.ai_validator = AIValidator()
                    self.logger.info("âœ… ì—°ë½ì²˜ ê²€ì¦ê¸° ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ ê²€ì¦ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.contact_validator = None
                    self.ai_validator = None
            
            # 5. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            if DATABASE_AVAILABLE:
                try:
                    self.database = get_database()
                    self.logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
                    self.database = None
            
            self.logger.info("ğŸ¯ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë“ˆ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    async def process_organizations(self, organizations: List[Dict], options: Dict = None) -> List[Dict]:
        """AI ê°•í™” ì¡°ì§ ì²˜ë¦¬"""
        if not organizations:
            self.logger.warning("ì²˜ë¦¬í•  ì¡°ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        options = options or {}
        self.stats["start_time"] = datetime.now()
        self.stats["total_processed"] = len(organizations)
        
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.initialize_modules()
        
        self.logger.info(f"ğŸ“Š ì´ {len(organizations)}ê°œ ì¡°ì§ AI ê°•í™” ì²˜ë¦¬ ì‹œì‘")
        
        results = []
        
        try:
            for i, org in enumerate(organizations, 1):
                try:
                    # AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¡œ ì²˜ë¦¬
                    processed_org = await self.process_single_organization_with_ai(org, i)
                    results.append(processed_org)
                    
                    self.stats["successful"] += 1
                    if processed_org.get('ai_enhanced'):
                        self.stats["ai_enhanced"] += 1
                    
                    # ë”œë ˆì´
                    await asyncio.sleep(self.config.get("default_delay", 2))
                    
                except Exception as e:
                    self.logger.error(f"âŒ ì¡°ì§ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {org.get('name', 'Unknown')} - {e}")
                    self.stats["failed"] += 1
                    results.append(org)
        
        finally:
            # ëª¨ë“ˆ ì •ë¦¬
            self.cleanup_modules()
        
        self.stats["end_time"] = datetime.now()
        self.print_ai_enhanced_statistics()
        
        return results
    
    async def process_single_organization_with_ai(self, org: Dict, index: int) -> Dict:
        """AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¡œ ë‹¨ì¼ ì¡°ì§ ì²˜ë¦¬"""
        org_name = org.get('name', 'Unknown')
        self.logger.info(f"ğŸ¤– AI ì›Œí¬í”Œë¡œìš° ì‹œì‘ [{index}]: {org_name}")
        
        # í¬ë¡¤ë§ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        context = CrawlingContext(
            organization=org,
            current_stage=CrawlingStage.INITIALIZATION,
            extracted_data={},
            ai_insights={},
            error_log=[],
            processing_time=0,
            confidence_scores={}
        )
        
        start_time = time.time()
        
        try:
            # AI ì—ì´ì „íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            if self.ai_agents:
                for agent in self.ai_agents:
                    if await agent.should_execute(context):
                        self.logger.info(f"ğŸ”„ ì—ì´ì „íŠ¸ ì‹¤í–‰: {agent.name}")
                        context = await agent.execute(context)
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.stats["agent_stats"][agent.name]["executed"] += 1
                        if not context.error_log:
                            self.stats["agent_stats"][agent.name]["success"] += 1
            else:
                # AI ì—ì´ì „íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë“ˆë§Œ ì‚¬ìš©
                self.logger.info("âš ï¸ AI ì—ì´ì „íŠ¸ ì—†ìŒ - ê¸°ë³¸ ëª¨ë“ˆ ì²˜ë¦¬")
                await self._fallback_to_traditional_processing(context)
            
            processing_time = time.time() - start_time
            context.processing_time = processing_time
            
            # ê²°ê³¼ ì¡°í•©
            result = self._combine_ai_results(org, context)
            
            # ê¸°ì¡´ ëª¨ë“ˆ ê¸°ëŠ¥ë„ ì¶”ê°€ (ë³´ì™„ì ìœ¼ë¡œ)
            await self._supplement_with_traditional_modules(result, context)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            if self.database:
                db_result = await self.save_to_database(result)
                if db_result:
                    result.update(db_result)
                    self.stats["saved_to_db"] += 1
            
            self.logger.info(f"ğŸ‰ AI ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: {org_name}")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {org_name} - {e}")
            context.error_log.append(f"ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì²˜ë¦¬ë¡œ ëŒ€ì²´
            return await self._fallback_to_traditional_processing_simple(org)
    
    def _combine_ai_results(self, original_org: Dict, context: CrawlingContext) -> Dict:
        """AI ê²°ê³¼ ì¡°í•©"""
        result = original_org.copy()
        
        # ì¶”ì¶œëœ ë°ì´í„° ì¶”ê°€
        result.update(context.extracted_data)
        
        # AI ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
        result['ai_insights'] = context.ai_insights
        result['confidence_scores'] = context.confidence_scores
        
        # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result['processing_metadata'] = {
            'processing_time': context.processing_time,
            'final_stage': context.current_stage.value,
            'error_count': len(context.error_log),
            'errors': context.error_log,
            'extraction_method': 'ai_enhanced_modular',
            'ai_enhanced': True,
            'timestamp': datetime.now().isoformat()
        }
        
        return result
    
    async def _supplement_with_traditional_modules(self, result: Dict, context: CrawlingContext):
        """ê¸°ì¡´ ëª¨ë“ˆë¡œ ë³´ì™„ ì²˜ë¦¬"""
        try:
            org_name = result.get('name', 'Unknown')
            
            # ì „í™”ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ëª¨ë“ˆë¡œ ì¶”ê°€ ì‹œë„
            if not result.get('phone') and self.phone_driver:
                try:
                    found_phones = search_phone_number(self.phone_driver, org_name)
                    if found_phones:
                        result['phone'] = found_phones[0]
                        result['phone_source'] = 'traditional_module_supplement'
                        self.stats["phone_extracted"] += 1
                except Exception as e:
                    self.logger.warning(f"ì „í™”ë²ˆí˜¸ ë³´ì™„ ì‹¤íŒ¨: {e}")
            
            # íŒ©ìŠ¤ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ëª¨ë“ˆë¡œ ì¶”ê°€ ì‹œë„
            if not result.get('fax') and self.fax_extractor:
                try:
                    found_faxes = self.fax_extractor.search_fax_number(org_name)
                    if found_faxes:
                        result['fax'] = found_faxes[0]
                        result['fax_source'] = 'traditional_module_supplement'
                        self.stats["fax_extracted"] += 1
                except Exception as e:
                    self.logger.warning(f"íŒ©ìŠ¤ë²ˆí˜¸ ë³´ì™„ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.error(f"ê¸°ì¡´ ëª¨ë“ˆ ë³´ì™„ ì˜¤ë¥˜: {e}")
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (í˜¸í™˜ì„±)
    def cleanup_modules(self):
        """ëª¨ë“ˆë“¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ ëª¨ë“ˆ ì •ë¦¬ ì‹œì‘...")
            
            if self.fax_extractor:
                try:
                    self.fax_extractor.close()
                except:
                    pass
            
            if self.phone_driver:
                try:
                    self.phone_driver.quit()
                except:
                    pass
            
            if self.homepage_parser:
                try:
                    self.homepage_parser.close_driver()
                except:
                    pass
            
            self.logger.info("ğŸ¯ ëª¨ë“  ëª¨ë“ˆ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë“ˆ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def save_to_database(self, org_data: Dict) -> Optional[Dict]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        if not self.database:
            return None
        
        try:
            org_name = org_data.get('name', 'Unknown')
            self.logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥: {org_name}")
            
            # AI ê°•í™” ë©”íƒ€ë°ì´í„° í¬í•¨
            db_org_data = {
                "name": org_data.get('name', ''),
                "type": org_data.get('type', 'UNKNOWN'),
                "category": org_data.get('category', 'ê¸°íƒ€'),
                "homepage": org_data.get('homepage', ''),
                "phone": org_data.get('phone', ''),
                "fax": org_data.get('fax', ''),
                "email": org_data.get('email', ''),
                "address": org_data.get('address', ''),
                "contact_status": "AI_ENHANCED",
                "priority": "HIGH",
                "created_by": "ai_enhanced_crawler",
                "updated_by": "ai_enhanced_crawler"
            }
            
            # AI ê°•í™” ë©”íƒ€ë°ì´í„°
            ai_metadata = {
                "ai_insights": org_data.get('ai_insights', {}),
                "confidence_scores": org_data.get('confidence_scores', {}),
                "processing_metadata": org_data.get('processing_metadata', {}),
                "extraction_method": "ai_enhanced_modular"
            }
            
            db_org_data["crawling_data"] = json.dumps(ai_metadata, ensure_ascii=False)
            
            org_id = self.database.create_organization(db_org_data)
            
            return {
                "db_saved": True,
                "db_organization_id": org_id,
                "db_save_timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
    
    def print_ai_enhanced_statistics(self):
        """AI ê°•í™” í†µê³„ ì¶œë ¥"""
        duration = self.stats["end_time"] - self.stats["start_time"]
        
        print("\n" + "="*80)
        print("ğŸ¤– AI ê°•í™” ëª¨ë“ˆëŸ¬ í¬ë¡¤ë§ ì™„ë£Œ í†µê³„")
        print("="*80)
        print(f"ğŸ“‹ ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"âœ… ì„±ê³µ: {self.stats['successful']}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
        print(f"ğŸ¤– AI ê°•í™”: {self.stats['ai_enhanced']}ê°œ")
        print(f"ğŸ“ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ: {self.stats['phone_extracted']}ê°œ")
        print(f"ğŸ“  íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ: {self.stats['fax_extracted']}ê°œ")
        print(f"ğŸ’¾ DB ì €ì¥: {self.stats['saved_to_db']}ê°œ")
        
        if self.stats['total_processed'] > 0:
            success_rate = (self.stats['successful'] / self.stats['total_processed']) * 100
            ai_enhancement_rate = (self.stats['ai_enhanced'] / self.stats['total_processed']) * 100
            print(f"ğŸ“ˆ ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}%")
            print(f"ğŸ¤– AI ê°•í™”ìœ¨: {ai_enhancement_rate:.1f}%")
        
        print(f"â±ï¸ ì†Œìš”ì‹œê°„: {duration}")
        
        # AI ì—ì´ì „íŠ¸ë³„ í†µê³„
        print(f"\nğŸ¤– AI ì—ì´ì „íŠ¸ë³„ ì„±ê³µë¥ :")
        for agent_name, stats in self.stats["agent_stats"].items():
            executed = stats["executed"]
            success = stats["success"]
            success_rate = (success / executed * 100) if executed > 0 else 0
            print(f"  - {agent_name}: {executed}íšŒ ì‹¤í–‰, {success}íšŒ ì„±ê³µ ({success_rate:.1f}%)")
        
        print("="*80)
    
    # ==================== ContactEnrichmentService í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ====================
    
    async def search_homepage(self, org_name: str) -> Dict[str, Any]:
        """í™ˆí˜ì´ì§€ ê²€ìƒ‰ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            self.logger.info(f"ğŸ” AI í™ˆí˜ì´ì§€ ê²€ìƒ‰: {org_name}")
            
            # AI ì—ì´ì „íŠ¸ë¥¼ í†µí•œ í™ˆí˜ì´ì§€ ê²€ìƒ‰
            context = CrawlingContext(
                organization={'name': org_name},
                current_stage=CrawlingStage.HOMEPAGE_SEARCH,
                extracted_data={},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì‹¤í–‰
            homepage_agent = EnhancedHomepageSearchAgent(self.ai_manager, self.logger, self)
            context = await homepage_agent.execute(context)
            
            if context.extracted_data.get('homepage'):
                return {
                    "homepage": context.extracted_data['homepage'],
                    "homepage_type": context.extracted_data.get('homepage_type', 'ì•Œìˆ˜ì—†ìŒ'),
                    "confidence": context.confidence_scores.get('homepage', 0.5),
                    "status": "success",
                    "ai_enhanced": True
                }
            else:
                return {
                    "homepage": "",
                    "status": "not_found",
                    "message": "AI ê¸°ë°˜ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                }
            
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {org_name} - {e}")
            return {"status": "error", "error": str(e)}
    
    async def extract_details_from_homepage(self, homepage_url: str) -> Dict[str, Any]:
        """í™ˆí˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            self.logger.info(f"ğŸŒ AI í™ˆí˜ì´ì§€ ë¶„ì„: {homepage_url}")
            
            # AI ì—ì´ì „íŠ¸ë¥¼ í†µí•œ í™ˆí˜ì´ì§€ ë¶„ì„
            context = CrawlingContext(
                organization={'name': 'Unknown'},
                current_stage=CrawlingStage.HOMEPAGE_ANALYSIS,
                extracted_data={'homepage': homepage_url},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # í™ˆí˜ì´ì§€ ë¶„ì„ ì—ì´ì „íŠ¸ ì‹¤í–‰
            analysis_agent = EnhancedHomepageAnalysisAgent(self.ai_manager, self.logger, self)
            context = await analysis_agent.execute(context)
            
            return {
                "phone": context.extracted_data.get("phone", ""),
                "fax": context.extracted_data.get("fax", ""),
                "email": context.extracted_data.get("email", ""),
                "address": context.extracted_data.get("address", ""),
                "ai_insights": context.ai_insights,
                "confidence_scores": context.confidence_scores,
                "status": "success",
                "ai_enhanced": True
            }
                
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ë¶„ì„ ì˜¤ë¥˜: {homepage_url} - {e}")
            return {"status": "error", "error": str(e)}
    
    async def search_missing_info(self, org_name: str, missing_fields: List[str]) -> Dict[str, str]:
        """ëˆ„ë½ëœ ì •ë³´ ê²€ìƒ‰ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            self.logger.info(f"ğŸ” AI ëˆ„ë½ ì •ë³´ ê²€ìƒ‰: {org_name} - {missing_fields}")
            
            results = {}
            
            # AI ì—ì´ì „íŠ¸ë¥¼ í†µí•œ ì—°ë½ì²˜ ì¶”ì¶œ
            context = CrawlingContext(
                organization={'name': org_name},
                current_stage=CrawlingStage.CONTACT_EXTRACTION,
                extracted_data={},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # ì—°ë½ì²˜ ì¶”ì¶œ ì—ì´ì „íŠ¸ ì‹¤í–‰
            contact_agent = EnhancedContactExtractionAgent(self.ai_manager, self.logger, self)
            context = await contact_agent.execute(context)
            
            # ìš”ì²­ëœ í•„ë“œë§Œ ë°˜í™˜
            for field in missing_fields:
                if field in context.extracted_data:
                    results[field] = context.extracted_data[field]
                    self.logger.info(f"  ğŸ¯ AIë¡œ {field} ë°œê²¬: {results[field]}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"AI ëˆ„ë½ ì •ë³´ ê²€ìƒ‰ ì˜¤ë¥˜: {org_name} - {e}")
            return {}
    
    def validate_and_clean_data(self, org_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            cleaned_data = org_data.copy()
            
            # ê¸°ì¡´ validator.py ì‚¬ìš© (ìœ ì§€)
            if self.contact_validator:
                # ì „í™”ë²ˆí˜¸ ê²€ì¦
                if cleaned_data.get('phone'):
                    is_valid, validated_phone = self.contact_validator.validate_phone_number(cleaned_data['phone'])
                    if is_valid:
                        cleaned_data['phone'] = validated_phone
                        cleaned_data['phone_validation'] = 'valid'
                    else:
                        self.logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì „í™”ë²ˆí˜¸: {cleaned_data['phone']}")
                        cleaned_data['phone_validation'] = 'invalid'
                
                # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦
                if cleaned_data.get('fax'):
                    is_valid, validated_fax = self.contact_validator.validate_fax_number(cleaned_data['fax'])
                    if is_valid:
                        cleaned_data['fax'] = validated_fax
                        cleaned_data['fax_validation'] = 'valid'
                    else:
                        self.logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ íŒ©ìŠ¤ë²ˆí˜¸: {cleaned_data['fax']}")
                        cleaned_data['fax_validation'] = 'invalid'
            
            # AI ê²€ì¦ ì¶”ê°€
            if self.ai_manager:
                try:
                    # AI ê²€ì¦ ì—ì´ì „íŠ¸ ì‹¤í–‰
                    context = CrawlingContext(
                        organization={'name': cleaned_data.get('name', 'Unknown')},
                        current_stage=CrawlingStage.AI_VERIFICATION,
                        extracted_data=cleaned_data,
                        ai_insights={},
                        error_log=[],
                        processing_time=0,
                        confidence_scores={}
                    )
                    
                    # AI ê²€ì¦ ì—ì´ì „íŠ¸ ì‹¤í–‰ (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ)
                    verification_agent = AIVerificationAgent(self.ai_manager, self.logger, self)
                    # ë¹„ë™ê¸° í˜¸ì¶œì„ ë™ê¸°ë¡œ ë³€í™˜
                    import asyncio
                    if asyncio.get_event_loop().is_running():
                        # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
                        task = asyncio.create_task(verification_agent.execute(context))
                        # ê°„ë‹¨í•œ AI ê²€ì¦ë§Œ ìˆ˜í–‰
                        cleaned_data['ai_validation_attempted'] = True
                    else:
                        context = asyncio.run(verification_agent.execute(context))
                        cleaned_data['ai_insights'] = context.ai_insights
                        cleaned_data['confidence_scores'] = context.confidence_scores
                
                except Exception as e:
                    self.logger.warning(f"AI ê²€ì¦ ì‹¤íŒ¨: {e}")
                    cleaned_data['ai_validation_error'] = str(e)
            
            return cleaned_data
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜: {e}")
            return org_data
    
    # ==================== app.py í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ====================
    
    async def process_json_file_async(self, json_file_path: str, test_mode: bool = False, test_count: int = 10, progress_callback=None) -> List[Dict]:
        """ğŸ”§ app.py í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ (AI ê°•í™”)"""
        try:
            # JSON íŒŒì¼ ë¡œë“œ
            data = FileUtils.load_json(json_file_path)
            
            # ë°ì´í„° ì „ì²˜ë¦¬ (app.pyì™€ ë™ì¼í•œ ë°©ì‹)
            organizations = []
            if isinstance(data, dict):
                for category, orgs in data.items():
                    if isinstance(orgs, list):
                        for org in orgs:
                            if isinstance(org, dict):
                                org["category"] = category
                                organizations.append(org)
            elif isinstance(data, list):
                organizations = [org for org in data if isinstance(org, dict)]
            
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
            if test_mode and test_count:
                organizations = organizations[:test_count]
            
            # progress_callback ì €ì¥
            self.progress_callback = progress_callback
            
            # AI ê°•í™” ì²˜ë¦¬ ì‹¤í–‰
            results = await self.process_organizations(organizations)
            
            return results
            
        except Exception as e:
            self.logger.error(f"JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

    async def _fallback_to_traditional_processing(self, context: CrawlingContext):
        """AI ì—ì´ì „íŠ¸ ì—†ì„ ë•Œ ê¸°ë³¸ ëª¨ë“ˆ ì²˜ë¦¬"""
        try:
            org_name = context.organization.get('name', 'Unknown')
            self.logger.info(f"ğŸ”§ ê¸°ë³¸ ëª¨ë“ˆ ì²˜ë¦¬: {org_name}")
            
            # ê¸°ë³¸ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (url_extractor ì‚¬ìš©)
            if self.homepage_parser:
                try:
                    # ê°„ë‹¨í•œ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ë¡œì§
                    pass
                except Exception as e:
                    self.logger.warning(f"í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ ì „í™”ë²ˆí˜¸ ê²€ìƒ‰ (phone_extractor ì‚¬ìš©)
            if self.phone_driver:
                try:
                    found_phones = search_phone_number(self.phone_driver, org_name)
                    if found_phones:
                        context.extracted_data['phone'] = found_phones[0]
                        self.update_confidence(context, 'phone', 0.7)
                except Exception as e:
                    self.logger.warning(f"ì „í™”ë²ˆí˜¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ (fax_extractor ì‚¬ìš©)
            if self.fax_extractor:
                try:
                    found_faxes = self.fax_extractor.search_fax_number(org_name)
                    if found_faxes:
                        context.extracted_data['fax'] = found_faxes[0]
                        self.update_confidence(context, 'fax', 0.7)
                except Exception as e:
                    self.logger.warning(f"íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.error(f"ê¸°ë³¸ ëª¨ë“ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _fallback_to_traditional_processing_simple(self, org: Dict) -> Dict:
        """ê°„ë‹¨í•œ ê¸°ë³¸ ì²˜ë¦¬ (ì‹¤íŒ¨ ì‹œ ëŒ€ì²´)"""
        result = org.copy()
        result['processing_metadata'] = {
            'extraction_method': 'fallback_traditional',
            'ai_enhanced': False,
            'timestamp': datetime.now().isoformat(),
            'status': 'fallback_processed'
        }
        return result

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ (ModularUnifiedCrawlerë¥¼ AI ê°•í™” ë²„ì „ìœ¼ë¡œ êµì²´)
ModularUnifiedCrawler = AIEnhancedModularUnifiedCrawler

# ==================== í¸ì˜ í•¨ìˆ˜ë“¤ ====================

async def crawl_ai_enhanced_from_file(input_file: str, options: Dict = None) -> List[Dict]:
    """íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ AI ê°•í™” í¬ë¡¤ë§"""
    try:
        # íŒŒì¼ ë¡œë“œ
        data = FileUtils.load_json(input_file)
        if not data:
            raise ValueError(f"íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        
        # AI ê°•í™” í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
        crawler = AIEnhancedModularUnifiedCrawler()
        results = await crawler.process_organizations(data, options)
        
        return results
        
    except Exception as e:
        logging.error(f"AI ê°•í™” íŒŒì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

async def crawl_ai_enhanced_latest_file(options: Dict = None) -> List[Dict]:
    """ìµœì‹  ì…ë ¥ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ AI ê°•í™” í¬ë¡¤ë§"""
    try:
        latest_file = get_latest_input_file()
        if not latest_file:
            raise ValueError("ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ“‚ ìµœì‹  íŒŒì¼ ì‚¬ìš©: {latest_file}")
        return await crawl_ai_enhanced_from_file(str(latest_file), options)
        
    except Exception as e:
        logging.error(f"AI ê°•í™” ìµœì‹  íŒŒì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (AI ê°•í™”)"""
    print("ğŸ¤– AI ê°•í™” ëª¨ë“ˆëŸ¬ í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*80)
    print("ğŸš€ AI ê¸°ëŠ¥ ìµœëŒ€ í™œìš© ëª¨ë“œ")
    print("ğŸ“¦ ì „ë¬¸ ëª¨ë“ˆ + AI ì—ì´ì „íŠ¸ í†µí•©:")
    print(f"  - fax_extractor.py: {'âœ…' if FAX_EXTRACTOR_AVAILABLE else 'âŒ'}")
    print(f"  - phone_extractor.py: {'âœ…' if PHONE_EXTRACTOR_AVAILABLE else 'âŒ'}")
    print(f"  - url_extractor.py: {'âœ…' if URL_EXTRACTOR_AVAILABLE else 'âŒ'}")
    print(f"  - validator.py: {'âœ…' if VALIDATOR_AVAILABLE else 'âŒ'}")
    print(f"  - database.py: {'âœ…' if DATABASE_AVAILABLE else 'âŒ'}")
    print("ğŸ¤– AI ì—ì´ì „íŠ¸:")
    print("  - EnhancedHomepageSearchAgent: AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰")
    print("  - EnhancedHomepageAnalysisAgent: AI ê¸°ë°˜ í™ˆí˜ì´ì§€ ë¶„ì„")
    print("  - EnhancedContactExtractionAgent: AI ê°•í™” ì—°ë½ì²˜ ì¶”ì¶œ")
    print("  - AIVerificationAgent: AI ì¢…í•© ê²€ì¦")
    print("="*80)
    
    try:
        # í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
        initialize_project()
        
        # AI ê°•í™” í¬ë¡¤ë§ ì‹¤í–‰
        results = await crawl_ai_enhanced_latest_file()
        
        if results:
            ai_enhanced_count = sum(1 for r in results if r.get('ai_enhanced'))
            print(f"\nâœ… AI ê°•í™” í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ì¡°ì§ ì²˜ë¦¬")
            print(f"ğŸ¤– AI ê°•í™”ëœ ì¡°ì§: {ai_enhanced_count}ê°œ")
            print(f"ğŸ“ˆ AI ê°•í™”ìœ¨: {(ai_enhanced_count/len(results)*100):.1f}%")
        else:
            print("\nâŒ AI ê°•í™” í¬ë¡¤ë§ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main())