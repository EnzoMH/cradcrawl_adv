#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© í¬ë¡¤ë§ ì—”ì§„ v2.0
advcrawler.pyì˜ ê³ ê¸‰ í¬ë¡¤ë§ ê¸°ëŠ¥ì„ ì™„ì „íˆ ì´ì‹í•˜ì—¬ í†µí•©
âœ… app.py ì™„ì „ í˜¸í™˜ (API í‚¤, ì½œë°± í•¨ìˆ˜ ì§€ì›)
âœ… ê°•í™”ëœ ì›¹ í¬ë¡¤ë§ (ì°¨ë‹¨ ìš°íšŒ, ì¬ì‹œë„ ë¡œì§)
âœ… AI ê¸°ë°˜ ì—°ë½ì²˜ ì¶”ì¶œ (Gemini API)
âœ… êµ¬ê¸€ ê²€ìƒ‰ ê¸°ë°˜ ì—°ë½ì²˜ ê²€ìƒ‰
âœ… ë‹¤ë‹¨ê³„ ê²€ì¦ (Parser + Validator + AI)
âœ… ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì½œë°±
âœ… ìƒì„¸í•œ í†µê³„ ë° ëª¨ë‹ˆí„°ë§
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'test'))

import asyncio
import json
import time
import logging
import requests
import re
import random
import os
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import urllib3

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í”„ë¡œì íŠ¸ ì„¤ì • import
from utils.settings import *
from utils.logger_utils import LoggerUtils
from utils.file_utils import FileUtils
from utils.phone_utils import PhoneUtils
from utils.ai_helpers import AIModelManager
from utils.parser import WebPageParser
from utils.validator import ContactValidator

class UnifiedCrawler:
    """í†µí•© í¬ë¡¤ë§ ì—”ì§„ - app.py í˜¸í™˜ì„± ìœ ì§€"""
    
    def __init__(self, config_override=None, api_key=None, progress_callback=None):
        """ì´ˆê¸°í™” - app.py í˜¸í™˜ì„±ì„ ìœ„í•œ ë§¤ê°œë³€ìˆ˜ ì¶”ê°€"""
        self.config = config_override or CRAWLING_CONFIG
        self.logger = LoggerUtils.setup_crawler_logger()
        self.phone_utils = PhoneUtils()
        self.progress_callback = progress_callback
        
        # AI ë§¤ë‹ˆì € ì´ˆê¸°í™” (API í‚¤ ìš°ì„  ì²˜ë¦¬)
        if api_key:
            os.environ['GEMINI_API_KEY'] = api_key
        
        try:
            self.ai_manager = AIModelManager()
            self.use_ai = True
            self.logger.info("ğŸ¤– AI ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            self.logger.warning(f"AI ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ai_manager = None
            self.use_ai = False
        
        # ì›¹ íŒŒì„œ ë° ê²€ì¦ê¸° ì´ˆê¸°í™”
        try:
            self.web_parser = WebPageParser()
            self.validator = ContactValidator()
            self.logger.info("ğŸ” ì›¹ íŒŒì„œ ë° ê²€ì¦ê¸° ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            self.logger.warning(f"ì›¹ íŒŒì„œ/ê²€ì¦ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.web_parser = None
            self.validator = None
        
        # êµ¬ê¸€ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
        self.google_searcher = None
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.timeout = 15
        self.max_retries = 3
        self.delay_range = (2, 5)
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # í†µê³„ ì •ë³´ (advcrawler.py ìŠ¤íƒ€ì¼ë¡œ í™•ì¥)
        self.stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "successful_crawls": 0,
            "failed_crawls": 0,
            "ai_enhanced": 0,
            "contacts_found": 0,
            "api_calls_made": 0,
            "ai_failures": 0,
            "google_searches_performed": 0,
            "google_contacts_found": 0,
            "start_time": None,
            "end_time": None
        }
        
        self.logger.info("ğŸš€ í†µí•© í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ - app.py í˜¸í™˜")
    
    async def process_organizations(self, organizations: List[Dict], options: Dict = None) -> List[Dict]:
        """ì¡°ì§/ê¸°ê´€ ëª©ë¡ ì²˜ë¦¬"""
        if not organizations:
            self.logger.warning("ì²˜ë¦¬í•  ì¡°ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        options = options or {}
        self.stats["start_time"] = datetime.now()
        self.stats["total_processed"] = len(organizations)
        
        self.logger.info(f"ğŸ“Š ì´ {len(organizations)}ê°œ ì¡°ì§ ì²˜ë¦¬ ì‹œì‘")
        
        results = []
        save_interval = self.config.get("save_interval", 50)
        
        for i, org in enumerate(organizations, 1):
            try:
                # ë‹¨ì¼ ì¡°ì§ ì²˜ë¦¬
                processed_org = await self.process_single_organization(org, i)
                results.append(processed_org)
                
                self.stats["successful"] += 1
                
                # ì¤‘ê°„ ì €ì¥
                if i % save_interval == 0:
                    await self.save_intermediate_results(results, i)
                
                # ë”œë ˆì´
                await asyncio.sleep(self.config.get("default_delay", 2))
                
            except Exception as e:
                self.logger.error(f"âŒ ì¡°ì§ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {org.get('name', 'Unknown')} - {e}")
                self.stats["failed"] += 1
                
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ì›ë³¸ ë°ì´í„°ëŠ” ìœ ì§€
                results.append(org)
        
        self.stats["end_time"] = datetime.now()
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        final_file = await self.save_final_results(results)
        
        # í†µê³„ ì¶œë ¥
        self.print_final_statistics()
        
        return results
    
    async def process_single_organization(self, org: Dict, index: int) -> Dict:
        """ë‹¨ì¼ ì¡°ì§ ì²˜ë¦¬"""
        org_name = org.get('name', 'Unknown')
        self.logger.info(f"ğŸ¢ ì²˜ë¦¬ ì¤‘ [{index}]: {org_name}")
        
        result = org.copy()
        
        try:
            # 1. í™ˆí˜ì´ì§€ URL ê²€ìƒ‰ (ì—†ëŠ” ê²½ìš°)
            if not result.get('homepage'):
                homepage = await self.search_homepage(org_name)
                if homepage:
                    result['homepage'] = homepage
                    self.logger.info(f"  âœ… í™ˆí˜ì´ì§€ ë°œê²¬: {homepage}")
            
            # 2. í™ˆí˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            if result.get('homepage'):
                details = await self.extract_details_from_homepage(result['homepage'])
                result.update(details)
            
            # 3. êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ëˆ„ë½ ì •ë³´ ë³´ì™„
            missing_fields = self.find_missing_fields(result)
            if missing_fields:
                google_results = await self.search_missing_info(org_name, missing_fields)
                result.update(google_results)
            
            # 4. ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
            result = self.validate_and_clean_data(result)
            
            self.logger.info(f"  âœ… ì²˜ë¦¬ ì™„ë£Œ: {org_name}")
            
        except Exception as e:
            self.logger.error(f"  âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {org_name} - {e}")
            result['processing_error'] = str(e)
        
        return result
    
    async def process_json_file_async(self, json_file_path: str, test_mode: bool = False, test_count: int = 10, progress_callback=None) -> List[Dict]:
        """ğŸ”§ app.py í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ"""
        try:
            # JSON íŒŒì¼ ë¡œë“œ
            data = FileUtils.load_json(json_file_path)
            
            # ë°ì´í„° ì „ì²˜ë¦¬ (app.pyì™€ ë™ì¼í•œ ë°©ì‹)
            organizations = []
            if isinstance(data, dict):
                for category, orgs in data.items():
                    if isinstance(orgs, list):
                        for org in orgs:
                            if isinstance(org, dict):
                                org["category"] = category
                                organizations.append(org)
            elif isinstance(data, list):
                organizations = [org for org in data if isinstance(org, dict)]
            
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
            if test_mode and test_count:
                organizations = organizations[:test_count]
            
            # progress_callback ì €ì¥
            self.progress_callback = progress_callback
            
            # ì²˜ë¦¬ ì‹¤í–‰
            results = await self.process_organizations_with_callback(organizations)
            
            return results
            
        except Exception as e:
            self.logger.error(f"JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    async def process_organizations_with_callback(self, organizations: List[Dict]) -> List[Dict]:
        """ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ì¡°ì§ ì²˜ë¦¬"""
        if not organizations:
            return []
        
        self.stats["start_time"] = datetime.now()
        self.stats["total_processed"] = len(organizations)
        
        results = []
        
        for i, org in enumerate(organizations, 1):
            try:
                # ì²˜ë¦¬ ì‹œì‘ ì½œë°±
                if hasattr(self, 'progress_callback') and self.progress_callback:
                    callback_data = {
                        'name': org.get('name', 'Unknown'),
                        'category': org.get('category', ''),
                        'homepage_url': org.get('homepage', ''),
                        'status': 'processing',
                        'current_step': f'{i}/{len(organizations)}',
                        'processing_time': 0
                    }
                    try:
                        self.progress_callback(callback_data)
                    except Exception as e:
                        self.logger.error(f"ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                # ì‹¤ì œ ì²˜ë¦¬
                start_time = time.time()
                processed_org = await self.process_single_organization(org, i)
                processing_time = time.time() - start_time
                
                results.append(processed_org)
                self.stats["successful"] += 1
                
                # ì²˜ë¦¬ ì™„ë£Œ ì½œë°±
                if hasattr(self, 'progress_callback') and self.progress_callback:
                    callback_data = {
                        'name': processed_org.get('name', 'Unknown'),
                        'category': processed_org.get('category', ''),
                        'homepage_url': processed_org.get('homepage', ''),
                        'status': 'completed',
                        'current_step': f'{i}/{len(organizations)}',
                        'processing_time': processing_time,
                        'phone': processed_org.get('phone', ''),
                        'fax': processed_org.get('fax', ''),
                        'email': processed_org.get('email', ''),
                        'address': processed_org.get('address', ''),
                        'extraction_method': processed_org.get('extraction_method', 'unified_crawler')
                    }
                    try:
                        self.progress_callback(callback_data)
                    except Exception as e:
                        self.logger.error(f"ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                # ë”œë ˆì´
                await asyncio.sleep(self.config.get("default_delay", 2))
                
            except Exception as e:
                self.logger.error(f"âŒ ì¡°ì§ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {org.get('name', 'Unknown')} - {e}")
                self.stats["failed"] += 1
                
                # ì‹¤íŒ¨ ì½œë°±
                if hasattr(self, 'progress_callback') and self.progress_callback:
                    callback_data = {
                        'name': org.get('name', 'Unknown'),
                        'status': 'failed',
                        'error_message': str(e)
                    }
                    try:
                        self.progress_callback(callback_data)
                    except Exception as e:
                        self.logger.error(f"ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                results.append(org)
        
        self.stats["end_time"] = datetime.now()
        return results
    
    async def search_homepage(self, org_name: str) -> Optional[str]:
        """í™ˆí˜ì´ì§€ URL ê²€ìƒ‰"""
        try:
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ êµ¬ê¸€ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
            # ì‹¤ì œë¡œëŠ” seleniumì´ë‚˜ requestsë¥¼ ì‚¬ìš©
            search_query = f"{org_name} í™ˆí˜ì´ì§€"
            self.logger.debug(f"ğŸ” í™ˆí˜ì´ì§€ ê²€ìƒ‰: {search_query}")
            
            # TODO: ì‹¤ì œ ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
            # í˜„ì¬ëŠ” None ë°˜í™˜
            return None
            
        except Exception as e:
            self.logger.error(f"í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_details_from_homepage(self, homepage_url: str) -> Dict:
        """í™ˆí˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ - advcrawler.py ë¡œì§ ì´ì‹"""
        try:
            self.logger.info(f"ğŸŒ í™ˆí˜ì´ì§€ ë¶„ì„: {homepage_url}")
            
            # ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            html_content = self.fetch_webpage_enhanced(homepage_url)
            if not html_content:
                return {"extraction_method": "homepage_crawling", "status": "fetch_failed"}
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            parsed_data = self.parse_with_bs4(html_content, homepage_url)
            
            # ê¸°ë³¸ ì—°ë½ì²˜ ì¶”ì¶œ
            extracted_contacts = {}
            if self.web_parser:
                extracted_contacts = self.extract_with_parser(parsed_data)
            
            # ê²€ì¦
            validated_contacts = {}
            if self.validator and extracted_contacts:
                validated_contacts = self.validate_with_validator(extracted_contacts)
            
            # AI ì¶”ê°€ ì¶”ì¶œ
            ai_contacts = {}
            if self.use_ai and self.ai_manager:
                try:
                    ai_contacts = await self.enhance_with_ai(parsed_data, homepage_url)
                    self.stats["ai_enhanced"] += 1
                except Exception as e:
                    self.logger.warning(f"AI ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    self.stats["ai_failures"] += 1
            
            # ê²°ê³¼ ë³‘í•©
            final_contacts = self.merge_extraction_results(validated_contacts, ai_contacts)
            
            extracted_data = {
                "extraction_method": "homepage_crawling",
                "extraction_timestamp": datetime.now().isoformat(),
                "source_url": homepage_url,
                "status": "completed",
                **final_contacts
            }
            
            if any(final_contacts.values()):
                self.stats["contacts_found"] += 1
                self.stats["successful_crawls"] += 1
            else:
                self.stats["failed_crawls"] += 1
            
            return extracted_data
            
        except Exception as e:
            self.logger.error(f"í™ˆí˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.stats["failed_crawls"] += 1
            return {"extraction_method": "homepage_crawling", "status": "error", "error": str(e)}
    
    async def search_missing_info(self, org_name: str, missing_fields: List[str]) -> Dict:
        """êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ëˆ„ë½ ì •ë³´ ê²€ìƒ‰ - advcrawler.py ë¡œì§ ì´ì‹"""
        try:
            self.logger.info(f"ğŸ” ëˆ„ë½ ì •ë³´ ê²€ìƒ‰: {org_name} - {missing_fields}")
            
            # êµ¬ê¸€ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
            if not self.google_searcher:
                try:
                    self.google_searcher = GoogleContactSearcher()
                    self.logger.info("ğŸ” êµ¬ê¸€ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.warning(f"êµ¬ê¸€ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    return {}
            
            # êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ì—°ë½ì²˜ ê²€ìƒ‰
            google_contacts = await self.google_searcher.search_organization_contacts(org_name)
            self.stats["google_searches_performed"] += 1
            
            if sum(len(v) for v in google_contacts.values()) > 0:
                self.stats["google_contacts_found"] += 1
            
            # ê²°ê³¼ ë³€í™˜ (missing_fieldsì— ë§ê²Œ)
            results = {}
            field_mapping = {
                'phone': google_contacts.get('phones', []),
                'fax': google_contacts.get('faxes', []),
                'email': google_contacts.get('emails', []),
                'address': google_contacts.get('addresses', [])
            }
            
            for field in missing_fields:
                if field in field_mapping and field_mapping[field]:
                    results[field] = field_mapping[field][0]  # ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©
            
            return results
            
        except Exception as e:
            self.logger.error(f"ëˆ„ë½ ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {}
    
    def find_missing_fields(self, org: Dict) -> List[str]:
        """ëˆ„ë½ëœ í•„ë“œ ì°¾ê¸°"""
        required_fields = ["phone", "fax", "email", "address"]
        missing = []
        
        for field in required_fields:
            if not org.get(field) or str(org.get(field)).strip() == "":
                missing.append(field)
        
        return missing
    
    def validate_and_clean_data(self, org: Dict) -> Dict:
        """ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬"""
        try:
            # ì „í™”ë²ˆí˜¸ ê²€ì¦
            if org.get('phone'):
                org['phone'] = self.phone_utils.clean_phone_number(org['phone'])
                if not self.phone_utils.validate_phone_number(org['phone']):
                    org['phone_validation_error'] = "Invalid phone format"
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦
            if org.get('fax'):
                org['fax'] = self.phone_utils.clean_phone_number(org['fax'])
            
            # ì´ë©”ì¼ ê²€ì¦
            if org.get('email'):
                # ê°„ë‹¨í•œ ì´ë©”ì¼ ê²€ì¦
                import re
                email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
                if not re.match(email_pattern, org['email']):
                    org['email_validation_error'] = "Invalid email format"
            
            return org
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return org
    
    async def save_intermediate_results(self, results: List[Dict], count: int):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        try:
            filename = generate_output_filename("intermediate", OUTPUT_DIR, count=count)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {filename} ({count}ê°œ)")
            
        except Exception as e:
            self.logger.error(f"ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def save_final_results(self, results: List[Dict]) -> str:
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        try:
            filename = generate_output_filename("enhanced_final", OUTPUT_DIR)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ ìµœì¢… ì €ì¥ ì™„ë£Œ: {filename}")
            return str(filename)
            
        except Exception as e:
            self.logger.error(f"ìµœì¢… ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def print_final_statistics(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥ - advcrawler.py ìŠ¤íƒ€ì¼ë¡œ í™•ì¥"""
        duration = self.stats["end_time"] - self.stats["start_time"]
        
        print("\n" + "="*60)
        print("ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ í†µê³„")
        print("="*60)
        print(f"ğŸ“‹ ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"âœ… ì„±ê³µ: {self.stats['successful']}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
        print(f"ğŸŒ ì›¹ í¬ë¡¤ë§ ì„±ê³µ: {self.stats['successful_crawls']}ê°œ")
        print(f"ğŸŒ ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {self.stats['failed_crawls']}ê°œ")
        print(f"ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ìˆ˜í–‰: {self.stats['google_searches_performed']}ê°œ")
        print(f"ğŸ“ êµ¬ê¸€ì—ì„œ ì—°ë½ì²˜ ë°œê²¬: {self.stats['google_contacts_found']}ê°œ")
        print(f"ğŸ¤– AI í˜¸ì¶œ: {self.stats['api_calls_made']}íšŒ")
        print(f"ğŸ¯ AI ì„±ê³µ: {self.stats['ai_enhanced']}ê°œ")
        print(f"âš ï¸ AI ì‹¤íŒ¨: {self.stats['ai_failures']}ê°œ")
        print(f"ğŸ“ ì´ ì—°ë½ì²˜ ë°œê²¬: {self.stats['contacts_found']}ê°œ")
        
        if self.stats['total_processed'] > 0:
            success_rate = (self.stats['successful'] / self.stats['total_processed']) * 100
            print(f"ğŸ“ˆ ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        if self.stats['successful_crawls'] + self.stats['failed_crawls'] > 0:
            crawl_success_rate = (self.stats['successful_crawls'] / (self.stats['successful_crawls'] + self.stats['failed_crawls'])) * 100
            print(f"ğŸŒ ì›¹ í¬ë¡¤ë§ ì„±ê³µë¥ : {crawl_success_rate:.1f}%")
        
        if self.stats['api_calls_made'] > 0:
            ai_success_rate = (self.stats['ai_enhanced'] / self.stats['api_calls_made']) * 100
            print(f"ğŸ¤– AI ì„±ê³µë¥ : {ai_success_rate:.1f}%")
        
        if self.stats['google_searches_performed'] > 0:
            google_success_rate = (self.stats['google_contacts_found'] / self.stats['google_searches_performed']) * 100
            print(f"ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ì„±ê³µë¥ : {google_success_rate:.1f}%")
        
        print(f"â±ï¸ ì†Œìš”ì‹œê°„: {duration}")
        print(f"ğŸš€ í‰ê·  ì²˜ë¦¬ì‹œê°„: {duration.total_seconds()/self.stats['total_processed']:.2f}ì´ˆ/ê°œ")
        print("="*60)

    # ==================== advcrawler.py í•µì‹¬ ë©”ì„œë“œë“¤ ì´ì‹ ====================
    
    def fetch_webpage_enhanced(self, url: str, max_retries: int = 3) -> Optional[str]:
        """ê°•í™”ëœ ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ì°¨ë‹¨ ìš°íšŒ) - advcrawler.pyì—ì„œ ì´ì‹"""
        if not url or not url.startswith(('http://', 'https://')):
            return None
        
        # ë‹¤ì–‘í•œ í—¤ë” ì„¸íŠ¸
        header_sets = [
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1'
            },
            {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
        ]
        
        for attempt in range(max_retries):
            try:
                # í—¤ë” ë¡œí…Œì´ì…˜
                headers = header_sets[attempt % len(header_sets)]
                
                self.logger.info(f"ì›¹í˜ì´ì§€ ìš”ì²­ ì‹œë„ {attempt + 1}/{max_retries}: {url}")
                
                # ìƒˆ ì„¸ì…˜ ìƒì„± (í•„ìš”ì‹œ)
                if attempt > 0:
                    self.session.close()
                    self.session = requests.Session()
                
                # í—¤ë” ì„¤ì •
                self.session.headers.clear()
                self.session.headers.update(headers)
                
                response = self.session.get(
                    url, 
                    timeout=(10, 30),  # (ì—°ê²°, ì½ê¸°) íƒ€ì„ì•„ì›ƒ
                    verify=False,
                    allow_redirects=True
                )
                
                if response.status_code == 200:
                    # ì¸ì½”ë”© ì„¤ì •
                    if response.encoding is None:
                        response.encoding = response.apparent_encoding or 'utf-8'
                    
                    content = response.text
                    
                    # ì°¨ë‹¨ í˜ì´ì§€ ê°ì§€
                    if self.is_blocked_content(content):
                        self.logger.warning(f"ì°¨ë‹¨ëœ ì½˜í…ì¸  ê°ì§€ (ì‹œë„ {attempt + 1}): {url}")
                        if attempt < max_retries - 1:
                            delay = random.uniform(5, 10)
                            time.sleep(delay)
                            continue
                        else:
                            return None
                    
                    self.logger.info(f"ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {url}")
                    return content
                    
                elif response.status_code == 403:
                    self.logger.warning(f"ì ‘ê·¼ ê±°ë¶€ (403) - ì‹œë„ {attempt + 1}: {url}")
                    if attempt < max_retries - 1:
                        delay = random.uniform(10, 20)
                        time.sleep(delay)
                        continue
                        
                else:
                    self.logger.warning(f"HTTP ì˜¤ë¥˜ {response.status_code} (ì‹œë„ {attempt + 1}): {url}")
                    
            except requests.exceptions.Timeout:
                self.logger.warning(f"íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}): {url}")
            except requests.exceptions.ConnectionError:
                self.logger.warning(f"ì—°ê²° ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {url}")
            except Exception as e:
                self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {url} - {e}")
            
            # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            if attempt < max_retries - 1:
                delay = random.uniform(3, 8)
                time.sleep(delay)
        
        self.logger.error(f"ëª¨ë“  ì‹œë„ ì‹¤íŒ¨: {url}")
        return None

    def is_blocked_content(self, content: str) -> bool:
        """ì°¨ë‹¨ëœ ì½˜í…ì¸  ê°ì§€"""
        if not content or len(content) < 100:
            return True
        
        block_indicators = [
            'access denied', 'forbidden', 'blocked', 'ì ‘ê·¼ì´ ê±°ë¶€', 'ì°¨ë‹¨ëœ',
            'cloudflare', 'checking your browser', 'ddos protection', 'security check', 'captcha'
        ]
        
        content_lower = content.lower()
        return any(indicator in content_lower for indicator in block_indicators)

    def parse_with_bs4(self, html_content: str, base_url: str) -> Dict[str, Any]:
        """BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            parsed_data = {
                'title': '',
                'meta_description': '',
                'all_text': '',
                'footer_text': '',
                'contact_sections': []
            }
            
            # ì œëª© ì¶”ì¶œ
            title_tag = soup.find('title')
            if title_tag:
                parsed_data['title'] = title_tag.get_text().strip()
            
            # ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                parsed_data['meta_description'] = meta_desc.get('content', '')
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°)
            for script in soup(["script", "style"]):
                script.decompose()
            
            parsed_data['all_text'] = soup.get_text()
            
            # footer ì˜ì—­ ì¶”ì¶œ
            footer_elements = soup.find_all(['footer', 'div'], 
                                        class_=re.compile(r'footer|bottom|contact|info', re.I))
            footer_texts = []
            for footer in footer_elements:
                footer_text = footer.get_text().strip()
                if footer_text and len(footer_text) > 20:
                    footer_texts.append(footer_text)
            
            parsed_data['footer_text'] = '\n'.join(footer_texts)
            
            # ì—°ë½ì²˜ ê´€ë ¨ ì„¹ì…˜ ì¶”ì¶œ
            contact_keywords = ['ì—°ë½ì²˜', 'contact', 'ì „í™”', 'phone', 'íŒ©ìŠ¤', 'fax', 'ì´ë©”ì¼', 'email']
            contact_elements = soup.find_all(string=re.compile('|'.join(contact_keywords), re.I))
            
            for element in contact_elements[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                parent = element.parent
                if parent:
                    section_text = parent.get_text().strip()
                    if len(section_text) > 10:
                        parsed_data['contact_sections'].append(section_text)
            
            return parsed_data
            
        except Exception as e:
            self.logger.error(f"BS4 íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {'all_text': html_content[:5000]}  # ì‹¤íŒ¨ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì¼ë¶€ ë°˜í™˜

    def extract_with_parser(self, parsed_data: Dict[str, Any]) -> Dict[str, List]:
        """parser.pyë¥¼ ì´ìš©í•œ ê¸°ë³¸ ì¶”ì¶œ"""
        try:
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
            all_text = parsed_data.get('all_text', '')
            footer_text = parsed_data.get('footer_text', '')
            contact_sections = ' '.join(parsed_data.get('contact_sections', []))
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ í…ìŠ¤íŠ¸ ê²°í•©
            combined_text = f"{footer_text}\n{contact_sections}\n{all_text}"
            
            # parserë¡œ ì¶”ì¶œ
            extracted_contacts = self.web_parser.extract_contact_info(combined_text)
            
            self.logger.info(f"Parser ì¶”ì¶œ ì™„ë£Œ: {len(extracted_contacts.get('phones', []))}ê°œ ì „í™”ë²ˆí˜¸, "
                           f"{len(extracted_contacts.get('faxes', []))}ê°œ íŒ©ìŠ¤ë²ˆí˜¸, "
                           f"{len(extracted_contacts.get('emails', []))}ê°œ ì´ë©”ì¼")
            
            return extracted_contacts
            
        except Exception as e:
            self.logger.error(f"Parser ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {'phones': [], 'faxes': [], 'emails': [], 'addresses': []}

    def validate_with_validator(self, extracted_data: Dict[str, List]) -> Dict[str, List]:
        """validator.pyë¥¼ ì´ìš©í•œ ê²€ì¦"""
        try:
            validated_data = {
                'phones': [],
                'faxes': [],
                'emails': [],
                'addresses': [],
                'postal_codes': []
            }
            
            # ì „í™”ë²ˆí˜¸ ê²€ì¦
            for phone in extracted_data.get('phones', []):
                is_valid, result = self.validator.validate_phone_number(phone)
                if is_valid:
                    validated_data['phones'].append(result)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦
            for fax in extracted_data.get('faxes', []):
                is_valid, result = self.validator.validate_fax_number(fax)
                if is_valid:
                    validated_data['faxes'].append(result)
            
            # ì´ë©”ì¼ì€ ê¸°ë³¸ ê²€ì¦ë§Œ
            validated_data['emails'] = extracted_data.get('emails', [])
            validated_data['addresses'] = extracted_data.get('addresses', [])
            
            self.logger.info(f"Validator ê²€ì¦ ì™„ë£Œ: {len(validated_data['phones'])}ê°œ ìœ íš¨ ì „í™”ë²ˆí˜¸, "
                           f"{len(validated_data['faxes'])}ê°œ ìœ íš¨ íŒ©ìŠ¤ë²ˆí˜¸")
            
            return validated_data
            
        except Exception as e:
            self.logger.error(f"Validator ê²€ì¦ ì˜¤ë¥˜: {e}")
            return extracted_data

    async def enhance_with_ai(self, parsed_data: Dict[str, Any], source_url: str) -> Dict[str, List]:
        """AIë¥¼ ì´ìš©í•œ ì¶”ê°€ ì¶”ì¶œ"""
        if not self.use_ai or not self.ai_manager:
            return {}
        
        try:
            self.logger.info(f"ğŸ¤– AI ì¶”ê°€ ì¶”ì¶œ ì‹œì‘: {source_url}")
            self.stats['api_calls_made'] += 1
            
            # AIìš© í…ìŠ¤íŠ¸ ì¤€ë¹„ (ê¸¸ì´ ì œí•œ)
            all_text = parsed_data.get('all_text', '')
            footer_text = parsed_data.get('footer_text', '')
            contact_sections = ' '.join(parsed_data.get('contact_sections', []))
            
            # ì¤‘ìš” ì„¹ì…˜ ìš°ì„  ì¡°í•©
            ai_text_parts = []
            if footer_text:
                ai_text_parts.append(f"=== Footer ì •ë³´ ===\n{footer_text[:1500]}")
            if contact_sections:
                ai_text_parts.append(f"=== ì—°ë½ì²˜ ì„¹ì…˜ ===\n{contact_sections[:1500]}")
            if all_text:
                ai_text_parts.append(f"=== ê¸°íƒ€ ë‚´ìš© ===\n{all_text[:2000]}")
            
            ai_text = '\n\n'.join(ai_text_parts)
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ìµœëŒ€ 5000ì)
            if len(ai_text) > 5000:
                ai_text = ai_text[:5000]
            
            # AI í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""
ë‹¤ìŒ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì—°ë½ì²˜ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ì¶”ì¶œí•  ì •ë³´:**
- ì „í™”ë²ˆí˜¸: í•œêµ­ í˜•ì‹ (02-1234-5678, 031-123-4567, 010-1234-5678)
- íŒ©ìŠ¤ë²ˆí˜¸: í•œêµ­ í˜•ì‹ (02-1234-5679)
- ì´ë©”ì¼: ìœ íš¨í•œ í˜•ì‹ (info@example.com)
- ì£¼ì†Œ: ì™„ì „í•œ ì£¼ì†Œ

**ì‘ë‹µ í˜•ì‹:**
```markdown
**ì „í™”ë²ˆí˜¸:** [ë°œê²¬ëœ ë²ˆí˜¸ ë˜ëŠ” "ì—†ìŒ"]
**íŒ©ìŠ¤ë²ˆí˜¸:** [ë°œê²¬ëœ ë²ˆí˜¸ ë˜ëŠ” "ì—†ìŒ"]  
**ì´ë©”ì¼:** [ë°œê²¬ëœ ì´ë©”ì¼ ë˜ëŠ” "ì—†ìŒ"]
**ì£¼ì†Œ:** [ë°œê²¬ëœ ì£¼ì†Œ ë˜ëŠ” "ì—†ìŒ"]
```

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
{ai_text}
"""
            
            # AI í˜¸ì¶œ
            ai_response = await self.ai_manager.extract_with_gemini(ai_text, prompt)
            
            if ai_response:
                ai_extracted = self.parse_ai_response(ai_response)
                self.logger.info(f"âœ… AI ì¶”ì¶œ ì™„ë£Œ")
                return ai_extracted
            else:
                self.logger.warning(f"âš ï¸ AI ì‘ë‹µ ì—†ìŒ")
                return {}
                
        except Exception as e:
            self.logger.error(f"âŒ AI ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {}

    def parse_ai_response(self, ai_response: str) -> Dict[str, List]:
        """AI ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”"""
        try:
            result = {
                'phones': [],
                'faxes': [],
                'emails': [],
                'addresses': []
            }
            
            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ íŒŒì‹±
            lines = ai_response.split('\n')
            
            for line in lines:
                line = line.strip()
                if ':' in line and ('**' in line or '*' in line):
                    # ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ ì œê±°
                    line = line.replace('**', '').replace('*', '')
                    
                    try:
                        key, value = line.split(':', 1)
                        key = key.strip().lower()
                        value = value.strip()
                        
                        if value and value not in ["ì—†ìŒ", "ì •ë³´ì—†ìŒ", "í™•ì¸ì•ˆë¨", "-"]:
                            if 'ì „í™”ë²ˆí˜¸' in key or 'phone' in key:
                                if self._is_valid_phone_format(value):
                                    result['phones'].append(value)
                            elif 'íŒ©ìŠ¤' in key or 'fax' in key:
                                if self._is_valid_phone_format(value):
                                    result['faxes'].append(value)
                            elif 'ì´ë©”ì¼' in key or 'email' in key:
                                if self._is_valid_email_format(value):
                                    result['emails'].append(value)
                            elif 'ì£¼ì†Œ' in key or 'address' in key:
                                if len(value) > 10:
                                    result['addresses'].append(value)
                    except ValueError:
                        continue
            
            return result
            
        except Exception as e:
            self.logger.error(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {}

    def _is_valid_phone_format(self, phone: str) -> bool:
        """ì „í™”ë²ˆí˜¸ í˜•ì‹ ê²€ì¦"""
        phone_pattern = r'^\d{2,3}-\d{3,4}-\d{4}$'
        return bool(re.match(phone_pattern, phone))
    
    def _is_valid_email_format(self, email: str) -> bool:
        """ì´ë©”ì¼ í˜•ì‹ ê²€ì¦"""
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return bool(re.match(email_pattern, email))

    def merge_extraction_results(self, validated_result: Dict, ai_result: Dict) -> Dict:
        """ì¶”ì¶œ ê²°ê³¼ ë³‘í•©"""
        merged = {
            'phone': '',
            'fax': '',
            'email': '',
            'address': ''
        }
        
        # AI ê²°ê³¼ ìš°ì„ , ê²€ì¦ëœ ê²°ê³¼ë¡œ ë³´ì™„
        field_mappings = [
            ('phones', 'phone'),
            ('faxes', 'fax'),
            ('emails', 'email'),
            ('addresses', 'address')
        ]
        
        for source_field, target_field in field_mappings:
            # AI ê²°ê³¼ ìš°ì„ 
            if ai_result.get(source_field):
                merged[target_field] = ai_result[source_field][0]
            # ê²€ì¦ëœ ê²°ê³¼ë¡œ ë³´ì™„
            elif validated_result.get(source_field):
                merged[target_field] = validated_result[source_field][0]
        
        return merged

# ==================== GoogleContactSearcher í´ë˜ìŠ¤ ì¶”ê°€ ====================

class GoogleContactSearcher:
    """êµ¬ê¸€ ê²€ìƒ‰ì„ í†µí•œ ì—°ë½ì²˜ ì •ë³´ ì§ì ‘ ê²€ìƒ‰ - advcrawler.pyì—ì„œ ì´ì‹"""
    
    def __init__(self):
        try:
            self.session = requests.Session()
            self.user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'
            ]
            self.current_ua_index = 0
            self.setup_session()
            
        except Exception as e:
            raise Exception(f"GoogleContactSearcher ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def setup_session(self):
        """ì„¸ì…˜ ì„¤ì •"""
        ua = self.user_agents[self.current_ua_index % len(self.user_agents)]
        self.current_ua_index += 1
        
        self.session.headers.clear()
        self.session.headers.update({
            'User-Agent': ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    async def search_organization_contacts(self, organization_name: str) -> Dict[str, List]:
        """ê¸°ê´€ëª…ìœ¼ë¡œ ì—°ë½ì²˜ ì •ë³´ ê²€ìƒ‰"""
        all_contacts = {
            'phones': [],
            'faxes': [],
            'emails': [],
            'addresses': []
        }
        
        try:
            # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬
            search_queries = [
                f'"{organization_name}" ì „í™”ë²ˆí˜¸',
                f'"{organization_name}" ì—°ë½ì²˜',
                f'{organization_name} contact'
            ]
            
            # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ (ìµœëŒ€ 2ê°œë§Œ)
            for i, query in enumerate(search_queries[:2]):
                try:
                    # êµ¬ê¸€ ê²€ìƒ‰
                    html_content = self.search_google_with_retry(query)
                    
                    if html_content:
                        # ì—°ë½ì²˜ ì¶”ì¶œ
                        extracted = self.extract_contacts_from_search_results(html_content, organization_name)
                        
                        # ê²°ê³¼ ë³‘í•©
                        for contact_type, contact_list in extracted.items():
                            for contact in contact_list:
                                if contact not in all_contacts[contact_type]:
                                    all_contacts[contact_type].append(contact)
                    
                    # ê²€ìƒ‰ ê°„ê²©
                    await asyncio.sleep(random.uniform(2, 4))
                    
                except Exception as e:
                    continue
        
        except Exception as e:
            pass
        
        return all_contacts
    
    def search_google_with_retry(self, query: str, max_retries: int = 2) -> Optional[str]:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ êµ¬ê¸€ ê²€ìƒ‰"""
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    self.setup_session()
                
                search_url = f"https://www.google.com/search?q={requests.utils.quote(query)}&hl=ko&num=5"
                
                response = self.session.get(search_url, timeout=10, verify=False)
                
                if response.status_code == 200:
                    if not self.is_blocked_response(response.text):
                        return response.text
                
            except Exception as e:
                continue
            
            if attempt < max_retries - 1:
                time.sleep(random.uniform(3, 6))
        
        return None
    
    def is_blocked_response(self, html: str) -> bool:
        """êµ¬ê¸€ ì°¨ë‹¨ ì‘ë‹µ ê°ì§€"""
        if not html:
            return True
            
        block_indicators = ['unusual traffic', 'automated queries', 'captcha', 'blocked']
        html_lower = html.lower()
        return any(indicator in html_lower for indicator in block_indicators)
    
    def extract_contacts_from_search_results(self, html: str, organization_name: str) -> Dict[str, List]:
        """êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            all_text = soup.get_text()
            
            contacts = {
                'phones': [],
                'faxes': [],
                'emails': [],
                'addresses': []
            }
            
            # ì „í™”ë²ˆí˜¸ íŒ¨í„´ (í•œêµ­)
            phone_patterns = [
                r'\b0\d{1,2}-\d{3,4}-\d{4}\b',      # 02-1234-5678
                r'\b010-\d{4}-\d{4}\b',             # 010-1234-5678
            ]
            
            # ì´ë©”ì¼ íŒ¨í„´
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            
            # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in phone_patterns:
                matches = re.findall(pattern, all_text)
                for match in matches:
                    clean_phone = re.sub(r'[^\d-]', '', match)
                    if len(clean_phone) >= 9 and clean_phone not in contacts['phones']:
                        contacts['phones'].append(clean_phone)
            
            # ì´ë©”ì¼ ì¶”ì¶œ
            email_matches = re.findall(email_pattern, all_text)
            for email in email_matches:
                if email not in contacts['emails']:
                    contacts['emails'].append(email)
            
            return contacts
            
        except Exception as e:
            return {'phones': [], 'faxes': [], 'emails': [], 'addresses': []}

# í¸ì˜ í•¨ìˆ˜ë“¤
async def crawl_from_file(input_file: str, options: Dict = None) -> List[Dict]:
    """íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ í¬ë¡¤ë§"""
    try:
        # íŒŒì¼ ë¡œë“œ
        data = FileUtils.load_json(input_file)
        if not data:
            raise ValueError(f"íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        
        # í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
        crawler = UnifiedCrawler()
        results = await crawler.process_organizations(data, options)
        
        return results
        
    except Exception as e:
        logging.error(f"íŒŒì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

async def crawl_latest_file(options: Dict = None) -> List[Dict]:
    """ìµœì‹  ì…ë ¥ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ í¬ë¡¤ë§"""
    try:
        latest_file = get_latest_input_file()
        if not latest_file:
            raise ValueError("ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ“‚ ìµœì‹  íŒŒì¼ ì‚¬ìš©: {latest_file}")
        return await crawl_from_file(str(latest_file), options)
        
    except Exception as e:
        logging.error(f"ìµœì‹  íŒŒì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*60)
    
    try:
        # í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
        initialize_project()
        
        # ìµœì‹  íŒŒì¼ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        results = await crawl_latest_file()
        
        if results:
            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ì¡°ì§ ì²˜ë¦¬")
        else:
            print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main()) 