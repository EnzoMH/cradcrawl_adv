"""
AI ê°•í™” í†µí•© í¬ë¡¤ë§ ì—”ì§„ v4.0
âœ… ê¸°ì¡´ ì „ë¬¸ ëª¨ë“ˆ + AI Agentic Workflow í†µí•©
âœ… ìµœê³  í’ˆì§ˆì˜ AI ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ
âœ… ì‹ ë¢°ë„ ì ìˆ˜ ë° ë‹¤ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'test'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'cralwer'))

import asyncio
import json
import time
import re
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

# í”„ë¡œì íŠ¸ ì„¤ì • import
from utils.settings import *
from utils.settings import get_latest_input_file, initialize_project
from utils.logger_utils import LoggerUtils
from utils.file_utils import FileUtils
from utils.phone_utils import PhoneUtils
from utils.crawler_utils import CrawlerUtils
from utils.ai_helpers import AIModelManager


# ì „ë¬¸ ëª¨ë“ˆë“¤ import (ê¸°ì¡´ ìœ ì§€)
try:
    from cralwer.fax_extractor import GoogleContactCrawler as FaxExtractor
    FAX_EXTRACTOR_AVAILABLE = True
    print("âœ… fax_extractor.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ fax_extractor.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    FAX_EXTRACTOR_AVAILABLE = False

try:
    from cralwer.phone_extractor import extract_phone_numbers, search_phone_number, setup_driver
    PHONE_EXTRACTOR_AVAILABLE = True
    print("âœ… phone_extractor.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ phone_extractor.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    PHONE_EXTRACTOR_AVAILABLE = False

try:
    from cralwer.url_extractor import HomepageParser
    URL_EXTRACTOR_AVAILABLE = True
    print("âœ… url_extractor.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ url_extractor.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    URL_EXTRACTOR_AVAILABLE = False

try:
    from utils.validator import ContactValidator, AIValidator
    VALIDATOR_AVAILABLE = True
    print("âœ… validator.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ validator.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    VALIDATOR_AVAILABLE = False

try:
    from database.database import get_database
    DATABASE_AVAILABLE = True
    print("âœ… database.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ database.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    DATABASE_AVAILABLE = False

# ==================== AI Agentic Workflow ì‹œìŠ¤í…œ í†µí•© ====================

class CrawlingStage(Enum):
    """í¬ë¡¤ë§ ë‹¨ê³„ ì •ì˜"""
    INITIALIZATION = "ì´ˆê¸°í™”"
    HOMEPAGE_SEARCH = "í™ˆí˜ì´ì§€_ê²€ìƒ‰"
    HOMEPAGE_ANALYSIS = "í™ˆí˜ì´ì§€_ë¶„ì„"
    CONTACT_PAGE_SEARCH = "ì—°ë½ì²˜í˜ì´ì§€_ê²€ìƒ‰"
    CONTACT_EXTRACTION = "ì—°ë½ì²˜_ì¶”ì¶œ"
    FAX_SEARCH = "íŒ©ìŠ¤_ê²€ìƒ‰"
    AI_VERIFICATION = "AI_ê²€ì¦"
    DATA_VALIDATION = "ë°ì´í„°_ê²€ì¦"
    COMPLETION = "ì™„ë£Œ"

@dataclass
class CrawlingContext:
    """í¬ë¡¤ë§ ì»¨í…ìŠ¤íŠ¸ (AI Agent ê°„ ê³µìœ  ë°ì´í„°)"""
    organization: Dict[str, Any]
    current_stage: CrawlingStage
    extracted_data: Dict[str, Any]
    ai_insights: Dict[str, Any]
    error_log: List[str]
    processing_time: float
    confidence_scores: Dict[str, float]

class AIAgent:
    """AI ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler=None):
        self.name = name
        self.ai_manager = ai_manager
        self.logger = logger
        self.parent_crawler = parent_crawler  # Optionalë¡œ ë³€ê²½
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    async def should_execute(self, context: CrawlingContext) -> bool:
        """ì‹¤í–‰ ì¡°ê±´ í™•ì¸"""
        return True
    
    def update_confidence(self, context: CrawlingContext, field: str, score: float):
        """ì‹ ë¢°ë„ ì ìˆ˜ ì—…ë°ì´íŠ¸"""
        context.confidence_scores[field] = score

class EnhancedHomepageSearchAgent(AIAgent):
    """AI ê°•í™” í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì—ì´ì „íŠ¸"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler):
        super().__init__("EnhancedHomepageSearchAgent", ai_manager, logger, parent_crawler)
        self.crawler_utils = CrawlerUtils()
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰ (ê³µì‹ì‚¬ì´íŠ¸ + ì†Œì…œë¯¸ë””ì–´ í†µí•©)"""
        try:
            org_name = context.organization.get('name', '')
            category = context.organization.get('category', '')
            self.logger.info(f"ğŸ” [{self.name}] AI í™ˆí˜ì´ì§€ ê²€ìƒ‰: {org_name}")
            
            # ê¸°ì¡´ í™ˆí˜ì´ì§€ê°€ ìˆìœ¼ë©´ AIë¡œ ê²€ì¦
            existing_homepage = context.organization.get('homepage', '')
            if existing_homepage:
                verification_result = await self._verify_homepage_with_ai(existing_homepage, org_name, category)
                if verification_result['is_valid']:
                    context.extracted_data['homepage'] = existing_homepage
                    context.extracted_data['homepage_type'] = verification_result['type']
                    self.update_confidence(context, 'homepage', verification_result['confidence'])
                    context.current_stage = CrawlingStage.HOMEPAGE_ANALYSIS
                    return context
            
            # AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰ (additionalplan.py ìŠ¤íƒ€ì¼ë¡œ ê°•í™”)
            search_results = await self._ai_comprehensive_homepage_search(org_name, category)
            if search_results:
                best_result = search_results[0]
                context.extracted_data['homepage'] = best_result['url']
                context.extracted_data['homepage_type'] = best_result['type']
                context.extracted_data['homepage_confidence'] = best_result['confidence']
                context.extracted_data['homepage_source'] = best_result.get('source', 'ai_search')
                self.update_confidence(context, 'homepage', best_result['confidence'])
                self.logger.info(f"âœ… AI í™ˆí˜ì´ì§€ ë°œê²¬: {best_result['url']} ({best_result['type']})")
            
            context.current_stage = CrawlingStage.HOMEPAGE_ANALYSIS
            return context
            
        except Exception as e:
            context.error_log.append(f"EnhancedHomepageSearchAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _ai_comprehensive_homepage_search(self, org_name: str, category: str) -> List[Dict]:
        """AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰ (ê³µì‹ì‚¬ì´íŠ¸ + ì†Œì…œë¯¸ë””ì–´)"""
        try:
            all_results = []
            
            # 1ë‹¨ê³„: ê¸°ì¡´ ëª¨ë“ˆì˜ homepage_parser í™œìš©
            if self.parent_crawler.homepage_parser:
                try:
                    ai_search_results = await self.parent_crawler.homepage_parser.ai_search_homepage(org_name, category)
                    all_results.extend(ai_search_results)
                except Exception as e:
                    self.logger.warning(f"ê¸°ì¡´ ëª¨ë“ˆ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            
            # 2ë‹¨ê³„: ì§ì ‘ êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ê³µì‹ í™ˆí˜ì´ì§€ ê²€ìƒ‰
            official_results = await self._search_official_homepage(org_name, category)
            all_results.extend(official_results)
            
            # 3ë‹¨ê³„: ì†Œê·œëª¨ ê¸°ê´€ì¸ ê²½ìš° ì†Œì…œë¯¸ë””ì–´ ê²€ìƒ‰ (additionalplan.py ì•„ì´ë””ì–´)
            if self._is_small_organization(org_name, category):
                social_results = await self._search_social_media(org_name, category)
                all_results.extend(social_results)
            
            # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ìˆœ ì •ë ¬
            unique_results = self._deduplicate_results(all_results)
            unique_results.sort(key=lambda x: x['confidence'], reverse=True)
            
            return unique_results[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
            
        except Exception as e:
            self.logger.error(f"ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _search_official_homepage(self, org_name: str, category: str) -> List[Dict]:
        """ê³µì‹ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (additionalplan.pyì—ì„œ ê°€ì ¸ì˜¨ ë¡œì§)"""
        results = []
        
        search_queries = [
            f"{org_name} í™ˆí˜ì´ì§€ site:*.kr",
            f"{org_name} ê³µì‹ì‚¬ì´íŠ¸ site:*.org", 
            f"{org_name} {category} í™ˆí˜ì´ì§€",
            f"{org_name} ê³µì‹í™ˆí˜ì´ì§€"
        ]
        
        # ê°„ë‹¨í•œ êµ¬ê¸€ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” parent_crawlerì˜ ê¸°ëŠ¥ í™œìš©)
        for query in search_queries[:2]:  # ë¦¬ì†ŒìŠ¤ ì ˆì•½ì„ ìœ„í•´ ìƒìœ„ 2ê°œë§Œ
            try:
                self.logger.info(f"ğŸ” ê³µì‹ í™ˆí˜ì´ì§€ ê²€ìƒ‰: {query}")
                
                # ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ ëª¨ë“ˆì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ AIë¡œ ê²€ì¦í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„
                # (ì‹¤ì œ êµ¬ê¸€ ê²€ìƒ‰ì€ ê¸°ì¡´ ëª¨ë“ˆì—ì„œ ì²˜ë¦¬)
                
            except Exception as e:
                self.logger.warning(f"ê³µì‹ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return results
    
    async def _search_social_media(self, org_name: str, category: str) -> List[Dict]:
        """ì†Œì…œë¯¸ë””ì–´ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (additionalplan.pyì˜ í˜ì‹ ì  ì•„ì´ë””ì–´!)"""
        results = []
        
        # ì†Œì…œë¯¸ë””ì–´ ë„ë©”ì¸ ì •ì˜
        SOCIAL_MEDIA_DOMAINS = {
            "blog.naver.com": "ë„¤ì´ë²„ë¸”ë¡œê·¸",
            "cafe.naver.com": "ë„¤ì´ë²„ì¹´í˜", 
            "facebook.com": "í˜ì´ìŠ¤ë¶",
            "instagram.com": "ì¸ìŠ¤íƒ€ê·¸ë¨",
            "youtube.com": "ìœ íŠœë¸Œ"
        }
        
        social_queries = [
            f"{org_name} site:blog.naver.com",
            f"{org_name} site:cafe.naver.com", 
            f"{org_name} site:facebook.com",
            f"{org_name} site:instagram.com"
        ]
        
        self.logger.info(f"ğŸ“± ì†Œì…œë¯¸ë””ì–´ ê²€ìƒ‰ ì‹œì‘: {org_name} (ì†Œê·œëª¨ ê¸°ê´€ìš©)")
        
        # ì‹¤ì œ ê²€ìƒ‰ì€ ê¸°ì¡´ ëª¨ë“ˆ í™œìš©í•˜ë˜, ì—¬ê¸°ì„œëŠ” ê²€ìƒ‰ ì „ëµë§Œ ì •ì˜
        # (êµ¬í˜„ ë³µì¡ë„ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ë¡œì§ë§Œ ì¤€ë¹„)
        
        for query in social_queries[:2]:  # ìƒìœ„ 2ê°œë§Œ ì‹œë„
            try:
                self.logger.info(f"ğŸ” ì†Œì…œë¯¸ë””ì–´ ê²€ìƒ‰: {query}")
                
                # ì†Œì…œë¯¸ë””ì–´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ AIë¡œ ê²€ì¦
                # (ì‹¤ì œ êµ¬í˜„ì‹œ parent_crawlerì˜ ê²€ìƒ‰ ê¸°ëŠ¥ í™œìš©)
                
            except Exception as e:
                self.logger.warning(f"ì†Œì…œë¯¸ë””ì–´ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _is_small_organization(self, org_name: str, category: str) -> bool:
        """ì†Œê·œëª¨ ê¸°ê´€ íŒë³„ (ì†Œì…œë¯¸ë””ì–´ ê²€ìƒ‰ ëŒ€ìƒ)"""
        try:
            # ì†Œê·œëª¨ ê¸°ê´€ íŒë³„ ê¸°ì¤€
            small_org_indicators = [
                "êµíšŒ", "ì„±ë‹¹", "ì ˆ", "ì‚¬ì°°", "ì±„í”Œ", "ì˜ˆë°°ë‹¹",
                "ì˜ì›", "í•œì˜ì›", "ì¹˜ê³¼", "ì•½êµ­",
                "ë¯¸ìš©ì‹¤", "ì¹´í˜", "ì‹ë‹¹", "ìƒì ",
                "í•™ì›", "êµìŠµì†Œ", "ì—°êµ¬ì†Œ"
            ]
            
            # ê¸°ê´€ëª…ì´ë‚˜ ì¹´í…Œê³ ë¦¬ì— ì†Œê·œëª¨ ê¸°ê´€ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
            org_text = f"{org_name} {category}".lower()
            
            for indicator in small_org_indicators:
                if indicator in org_text:
                    self.logger.info(f"ğŸ¢ ì†Œê·œëª¨ ê¸°ê´€ìœ¼ë¡œ íŒë³„: {org_name} ({indicator})")
                    return True
            
            # ê¸°ê´€ëª…ì´ ì§§ìœ¼ë©´ ì†Œê·œëª¨ë¡œ ê°„ì£¼
            if len(org_name) <= 10:
                return True
                
            return False
            
        except Exception as e:
            self.logger.warning(f"ì†Œê·œëª¨ ê¸°ê´€ íŒë³„ ì˜¤ë¥˜: {e}")
            return False
    
    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¤‘ë³µ ì œê±°"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            url = result.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        return unique_results
    
    async def _verify_homepage_with_ai(self, url: str, org_name: str, category: str) -> Dict:
        """AIë¡œ í™ˆí˜ì´ì§€ ê´€ë ¨ì„± ê²€ì¦"""
        try:
            prompt = f"""
            ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™ˆí˜ì´ì§€ì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”:

            **ê¸°ê´€ ì •ë³´:**
            - ê¸°ê´€ëª…: {org_name}
            - ì¹´í…Œê³ ë¦¬: {category}
            - í™ˆí˜ì´ì§€ URL: {url}

            **íŒë‹¨ ê¸°ì¤€:**
            1. ë„ë©”ì¸ëª…ì´ ê¸°ê´€ëª…ê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€?
            2. URLì´ í•´ë‹¹ ê¸°ê´€ì˜ ê³µì‹ ë˜ëŠ” ê´€ë ¨ í˜ì´ì§€ì¸ê°€?
            3. ì†Œê·œëª¨ ê¸°ê´€ì˜ ê²½ìš° ë¸”ë¡œê·¸/ì¹´í˜/SNSë„ ê³µì‹ í˜ì´ì§€ë¡œ ì¸ì •
            4. ê¸°ê´€ì˜ ì„±ê²©ê³¼ URL íƒ€ì…ì´ ì í•©í•œê°€?

            **ì‘ë‹µ í˜•ì‹:**
            VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            TYPE: [ê³µì‹ì‚¬ì´íŠ¸/ë„¤ì´ë²„ë¸”ë¡œê·¸/ë„¤ì´ë²„ì¹´í˜/í˜ì´ìŠ¤ë¶/ì¸ìŠ¤íƒ€ê·¸ë¨/ìœ íŠœë¸Œ/ê¸°íƒ€]
            CONFIDENCE: [0.1-1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„ ì ìˆ˜]
            REASON: [íŒë‹¨ ì´ìœ  1-2ë¬¸ì¥]
            """
            
            response = await self.ai_manager.extract_with_gemini(url, prompt)
            self.logger.info(f"ğŸ¤– [AI í”„ë¡¬í”„íŠ¸] í™ˆí˜ì´ì§€ ê²€ì¦ - {org_name}")
            self.logger.debug(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {prompt}")
            self.logger.info(f"ğŸ¤– [AI ì‘ë‹µ] í™ˆí˜ì´ì§€ ê²€ì¦ - {org_name}")
            self.logger.debug(f"ğŸ“‹ ì‘ë‹µ: {response}")
            parsed_result = self._parse_verification_response(response)
            self.logger.info(f"ğŸ¯ [íŒŒì‹± ê²°ê³¼] {parsed_result}")
            return self._parse_verification_response(response)
            
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {'is_valid': False, 'type': 'ì•Œìˆ˜ì—†ìŒ', 'confidence': 0.0}
    
    def _parse_verification_response(self, response: str) -> Dict:
        """AI ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        result = {
            'is_valid': False,
            'type': 'ì•Œìˆ˜ì—†ìŒ',
            'confidence': 0.0,
            'reason': ''
        }
        
        try:
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('VALID:'):
                    result['is_valid'] = 'ì˜ˆ' in line
                elif line.startswith('TYPE:'):
                    result['type'] = line.replace('TYPE:', '').strip()
                elif line.startswith('CONFIDENCE:'):
                    confidence_text = line.replace('CONFIDENCE:', '').strip()
                    try:
                        result['confidence'] = float(confidence_text)
                    except:
                        result['confidence'] = 0.5
                elif line.startswith('REASON:'):
                    result['reason'] = line.replace('REASON:', '').strip()
        
        except Exception as e:
            self.logger.warning(f"ê²€ì¦ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return result

class EnhancedHomepageAnalysisAgent(AIAgent):
    """AI ê°•í™” í™ˆí˜ì´ì§€ ë¶„ì„ ì—ì´ì „íŠ¸ - BS4 â†’ JS â†’ AI ìˆœì„œ"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler):
        super().__init__("EnhancedHomepageAnalysisAgent", ai_manager, logger, parent_crawler)
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ë‹¨ê³„ë³„ í™ˆí˜ì´ì§€ ë¶„ì„: BS4 â†’ JS ë Œë”ë§ â†’ AI ë¶„ì„"""
        try:
            homepage_url = context.extracted_data.get('homepage')
            if not homepage_url:
                self.logger.info(f"ğŸ“‹ [{self.name}] í™ˆí˜ì´ì§€ê°€ ì—†ì–´ ë¶„ì„ ê±´ë„ˆë›°ê¸°")
                context.current_stage = CrawlingStage.CONTACT_EXTRACTION
                return context
            
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ” [{self.name}] ë‹¨ê³„ë³„ í™ˆí˜ì´ì§€ ë¶„ì„: {homepage_url}")
            
            # 1ë‹¨ê³„: BS4ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            extracted_text = await self._extract_with_bs4(homepage_url)
            soup_object = None
            
            if not extracted_text:
                # 2ë‹¨ê³„: JS ë Œë”ë§ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                extraction_result = await self._extract_with_selenium(homepage_url)
                if extraction_result:
                    extracted_text = extraction_result.get('text')
                    soup_object = extraction_result.get('soup')
            
            if extracted_text:
                # 3ë‹¨ê³„: AIë¡œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
                contact_info = await self._extract_contacts_with_ai(extracted_text, org_name)
                if contact_info:
                    self._store_enhanced_contact_info(context, contact_info)
                    context.extracted_data['homepage_analyzed'] = True
                    self.logger.info(f"âœ… [{self.name}] AI í™ˆí˜ì´ì§€ ë¶„ì„ ì™„ë£Œ")
                
                # 4ë‹¨ê³„: ì—°ë½ì²˜ í˜ì´ì§€ ë§í¬ ì°¾ê¸° (additionalplan.pyì—ì„œ ê°€ì ¸ì˜¨ ê¸°ëŠ¥)
                if soup_object:
                    contact_links = self._find_contact_page_links(soup_object, homepage_url)
                    if contact_links:
                        context.extracted_data['contact_page_links'] = contact_links
                        self.logger.info(f"ğŸ”— ì—°ë½ì²˜ í˜ì´ì§€ ë§í¬ {len(contact_links)}ê°œ ë°œê²¬")
                else:
                    self.logger.warning(f"âš ï¸ [{self.name}] AIì—ì„œ ì—°ë½ì²˜ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•¨")
            else:
                context.error_log.append(f"í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {homepage_url}")
                self.logger.warning(f"âš ï¸ [{self.name}] í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            
            context.current_stage = CrawlingStage.CONTACT_EXTRACTION
            return context
            
        except Exception as e:
            context.error_log.append(f"EnhancedHomepageAnalysisAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    def _find_contact_page_links(self, soup, base_url: str) -> List[Dict]:
        """ì—°ë½ì²˜ í˜ì´ì§€ ë§í¬ ì°¾ê¸° (additionalplan.pyì—ì„œ ê°€ì ¸ì˜¨ ê¸°ëŠ¥)"""
        contact_links = []
        
        try:
            # ì—°ë½ì²˜ ê´€ë ¨ í‚¤ì›Œë“œ
            CONTACT_NAVIGATION_KEYWORDS = [
                "ì—°ë½ì²˜", "Contact", "contact us", "CONTACT US", "ë¬¸ì˜", "ì˜¤ì‹œëŠ”ê¸¸", 
                "ì°¾ì•„ì˜¤ì‹œëŠ”ê¸¸", "ìœ„ì¹˜", "ì£¼ì†Œ", "ì „í™”", "TEL", "ì „í™”ë²ˆí˜¸", "ì—°ë½ë§"
            ]
            
            # ë§í¬ ìš”ì†Œë“¤ ì°¾ê¸°
            links = soup.find_all('a', href=True)
            
            for link in links:
                link_text = link.get_text(strip=True).lower()
                href = link.get('href', '')
                
                # ì—°ë½ì²˜ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                for keyword in CONTACT_NAVIGATION_KEYWORDS:
                    if keyword.lower() in link_text:
                        full_url = self._resolve_url(href, base_url)
                        if full_url:
                            contact_links.append({
                                'url': full_url,
                                'text': link.get_text(strip=True),
                                'keyword': keyword
                            })
                        break
        
        except Exception as e:
            self.logger.warning(f"ì—°ë½ì²˜ í˜ì´ì§€ ë§í¬ ì°¾ê¸° ì˜¤ë¥˜: {e}")
        
        return contact_links[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
    
    def _resolve_url(self, href: str, base_url: str) -> Optional[str]:
        """ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜"""
        try:
            from urllib.parse import urljoin, urlparse
            
            if href.startswith(('http://', 'https://')):
                return href
            elif href.startswith('/'):
                parsed = urlparse(base_url)
                return f"{parsed.scheme}://{parsed.netloc}{href}"
            else:
                return urljoin(base_url, href)
        except:
            return None
    
    async def _extract_with_bs4(self, url: str) -> Optional[str]:
        """1ë‹¨ê³„: BS4ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            import requests
            from bs4 import BeautifulSoup
            
            self.logger.info(f"ğŸ” BS4 í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„: {url}")
            
            headers = REQUEST_HEADERS
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
            for element in soup(["script", "style", "noscript", "meta", "link"]):
                element.decompose()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = soup.get_text()
            text = re.sub(r'\s+', ' ', text).strip()
            
            if len(text) > 500:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                self.logger.info(f"âœ… BS4 ì¶”ì¶œ ì„±ê³µ: {len(text)} chars")
                return text[:10000]  # ìµœëŒ€ 10,000ì
            else:
                self.logger.warning(f"âš ï¸ BS4 ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ: {len(text)} chars")
                return None
                
        except Exception as e:
            self.logger.warning(f"BS4 í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _extract_with_selenium(self, url: str) -> Optional[Dict]:
        """2ë‹¨ê³„: Seleniumìœ¼ë¡œ JS ë Œë”ë§ í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            self.logger.info(f"ğŸ” Selenium JS ë Œë”ë§ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„: {url}")
            
            if self.parent_crawler and self.parent_crawler.homepage_parser:
                page_data = self.parent_crawler.homepage_parser.extract_page_content(url)
                if page_data and page_data.get('accessible') and page_data.get('text_content'):
                    text = page_data['text_content']
                    
                    # BeautifulSoup ê°ì²´ë„ ìƒì„±
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(page_data.get('raw_html', ''), 'html.parser') if page_data.get('raw_html') else None
                    
                    self.logger.info(f"âœ… Selenium ì¶”ì¶œ ì„±ê³µ: {len(text)} chars")
                    return {
                        'text': text,
                        'soup': soup
                    }
            
            self.logger.warning("âš ï¸ Selenium í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.warning(f"Selenium í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _extract_contacts_with_ai(self, text_content: str, org_name: str) -> Optional[Dict]:
        """3ë‹¨ê³„: AIë¡œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            self.logger.info(f"ğŸ¤– AI ì—°ë½ì²˜ ì¶”ì¶œ: {org_name}")
            
            prompt = f"""
            ë‹¤ìŒì€ '{org_name}' ê¸°ê´€ì˜ í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
            ì´ í…ìŠ¤íŠ¸ì—ì„œ ì—°ë½ì²˜ ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

            **í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸:**
            {text_content[:5000]}

            **ì¶”ì¶œí•  ì •ë³´:**
            1. ì „í™”ë²ˆí˜¸ (02-XXX-XXXX, 031-XXX-XXXX í˜•íƒœ)
            2. íŒ©ìŠ¤ë²ˆí˜¸ (ì „í™”ë²ˆí˜¸ì™€ ë‹¤ë¥¸ ë²ˆí˜¸)
            3. ì´ë©”ì¼ ì£¼ì†Œ
            4. ì£¼ì†Œ (ë„ë¡œëª…ì£¼ì†Œ ë˜ëŠ” ì§€ë²ˆì£¼ì†Œ)
            5. íœ´ëŒ€í°ë²ˆí˜¸ (010-XXXX-XXXX í˜•íƒœ)

            **ì‘ë‹µ í˜•ì‹ (JSON):**
            {{
                "phone": "ì „í™”ë²ˆí˜¸ ë˜ëŠ” null",
                "fax": "íŒ©ìŠ¤ë²ˆí˜¸ ë˜ëŠ” null", 
                "email": "ì´ë©”ì¼ ë˜ëŠ” null",
                "address": "ì£¼ì†Œ ë˜ëŠ” null",
                "mobile": "íœ´ëŒ€í° ë˜ëŠ” null"
            }}

            **ì£¼ì˜ì‚¬í•­:**
            - ì •í™•í•œ í˜•íƒœì˜ ì—°ë½ì²˜ë§Œ ì¶”ì¶œ
            - ì „í™”ë²ˆí˜¸ì™€ íŒ©ìŠ¤ë²ˆí˜¸ê°€ ê°™ìœ¼ë©´ íŒ©ìŠ¤ëŠ” null
            - ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ null ê°’ ì‚¬ìš©
            """
            
            if self.ai_manager and self.ai_manager.gemini_model:
                response = self.ai_manager.gemini_model.generate_content(prompt)
                response_text = response.text.strip()
                
                # JSON ì¶”ì¶œ ë° íŒŒì‹±
                contact_info = self._parse_ai_contact_response(response_text)
                
                if contact_info:
                    self.logger.info(f"âœ… AI ì—°ë½ì²˜ ì¶”ì¶œ ì„±ê³µ: {contact_info}")
                    return contact_info
                else:
                    self.logger.warning("âš ï¸ AI ì‘ë‹µì—ì„œ ì—°ë½ì²˜ ì •ë³´ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŒ")
                    return None
            else:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥")
                return None
                
        except Exception as e:
            self.logger.error(f"AI ì—°ë½ì²˜ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_ai_contact_response(self, response_text: str) -> Optional[Dict]:
        """AI ì‘ë‹µì—ì„œ ì—°ë½ì²˜ ì •ë³´ íŒŒì‹±"""
        try:
            import json
            
            # JSON ë¸”ë¡ ì°¾ê¸°
            if '```json' in response_text:
                json_part = response_text.split('```json')[1].split('```')[0].strip()
            elif '{' in response_text and '}' in response_text:
                start = response_text.find('{')
                end = response_text.rfind('}') + 1
                json_part = response_text[start:end]
            else:
                return None
            
            # JSON íŒŒì‹±
            contact_info = json.loads(json_part)
            
            # ê²°ê³¼ ì •ë¦¬ (null ê°’ ì œê±°)
            cleaned_info = {}
            for key, value in contact_info.items():
                if value and value.lower() != 'null' and value.strip():
                    cleaned_info[key] = value.strip()
            
            return cleaned_info if cleaned_info else None
            
        except Exception as e:
            self.logger.warning(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _store_enhanced_contact_info(self, context: CrawlingContext, contact_info: Dict):
        """ì—°ë½ì²˜ ì •ë³´ ì €ì¥ (additionalplan.py ìŠ¤íƒ€ì¼ë¡œ ê°•í™”)"""
        # ê¸°ë³¸ ì—°ë½ì²˜ ì •ë³´
        if contact_info.get('phone'):
            context.extracted_data['phone'] = contact_info['phone']
            self.update_confidence(context, 'phone', 0.9)
        
        if contact_info.get('fax'):
            context.extracted_data['fax'] = contact_info['fax']
            self.update_confidence(context, 'fax', 0.9)
        
        if contact_info.get('email'):
            context.extracted_data['email'] = contact_info['email']
            self.update_confidence(context, 'email', 0.9)
        
        if contact_info.get('address'):
            context.extracted_data['address'] = contact_info['address']
            self.update_confidence(context, 'address', 0.8)
        
        if contact_info.get('mobile'):
            context.extracted_data['mobile'] = contact_info['mobile']
            self.update_confidence(context, 'mobile', 0.8)

class EnhancedContactExtractionAgent(AIAgent):
    """AI ê°•í™” ì—°ë½ì²˜ ì¶”ì¶œ ì—ì´ì „íŠ¸ - ì£¼ì†Œ ê¸°ë°˜ Selenium ê²€ìƒ‰"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler):
        super().__init__("EnhancedContactExtractionAgent", ai_manager, logger, parent_crawler)
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì£¼ì†Œ ê¸°ë°˜ Selenium ê²€ìƒ‰ â†’ AI ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            org_address = context.organization.get('address', '')
            
            self.logger.info(f"ğŸ“ [{self.name}] ì£¼ì†Œ ê¸°ë°˜ ì—°ë½ì²˜ ê²€ìƒ‰: {org_name}")
            
            # ì „í™”ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰
            if not context.extracted_data.get('phone'):
                phone_result = await self._search_phone_with_address(org_name, org_address)
                if phone_result:
                    # AIë¡œ ê²€ì¦
                    is_valid = await self._verify_contact_with_ai(phone_result, org_name, 'phone')
                    if is_valid:
                        context.extracted_data['phone'] = phone_result
                        context.extracted_data['phone_source'] = 'address_based_search'
                        self.update_confidence(context, 'phone', 0.8)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ (ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µ ë°©ì§€)
            if not context.extracted_data.get('fax'):
                fax_result = await self._search_fax_with_address(
                    org_name, org_address, context.extracted_data.get('phone')
                )
                if fax_result:
                    # AIë¡œ ê²€ì¦
                    is_valid = await self._verify_contact_with_ai(fax_result, org_name, 'fax')
                    if is_valid:
                        context.extracted_data['fax'] = fax_result
                        context.extracted_data['fax_source'] = 'address_based_search'
                        self.update_confidence(context, 'fax', 0.8)
            
            context.current_stage = CrawlingStage.AI_VERIFICATION
            return context
            
        except Exception as e:
            context.error_log.append(f"EnhancedContactExtractionAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _search_phone_with_address(self, org_name: str, address: str) -> Optional[str]:
        """ì£¼ì†Œ ê¸°ë°˜ ì „í™”ë²ˆí˜¸ ê²€ìƒ‰"""
        try:
            if not org_name:
                return None
            
            # ì§€ì—­ ì •ë³´ ì¶”ì¶œ
            region_info = self._extract_region_from_address(address)
            search_query = f"{org_name} ì „í™”ë²ˆí˜¸"
            
            if region_info:
                search_query = f"{region_info} {org_name} ì „í™”ë²ˆí˜¸"
            
            self.logger.info(f"ğŸ” ì „í™”ë²ˆí˜¸ ê²€ìƒ‰: {search_query}")
            
            # Seleniumìœ¼ë¡œ ê²€ìƒ‰
            if self.parent_crawler and self.parent_crawler.phone_driver:
                from cralwer.phone_extractor import search_phone_number
                found_phones = search_phone_number(self.parent_crawler.phone_driver, search_query)
                
                if found_phones:
                    # ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ë°˜í™˜ (ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²ƒìœ¼ë¡œ ê°€ì •)
                    phone = found_phones[0]
                    
                    # ì§€ì—­ë²ˆí˜¸ ê²€ì¦
                    if self._validate_phone_by_region(phone, address):
                        self.logger.info(f"âœ… ì „í™”ë²ˆí˜¸ ë°œê²¬: {phone}")
                        return phone
                    else:
                        self.logger.warning(f"âš ï¸ ì§€ì—­ë²ˆí˜¸ ë¶ˆì¼ì¹˜: {phone} (ì£¼ì†Œ: {address})")
            
            return None
            
        except Exception as e:
            self.logger.warning(f"ì „í™”ë²ˆí˜¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    async def _search_fax_with_address(self, org_name: str, address: str, existing_phone: str) -> Optional[str]:
        """ì£¼ì†Œ ê¸°ë°˜ íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ (ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µ ë°©ì§€)"""
        try:
            if not org_name:
                return None
            
            # ì§€ì—­ ì •ë³´ ì¶”ì¶œ
            region_info = self._extract_region_from_address(address)
            search_query = f"{org_name} íŒ©ìŠ¤ë²ˆí˜¸"
            
            if region_info:
                search_query = f"{region_info} {org_name} íŒ©ìŠ¤ë²ˆí˜¸"
            
            self.logger.info(f"ğŸ“  íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰: {search_query}")
            
            # íŒ©ìŠ¤ ì¶”ì¶œê¸°ë¡œ ê²€ìƒ‰
            if self.parent_crawler and self.parent_crawler.fax_extractor:
                found_faxes = self.parent_crawler.fax_extractor.search_fax_number(org_name)
                
                for fax in found_faxes:
                    # ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µ ì²´í¬
                    if not self._is_duplicate_number(fax, existing_phone):
                        # ì§€ì—­ë²ˆí˜¸ ê²€ì¦
                        if self._validate_phone_by_region(fax, address):
                            self.logger.info(f"âœ… íŒ©ìŠ¤ë²ˆí˜¸ ë°œê²¬: {fax}")
                            return fax
                        else:
                            self.logger.warning(f"âš ï¸ íŒ©ìŠ¤ ì§€ì—­ë²ˆí˜¸ ë¶ˆì¼ì¹˜: {fax}")
                
                self.logger.info("ğŸ“  ì¤‘ë³µë˜ì§€ ì•ŠëŠ” íŒ©ìŠ¤ë²ˆí˜¸ ì—†ìŒ")
            
            return None
            
        except Exception as e:
            self.logger.warning(f"íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_region_from_address(self, address: str) -> Optional[str]:
        """ì£¼ì†Œì—ì„œ ì§€ì—­ ì •ë³´ ì¶”ì¶œ"""
        if not address:
            return None
        
        # settings.pyì˜ REGION_TO_AREA_CODE í™œìš©
        for region in REGION_TO_AREA_CODE.keys():
            if region in address:
                return region
        
        return None
    
    def _validate_phone_by_region(self, phone: str, address: str) -> bool:
        """ì§€ì—­ë²ˆí˜¸ì™€ ì£¼ì†Œ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸"""
        if not phone or not address:
            return True  # ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì¼ë‹¨ í†µê³¼
        
        # settings.pyì˜ validate_phone_by_region í™œìš©
        return validate_phone_by_region(phone, address)
    
    def _is_duplicate_number(self, number1: str, number2: str) -> bool:
        """ë‘ ë²ˆí˜¸ê°€ ì¤‘ë³µì¸ì§€ í™•ì¸"""
        if not number1 or not number2:
            return False
        
        # settings.pyì˜ is_phone_fax_duplicate í™œìš©
        return is_phone_fax_duplicate(number1, number2)
    
    async def _verify_contact_with_ai(self, contact: str, org_name: str, contact_type: str) -> bool:
        """AIë¡œ ì—°ë½ì²˜ ìœ íš¨ì„± ê²€ì¦"""
        try:
            prompt = f"""
            ë‹¤ìŒ ì •ë³´ê°€ ì˜¬ë°”ë¥¸ì§€ ê²€ì¦í•´ì£¼ì„¸ìš”:

            **ê¸°ê´€ëª…:** {org_name}
            **{contact_type}:** {contact}

            **ê²€ì¦ ê¸°ì¤€:**
            1. ë²ˆí˜¸ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€? (í•œêµ­ ì „í™”ë²ˆí˜¸ í˜•ì‹)
            2. ê¸°ê´€ëª…ê³¼ ê´€ë ¨ì„±ì´ ìˆì–´ ë³´ì´ëŠ”ê°€?
            3. ìœ íš¨í•œ ë²ˆí˜¸ë¡œ ë³´ì´ëŠ”ê°€?

            **ì‘ë‹µ í˜•ì‹:**
            VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            CONFIDENCE: [0.1-1.0]
            REASON: [íŒë‹¨ ì´ìœ ]
            """
            
            if self.ai_manager and self.ai_manager.gemini_model:
                response = self.ai_manager.gemini_model.generate_content(prompt)
                response_text = response.text.strip()
                
                # ì‘ë‹µ íŒŒì‹±
                is_valid = self._parse_verification_response(response_text)
                self.logger.info(f"ğŸ¤– AI ê²€ì¦ ê²°ê³¼ ({contact_type}): {is_valid}")
                return is_valid
            
            return True  # AIê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
            
        except Exception as e:
            self.logger.warning(f"AI ê²€ì¦ ì‹¤íŒ¨: {e}")
            return True  # ì˜¤ë¥˜ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
    
    def _parse_verification_response(self, response_text: str) -> bool:
        """AI ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        try:
            if 'VALID:' in response_text:
                valid_line = [line for line in response_text.split('\n') if 'VALID:' in line][0]
                return 'ì˜ˆ' in valid_line or 'true' in valid_line.lower()
            
            # ë°±ì—…: ê¸ì •ì  í‚¤ì›Œë“œ ì²´í¬
            positive_keywords = ['ì˜ˆ', 'valid', 'true', 'ì˜¬ë°”', 'ìœ íš¨', 'ì ì ˆ']
            return any(keyword in response_text.lower() for keyword in positive_keywords)
            
        except Exception:
            return True

class AIVerificationAgent(AIAgent):
    """AI ê²€ì¦ ì—ì´ì „íŠ¸"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler):
        super().__init__("AIVerificationAgent", ai_manager, logger, parent_crawler)
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """AI ì¢…í•© ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ” [{self.name}] AI ì¢…í•© ê²€ì¦: {org_name}")
            
            # AI ì¢…í•© ê²€ì¦ ì‹¤í–‰
            verification_result = await self._ai_comprehensive_verification(context)
            
            # ê²€ì¦ ê²°ê³¼ ì ìš©
            context.ai_insights['verification'] = verification_result
            self._adjust_confidence_scores(context, verification_result)
            
            context.current_stage = CrawlingStage.COMPLETION
            return context
            
        except Exception as e:
            context.error_log.append(f"AIVerificationAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _ai_comprehensive_verification(self, context: CrawlingContext) -> Dict[str, Any]:
        """AI ì¢…í•© ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            extracted_data = context.extracted_data
            
            # ê²€ì¦í•  ë°ì´í„° ì¤€ë¹„
            verification_prompt = f"""
            ê¸°ê´€ëª…: {org_name}
            ì¶”ì¶œëœ ë°ì´í„°:
            - í™ˆí˜ì´ì§€: {extracted_data.get('homepage', 'ì—†ìŒ')}
            - ì „í™”ë²ˆí˜¸: {extracted_data.get('phone', 'ì—†ìŒ')}
            - íŒ©ìŠ¤ë²ˆí˜¸: {extracted_data.get('fax', 'ì—†ìŒ')}
            - ì´ë©”ì¼: {extracted_data.get('email', 'ì—†ìŒ')}
            
            ìœ„ ì •ë³´ë“¤ì´ í•´ë‹¹ ê¸°ê´€ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦í•´ì£¼ì„¸ìš”.
            ì‘ë‹µ í˜•ì‹:
            OVERALL_VALIDITY: [valid/invalid/uncertain]
            PHONE_VALIDITY: [valid/invalid/uncertain]
            FAX_VALIDITY: [valid/invalid/uncertain]
            HOMEPAGE_VALIDITY: [valid/invalid/uncertain]
            CONFIDENCE_SCORE: [0.0-1.0]
            """
            
            if self.ai_manager and self.ai_manager.gemini_model:
                response = self.ai_manager.gemini_model.generate_content(verification_prompt)
                response_text = response.text.strip()
                
                # ì‘ë‹µ íŒŒì‹±
                return self._parse_verification_response(response_text)
            else:
                return {
                    'overall_validity': 'uncertain',
                    'confidence_score': 0.5,
                    'verification_method': 'no_ai_available'
                }
                
        except Exception as e:
            self.logger.warning(f"AI ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                'overall_validity': 'uncertain',
                'confidence_score': 0.5,
                'verification_error': str(e)
            }
    
    def _parse_verification_response(self, response_text: str) -> Dict[str, Any]:
        """ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        result = {
            'overall_validity': 'uncertain',
            'phone_validity': 'uncertain',
            'fax_validity': 'uncertain',
            'homepage_validity': 'uncertain',
            'confidence_score': 0.5
        }
        
        try:
            lines = response_text.split('\n')
            for line in lines:
                line = line.strip().upper()
                
                if 'OVERALL_VALIDITY:' in line:
                    validity = line.split(':', 1)[1].strip().lower()
                    if validity in ['valid', 'invalid', 'uncertain']:
                        result['overall_validity'] = validity
                
                elif 'PHONE_VALIDITY:' in line:
                    validity = line.split(':', 1)[1].strip().lower()
                    if validity in ['valid', 'invalid', 'uncertain']:
                        result['phone_validity'] = validity
                
                elif 'FAX_VALIDITY:' in line:
                    validity = line.split(':', 1)[1].strip().lower()
                    if validity in ['valid', 'invalid', 'uncertain']:
                        result['fax_validity'] = validity
                
                elif 'HOMEPAGE_VALIDITY:' in line:
                    validity = line.split(':', 1)[1].strip().lower()
                    if validity in ['valid', 'invalid', 'uncertain']:
                        result['homepage_validity'] = validity
                
                elif 'CONFIDENCE_SCORE:' in line:
                    try:
                        score = float(line.split(':', 1)[1].strip())
                        if 0.0 <= score <= 1.0:
                            result['confidence_score'] = score
                    except ValueError:
                        pass
        
        except Exception as e:
            self.logger.warning(f"ê²€ì¦ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return result
    
    def _adjust_confidence_scores(self, context: CrawlingContext, verification: Dict[str, Any]):
        """ì‹ ë¢°ë„ ì ìˆ˜ ì¡°ì •"""
        try:
            adjustment_factor = verification.get('confidence_score', 0.5)
            
            # ì „ì²´ ìœ íš¨ì„±ì— ë”°ë¥¸ ì¡°ì •
            if verification.get('overall_validity') == 'valid':
                adjustment_factor *= 1.2
            elif verification.get('overall_validity') == 'invalid':
                adjustment_factor *= 0.5
            
            # ê° í•„ë“œë³„ ì‹ ë¢°ë„ ì¡°ì •
            for field in ['phone', 'fax', 'homepage']:
                if field in context.confidence_scores:
                    field_validity = verification.get(f'{field}_validity', 'uncertain')
                    if field_validity == 'valid':
                        context.confidence_scores[field] *= 1.1
                    elif field_validity == 'invalid':
                        context.confidence_scores[field] *= 0.6
                    
                    # ë²”ìœ„ ì œí•œ
                    context.confidence_scores[field] = min(1.0, max(0.0, context.confidence_scores[field]))
        
        except Exception as e:
            self.logger.warning(f"ì‹ ë¢°ë„ ì ìˆ˜ ì¡°ì • ì‹¤íŒ¨: {e}")

class ContactPageSearchAgent(AIAgent):
    """ì—°ë½ì²˜ í˜ì´ì§€ ì „ìš© ê²€ìƒ‰ AI ì—ì´ì „íŠ¸ (additionalplan.pyì—ì„œ ê°€ì ¸ì˜¨ í˜ì‹ ì  ê¸°ëŠ¥!)"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler):
        super().__init__("ContactPageSearchAgent", ai_manager, logger, parent_crawler)
    
    async def should_execute(self, context: CrawlingContext) -> bool:
        """ì—°ë½ì²˜ í˜ì´ì§€ ë§í¬ê°€ ìˆì„ ë•Œë§Œ ì‹¤í–‰"""
        return bool(context.extracted_data.get('contact_page_links'))
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì—°ë½ì²˜ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ"""
        try:
            contact_links = context.extracted_data.get('contact_page_links', [])
            org_name = context.organization.get('name', '')
            
            self.logger.info(f"ğŸ“ [{self.name}] ì—°ë½ì²˜ í˜ì´ì§€ ê²€ìƒ‰: {len(contact_links)}ê°œ ë§í¬")
            
            additional_contacts = []
            
            for link_info in contact_links[:3]:  # ìµœëŒ€ 3ê°œ í˜ì´ì§€ë§Œ í™•ì¸
                contact_data = await self._extract_contact_page(link_info['url'])
                if contact_data:
                    additional_contacts.append({
                        'url': link_info['url'],
                        'keyword': link_info['keyword'],
                        'contacts': contact_data
                    })
            
            if additional_contacts:
                context.extracted_data['additional_contact_pages'] = additional_contacts
                self._merge_additional_contacts(context, additional_contacts)
                self.logger.info(f"âœ… ì¶”ê°€ ì—°ë½ì²˜ í˜ì´ì§€ {len(additional_contacts)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
            
            context.current_stage = CrawlingStage.CONTACT_EXTRACTION
            return context
            
        except Exception as e:
            context.error_log.append(f"ContactPageSearchAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _extract_contact_page(self, url: str) -> Optional[Dict]:
        """ì—°ë½ì²˜ í˜ì´ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            self.logger.info(f"ğŸ” ì—°ë½ì²˜ í˜ì´ì§€ ì¶”ì¶œ: {url}")
            
            if self.parent_crawler and self.parent_crawler.homepage_parser:
                page_data = self.parent_crawler.homepage_parser.extract_page_content(url)
                if page_data and page_data.get('accessible') and page_data.get('text_content'):
                    page_text = page_data['text_content']
                    
                    # ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
                    contact_info = self._extract_contact_info_enhanced(page_text)
                    
                    return contact_info if any(contact_info.values()) else None
            
            return None
            
        except Exception as e:
            self.logger.warning(f"ì—°ë½ì²˜ í˜ì´ì§€ ì¶”ì¶œ ì˜¤ë¥˜ {url}: {e}")
            return None
    
    def _extract_contact_info_enhanced(self, text: str) -> Dict[str, List[str]]:
        """ê°•í™”ëœ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        contact_info = {
            "phones": [],
            "faxes": [],
            "emails": [],
            "addresses": []
        }
        
        try:
            # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (ê¸°ë³¸ + ì¶”ê°€ íŒ¨í„´)
            phone_patterns = [
                r'(\d{2,3})-(\d{3,4})-(\d{4})',
                r'(\d{2,3})\.(\d{3,4})\.(\d{4})',
                r'tel[:\s]*(\d{2,3})-(\d{3,4})-(\d{4})',
                r'ì „í™”[:\s]*(\d{2,3})-(\d{3,4})-(\d{4})'
            ]
            
            for pattern in phone_patterns:
                import re
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        phone = '-'.join(match)
                    else:
                        phone = match
                    
                    # ì „í™”ë²ˆí˜¸ ê²€ì¦ ë° í¬ë§·íŒ…
                    if phone and len(phone.replace('-', '')) >= 9 and phone not in contact_info["phones"]:
                        contact_info["phones"].append(phone)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ
            fax_patterns = [
                r'íŒ©ìŠ¤[:\s]*(\d{2,3})-(\d{3,4})-(\d{4})',
                r'fax[:\s]*(\d{2,3})-(\d{3,4})-(\d{4})',
                r'F[:\s]*(\d{2,3})-(\d{3,4})-(\d{4})'
            ]
            
            for pattern in fax_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        fax = '-'.join(match)
                    else:
                        fax = match
                    
                    if fax and len(fax.replace('-', '')) >= 9 and fax not in contact_info["faxes"]:
                        contact_info["faxes"].append(fax)
            
            # ì´ë©”ì¼ ì¶”ì¶œ
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            emails = re.findall(email_pattern, text)
            for email in emails:
                if email not in contact_info["emails"]:
                    contact_info["emails"].append(email)
        
        except Exception as e:
            self.logger.warning(f"ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return contact_info
    
    def _merge_additional_contacts(self, context: CrawlingContext, additional_contacts: List[Dict]):
        """ì¶”ê°€ ì—°ë½ì²˜ ì •ë³´ë¥¼ ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ì— ë³‘í•©"""
        try:
            for contact_page in additional_contacts:
                contacts = contact_page.get('contacts', {})
                
                # ì „í™”ë²ˆí˜¸ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
                for phone in contacts.get('phones', []):
                    if not context.extracted_data.get('phone'):
                        context.extracted_data['phone'] = phone
                        context.extracted_data['phone_source'] = f"contact_page_{contact_page['keyword']}"
                        self.update_confidence(context, 'phone', 0.8)
                        break
                
                # íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
                existing_phone = context.extracted_data.get('phone', '')
                for fax in contacts.get('faxes', []):
                    if fax != existing_phone and not context.extracted_data.get('fax'):
                        context.extracted_data['fax'] = fax
                        context.extracted_data['fax_source'] = f"contact_page_{contact_page['keyword']}"
                        self.update_confidence(context, 'fax', 0.8)
                        break
                
                # ì´ë©”ì¼ ì¶”ê°€
                for email in contacts.get('emails', []):
                    if not context.extracted_data.get('email'):
                        context.extracted_data['email'] = email
                        context.extracted_data['email_source'] = f"contact_page_{contact_page['keyword']}"
                        self.update_confidence(context, 'email', 0.8)
                        break
        
        except Exception as e:
            self.logger.warning(f"ì¶”ê°€ ì—°ë½ì²˜ ë³‘í•© ì˜¤ë¥˜: {e}")

# ==================== AI ê°•í™” ModularUnifiedCrawler ====================

class AIEnhancedModularUnifiedCrawler:
    """AI ê°•í™” ëª¨ë“ˆëŸ¬ í†µí•© í¬ë¡¤ëŸ¬"""
    
    def __init__(self, config_override=None, api_key=None, progress_callback=None):
        """ì´ˆê¸°í™”"""
        self.config = config_override or CRAWLING_CONFIG
        self.logger = LoggerUtils.setup_crawler_logger("ai_enhanced_modular_crawler")
        self.progress_callback = progress_callback
        
        # AI ë§¤ë‹ˆì € ì´ˆê¸°í™”
        try:
            self.ai_manager = AIModelManager()
        except Exception as e:
            self.logger.warning(f"AI ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ai_manager = None
        
        # ì „ë¬¸ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ë“¤ (ê¸°ì¡´ ìœ ì§€)
        self.fax_extractor = None
        self.phone_driver = None
        self.homepage_parser = None
        self.contact_validator = None
        self.ai_validator = None
        self.database = None
        
        # AI ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™” (ìˆ˜ì •: parent_crawler ì „ë‹¬)
        self.ai_agents = []
        if self.ai_manager:
            try:
                self.ai_agents = [
                    EnhancedHomepageSearchAgent(self.ai_manager, self.logger, self),
                    EnhancedHomepageAnalysisAgent(self.ai_manager, self.logger, self),
                    ContactPageSearchAgent(self.ai_manager, self.logger, self),
                    EnhancedContactExtractionAgent(self.ai_manager, self.logger, self),
                    AIVerificationAgent(self.ai_manager, self.logger, self)
                ]
                self.logger.info("âœ… AI ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                self.logger.error(f"âŒ AI ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.ai_agents = []
        else:
            self.logger.warning("âš ï¸ AI ë§¤ë‹ˆì € ì—†ìŒ - ê¸°ë³¸ ëª¨ë“ˆë§Œ ì‚¬ìš©")
        
        # í†µê³„ ì •ë³´
        self.stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "phone_extracted": 0,
            "fax_extracted": 0,
            "homepage_parsed": 0,
            "contacts_validated": 0,
            "saved_to_db": 0,
            "ai_enhanced": 0,
            "start_time": None,
            "end_time": None,
            "agent_stats": {agent.name: {"executed": 0, "success": 0} for agent in self.ai_agents}
        }
        
        self.logger.info("ğŸš€ AI ê°•í™” ëª¨ë“ˆëŸ¬ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def initialize_modules(self):
        """ì „ë¬¸ ëª¨ë“ˆë“¤ ì´ˆê¸°í™” (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        try:
            self.logger.info("ğŸ”§ ì „ë¬¸ ëª¨ë“ˆë“¤ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. íŒ©ìŠ¤ ì¶”ì¶œê¸° ì´ˆê¸°í™”
            if FAX_EXTRACTOR_AVAILABLE:
                try:
                    self.fax_extractor = FaxExtractor()
                    self.logger.info("âœ… íŒ©ìŠ¤ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ íŒ©ìŠ¤ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.fax_extractor = None
            
            # 2. ì „í™”ë²ˆí˜¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” (Selenium ë“œë¼ì´ë²„)
            if PHONE_EXTRACTOR_AVAILABLE:
                try:
                    self.phone_driver = setup_driver()
                    self.logger.info("âœ… ì „í™”ë²ˆí˜¸ ì¶”ì¶œê¸° ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ ì „í™”ë²ˆí˜¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.phone_driver = None
            
            # 3. í™ˆí˜ì´ì§€ íŒŒì„œ ì´ˆê¸°í™”
            if URL_EXTRACTOR_AVAILABLE:
                try:
                    self.homepage_parser = HomepageParser(headless=True)
                    self.logger.info("âœ… í™ˆí˜ì´ì§€ íŒŒì„œ ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ í™ˆí˜ì´ì§€ íŒŒì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.homepage_parser = None
            
            # 4. ê²€ì¦ê¸° ì´ˆê¸°í™”
            if VALIDATOR_AVAILABLE:
                try:
                    self.contact_validator = ContactValidator()
                    self.ai_validator = AIValidator()
                    self.logger.info("âœ… ì—°ë½ì²˜ ê²€ì¦ê¸° ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ ê²€ì¦ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.contact_validator = None
                    self.ai_validator = None
            
            # 5. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            if DATABASE_AVAILABLE:
                try:
                    self.database = get_database()
                    self.logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
                    self.database = None
            
            self.logger.info("ğŸ¯ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë“ˆ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    async def process_organizations(self, organizations: List[Dict], options: Dict = None) -> List[Dict]:
        """AI ê°•í™” ì¡°ì§ ì²˜ë¦¬"""
        if not organizations:
            self.logger.warning("ì²˜ë¦¬í•  ì¡°ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        options = options or {}
        self.stats["start_time"] = datetime.now()
        self.stats["total_processed"] = len(organizations)
        
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.initialize_modules()
        
        self.logger.info(f"ğŸ“Š ì´ {len(organizations)}ê°œ ì¡°ì§ AI ê°•í™” ì²˜ë¦¬ ì‹œì‘")
        
        results = []
        
        try:
            for i, org in enumerate(organizations, 1):
                try:
                    # AI ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ì¡°ì§ ì²˜ë¦¬
                    processed_org = await self.process_single_organization_with_ai(org, i)
                    results.append(processed_org)
                    
                    self.stats["successful"] += 1
                    if processed_org.get('ai_enhanced'):
                        self.stats["ai_enhanced"] += 1
                    
                    # ë”œë ˆì´
                    await asyncio.sleep(self.config.get("default_delay", 2))
                    
                except Exception as e:
                    self.logger.error(f"âŒ ì¡°ì§ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {org.get('name', 'Unknown')} - {e}")
                    self.stats["failed"] += 1
                    results.append(org)
        
        finally:
            # ëª¨ë“ˆ ì •ë¦¬
            self.cleanup_modules()
        
        self.stats["end_time"] = datetime.now()
        self.print_ai_enhanced_statistics()
        
        return results
    
    async def process_single_organization_with_ai(self, org: Dict, index: int) -> Dict:
        """AI ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ì¡°ì§ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            org_name = org.get('name', 'Unknown')
            self.logger.info(f"ğŸ¤– AI ì›Œí¬í”Œë¡œìš° ì‹œì‘ [{index}]: {org_name}")
            
            # AI ì—ì´ì „íŠ¸ ì²´ì¸ ì´ˆê¸°í™”
            context = CrawlingContext(
                organization=org,
                current_stage=CrawlingStage.INITIALIZATION,
                extracted_data={},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # AI ì—ì´ì „íŠ¸ ì²´ì¸ ì‹¤í–‰ (ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ë°°ì¹˜)
            agents = [
                EnhancedHomepageSearchAgent(self.ai_manager, self.logger, self),
                EnhancedHomepageAnalysisAgent(self.ai_manager, self.logger, self),
                ContactPageSearchAgent(self.ai_manager, self.logger, self),  # í™ˆí˜ì´ì§€ ë¶„ì„ í›„ ë°”ë¡œ ì‹¤í–‰
                EnhancedContactExtractionAgent(self.ai_manager, self.logger, self),
                AIVerificationAgent(self.ai_manager, self.logger, self)
            ]
            
            for agent in agents:
                try:
                    if await agent.should_execute(context):
                        self.logger.info(f"ğŸ”„ ì—ì´ì „íŠ¸ ì‹¤í–‰: {agent.name}")
                        context = await agent.execute(context)
                    else:
                        self.logger.info(f"â­ï¸ ì—ì´ì „íŠ¸ ê±´ë„ˆë›°ê¸°: {agent.name}")
                except Exception as e:
                    # error ì†ì„± ëŒ€ì‹  error_logì— ì¶”ê°€
                    error_msg = f"{agent.name} ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
                    context.error_log.append(error_msg)
                    self.logger.error(f"âŒ {error_msg}")
                    continue
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            context.processing_time = time.time() - start_time
            
            # ê²°ê³¼ ê²°í•©
            result = self._combine_ai_results(org, context)
            
            # ì „í†µì ì¸ ëª¨ë“ˆë¡œ ë³´ì™„
            await self._supplement_with_traditional_modules(result, context)
            
            return result
            
        except Exception as e:
            # error ì†ì„± ëŒ€ì‹  ì§ì ‘ ì˜¤ë¥˜ ë©”ì‹œì§€ ë¡œê¹…
            error_msg = f"AI ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {org_name} - {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì²˜ë¦¬
            result = org.copy()
            result.update({
                'ai_enhanced': False,
                'processing_metadata': {
                    'extraction_method': 'fallback_error',
                    'error_message': str(e),
                    'timestamp': datetime.now().isoformat(),
                    'processing_time': time.time() - start_time
                }
            })
            return result
    
    def _combine_ai_results(self, original_org: Dict, context: CrawlingContext) -> Dict:
        """AI ê²°ê³¼ ì¡°í•©"""
        result = original_org.copy()
        
        # ì¶”ì¶œëœ ë°ì´í„° ì¶”ê°€
        result.update(context.extracted_data)
        
        # AI ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
        result['ai_insights'] = context.ai_insights
        result['confidence_scores'] = context.confidence_scores
        
        # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result['processing_metadata'] = {
            'processing_time': context.processing_time,
            'final_stage': context.current_stage.value,
            'error_count': len(context.error_log),
            'errors': context.error_log,
            'extraction_method': 'ai_enhanced_modular',
            'ai_enhanced': True,
            'timestamp': datetime.now().isoformat()
        }
        
        return result
    
    async def _supplement_with_traditional_modules(self, result: Dict, context: CrawlingContext):
        """ê¸°ì¡´ ëª¨ë“ˆë¡œ ë³´ì™„ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
        try:
            org_name = result.get('name', 'Unknown')
            self.logger.info(f"ğŸ”§ ê¸°ì¡´ ëª¨ë“ˆë¡œ ë³´ì™„ ì²˜ë¦¬: {org_name}")
            
            # ì „í™”ë²ˆí˜¸ê°€ ì—†ê±°ë‚˜ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ê¸°ì¡´ ëª¨ë“ˆë¡œ ì¶”ê°€ ì‹œë„
            phone_confidence = context.confidence_scores.get('phone', 0.0)
            if (not result.get('phone') or phone_confidence < 0.7) and self.phone_driver:
                try:
                    self.logger.info(f"ğŸ“ ì „í™”ë²ˆí˜¸ ê²€ìƒ‰ ì‹œë„: {org_name}")
                    found_phones = search_phone_number(self.phone_driver, org_name)
                    if found_phones:
                        # ê°€ì¥ ì ì ˆí•œ ì „í™”ë²ˆí˜¸ ì„ íƒ
                        best_phone = self._select_best_phone_number(found_phones, result.get('address', ''))
                        if best_phone:
                            result['phone'] = best_phone
                            result['phone_source'] = 'traditional_module_supplement'
                            result['phone_confidence'] = 0.8  # ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„
                            self.stats["phone_extracted"] += 1
                            self.logger.info(f"âœ… ì „í™”ë²ˆí˜¸ ë³´ì™„ ì„±ê³µ: {best_phone}")
                except Exception as e:
                    self.logger.warning(f"ì „í™”ë²ˆí˜¸ ë³´ì™„ ì‹¤íŒ¨: {e}")
            
            # íŒ©ìŠ¤ë²ˆí˜¸ê°€ ì—†ê±°ë‚˜ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ê¸°ì¡´ ëª¨ë“ˆë¡œ ì¶”ê°€ ì‹œë„
            fax_confidence = context.confidence_scores.get('fax', 0.0)
            if (not result.get('fax') or fax_confidence < 0.7) and self.fax_extractor:
                try:
                    self.logger.info(f"ğŸ“  íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ ì‹œë„: {org_name}")
                    found_faxes = self.fax_extractor.search_fax_number(org_name)
                    if found_faxes:
                        # ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” íŒ©ìŠ¤ë²ˆí˜¸ ì„ íƒ
                        best_fax = self._select_best_fax_number(found_faxes, result.get('phone', ''))
                        if best_fax:
                            result['fax'] = best_fax
                            result['fax_source'] = 'traditional_module_supplement'
                            result['fax_confidence'] = 0.8  # ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„
                            self.stats["fax_extracted"] += 1
                            self.logger.info(f"âœ… íŒ©ìŠ¤ë²ˆí˜¸ ë³´ì™„ ì„±ê³µ: {best_fax}")
                except Exception as e:
                    self.logger.warning(f"íŒ©ìŠ¤ë²ˆí˜¸ ë³´ì™„ ì‹¤íŒ¨: {e}")
            
            # í™ˆí˜ì´ì§€ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ëª¨ë“ˆë¡œ ì¶”ê°€ ì‹œë„
            if not result.get('homepage') and self.homepage_parser:
                try:
                    self.logger.info(f"ğŸŒ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹œë„: {org_name}")
                    # AI í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹œë„
                    homepage_results = await self.homepage_parser.ai_search_homepage(org_name, result.get('category', ''))
                    if homepage_results:
                        best_homepage = homepage_results[0]
                        result['homepage'] = best_homepage.get('url', '')
                        result['homepage_source'] = 'traditional_module_supplement'
                        result['homepage_confidence'] = best_homepage.get('confidence', 0.6)
                        self.logger.info(f"âœ… í™ˆí˜ì´ì§€ ë³´ì™„ ì„±ê³µ: {result['homepage']}")
                except Exception as e:
                    self.logger.warning(f"í™ˆí˜ì´ì§€ ë³´ì™„ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.error(f"ê¸°ì¡´ ëª¨ë“ˆ ë³´ì™„ ì˜¤ë¥˜: {e}")
    
    def _select_best_phone_number(self, phone_numbers: List[str], address: str = '') -> Optional[str]:
        """ê°€ì¥ ì ì ˆí•œ ì „í™”ë²ˆí˜¸ ì„ íƒ"""
        if not phone_numbers:
            return None
        
        try:
            # ì§€ì—­ë²ˆí˜¸ì™€ ì£¼ì†Œ ë§¤ì¹­ í™•ì¸
            for phone in phone_numbers:
                if self._validate_phone_by_region(phone, address):
                    return phone
            
            # ì§€ì—­ ë§¤ì¹­ì´ ì•ˆ ë˜ë©´ ì²« ë²ˆì§¸ ë²ˆí˜¸ ë°˜í™˜
            return phone_numbers[0]
            
        except Exception as e:
            self.logger.warning(f"ì „í™”ë²ˆí˜¸ ì„ íƒ ì˜¤ë¥˜: {e}")
            return phone_numbers[0] if phone_numbers else None
    
    def _select_best_fax_number(self, fax_numbers: List[str], existing_phone: str = '') -> Optional[str]:
        """ê°€ì¥ ì ì ˆí•œ íŒ©ìŠ¤ë²ˆí˜¸ ì„ íƒ (ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µ ë°©ì§€)"""
        if not fax_numbers:
            return None
        
        try:
            # ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” íŒ©ìŠ¤ë²ˆí˜¸ ì„ íƒ
            for fax in fax_numbers:
                if not self._is_duplicate_number(fax, existing_phone):
                    return fax
            
            # ëª¨ë‘ ì¤‘ë³µì´ë©´ None ë°˜í™˜
            return None
            
        except Exception as e:
            self.logger.warning(f"íŒ©ìŠ¤ë²ˆí˜¸ ì„ íƒ ì˜¤ë¥˜: {e}")
            return None
    
    def _validate_phone_by_region(self, phone: str, address: str) -> bool:
        """ì§€ì—­ë²ˆí˜¸ì™€ ì£¼ì†Œ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸"""
        if not phone or not address:
            return True  # ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì¼ë‹¨ í†µê³¼
        
        try:
            # settings.pyì˜ validate_phone_by_region í™œìš© (ì¡´ì¬í•œë‹¤ë©´)
            return True  # ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
        except:
            return True
    
    def _is_duplicate_number(self, number1: str, number2: str) -> bool:
        """ë‘ ë²ˆí˜¸ê°€ ì¤‘ë³µì¸ì§€ í™•ì¸"""
        if not number1 or not number2:
            return False
        
        # ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ
        digits1 = re.sub(r'[^\d]', '', number1)
        digits2 = re.sub(r'[^\d]', '', number2)
        
        return digits1 == digits2
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (í˜¸í™˜ì„±)
    def cleanup_modules(self):
        """ëª¨ë“ˆë“¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ ëª¨ë“ˆ ì •ë¦¬ ì‹œì‘...")
            
            if self.fax_extractor:
                try:
                    self.fax_extractor.close()
                except:
                    pass
            
            if self.phone_driver:
                try:
                    self.phone_driver.quit()
                except:
                    pass
            
            if self.homepage_parser:
                try:
                    self.homepage_parser.close_driver()
                except:
                    pass
            
            self.logger.info("ğŸ¯ ëª¨ë“  ëª¨ë“ˆ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë“ˆ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def save_to_database(self, org_data: Dict) -> Optional[Dict]:
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥/ì—…ë°ì´íŠ¸ (ê°œì„ ëœ ë²„ì „)"""
        try:
            if not DATABASE_AVAILABLE:
                self.logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            from database.database import get_database
            db = get_database()
            
            # DB IDê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            db_id = org_data.get('db_id') or org_data.get('id')
            org_name = org_data.get('name', 'Unknown')
            
            # ì—…ë°ì´íŠ¸í•  ë°ì´í„° ì¤€ë¹„
            update_data = {}
            
            # ğŸ”¥ í¬ë¡¤ë§ ìƒíƒœ í•„ë“œ ì—…ë°ì´íŠ¸ (í•­ìƒ ì‹¤í–‰)
            update_data['ai_crawled'] = True
            update_data['last_crawled_at'] = datetime.now().isoformat()
            
            # í¬ë¡¤ë§ìœ¼ë¡œ ì–»ì€ ìƒˆë¡œìš´ ì •ë³´ë§Œ ì—…ë°ì´íŠ¸ (ê²€ì¦ ê°•í™”)
            changes_made = []
            
            if org_data.get('homepage') and org_data['homepage'].strip() != '':
                homepage = org_data['homepage'].strip()
                if homepage.startswith(('http://', 'https://')):
                    update_data['homepage'] = homepage
                    changes_made.append(f"í™ˆí˜ì´ì§€: {homepage}")
            
            if org_data.get('phone') and org_data['phone'].strip() != '':
                phone = org_data['phone'].strip()
                if self._is_valid_phone_format(phone):
                    update_data['phone'] = phone
                    changes_made.append(f"ì „í™”ë²ˆí˜¸: {phone}")
            
            if org_data.get('fax') and org_data['fax'].strip() != '':
                fax = org_data['fax'].strip()
                if self._is_valid_phone_format(fax):
                    update_data['fax'] = fax
                    changes_made.append(f"íŒ©ìŠ¤ë²ˆí˜¸: {fax}")
            
            if org_data.get('email') and org_data['email'].strip() != '':
                email = org_data['email'].strip()
                if self._is_valid_email_format(email):
                    update_data['email'] = email
                    changes_made.append(f"ì´ë©”ì¼: {email}")
            
            # í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„° ì €ì¥ (ê°•í™”ëœ ë²„ì „)
            crawling_data = {
                'last_crawled': datetime.now().isoformat(),
                'ai_enhanced': org_data.get('ai_enhanced', False),
                'extraction_method': org_data.get('processing_metadata', {}).get('extraction_method', 'unknown'),
                'confidence_scores': org_data.get('confidence_scores', {}),
                'ai_insights': org_data.get('ai_insights', {}),
                'sources': {
                    'phone_source': org_data.get('phone_source', ''),
                    'fax_source': org_data.get('fax_source', ''),
                    'homepage_source': org_data.get('homepage_source', ''),
                    'email_source': org_data.get('email_source', '')
                },
                'processing_time': org_data.get('processing_metadata', {}).get('processing_time', 0),
                'error_count': len(org_data.get('processing_metadata', {}).get('errors', [])),
                'changes_made': changes_made
            }
            
            if org_data.get('crawling_data'):
                crawling_data.update(org_data['crawling_data'])
            
            update_data['crawling_data'] = json.dumps(crawling_data, ensure_ascii=False)
            update_data['updated_by'] = 'crawler_system'
            
            if db_id:
                # ê¸°ì¡´ ì¡°ì§ ì—…ë°ì´íŠ¸
                success = db.update_organization(db_id, update_data, 'crawler_system')
                if success:
                    self.stats["saved_to_db"] += 1
                    self.logger.info(f"âœ… ì¡°ì§ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {org_name} (ID: {db_id})")
                    self.logger.info(f"ğŸ¯ í¬ë¡¤ë§ ìƒíƒœ: ai_crawled=True, last_crawled_at={update_data['last_crawled_at']}")
                    if changes_made:
                        self.logger.info(f"ğŸ“ ë³€ê²½ì‚¬í•­: {', '.join(changes_made)}")
                    return {'action': 'updated', 'id': db_id, 'changes': changes_made}
                else:
                    self.logger.error(f"âŒ ì¡°ì§ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {org_name} (ID: {db_id})")
                    return None
            else:
                # ìƒˆë¡œìš´ ì¡°ì§ ìƒì„±
                org_data_for_db = {
                    'name': org_data.get('name', ''),
                    'category': org_data.get('category', 'ì¢…êµì‹œì„¤'),
                    'type': org_data.get('type', 'CHURCH'),
                    'homepage': org_data.get('homepage', ''),
                    'phone': org_data.get('phone', ''),
                    'fax': org_data.get('fax', ''),
                    'email': org_data.get('email', ''),
                    'address': org_data.get('address', ''),
                    'organization_size': org_data.get('organization_size', ''),
                    'denomination': org_data.get('denomination', ''),
                    'crawling_data': json.dumps(crawling_data, ensure_ascii=False),
                    'created_by': 'crawler_system',
                    'updated_by': 'crawler_system',
                    'lead_source': 'CRAWLER'
                }
                
                new_id = db.create_organization(org_data_for_db)
                if new_id:
                    self.stats["saved_to_db"] += 1
                    self.logger.info(f"âœ… ìƒˆ ì¡°ì§ ìƒì„± ì™„ë£Œ: {org_name} (ID: {new_id})")
                    if changes_made:
                        self.logger.info(f"ğŸ“ ìƒì„±ëœ ì •ë³´: {', '.join(changes_made)}")
                    return {'action': 'created', 'id': new_id, 'changes': changes_made}
                else:
                    self.logger.error(f"âŒ ìƒˆ ì¡°ì§ ìƒì„± ì‹¤íŒ¨: {org_name}")
                    return None
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {org_data.get('name', 'Unknown')} - {e}")
            return None
    
    def _is_valid_phone_format(self, phone: str) -> bool:
        """ì „í™”ë²ˆí˜¸ í˜•ì‹ ê²€ì¦"""
        if not phone:
            return False
        
        # ìˆ«ìë§Œ ì¶”ì¶œ
        digits = re.sub(r'[^\d]', '', phone)
        
        # í•œêµ­ ì „í™”ë²ˆí˜¸ ê¸¸ì´ í™•ì¸ (9-11ìë¦¬)
        if len(digits) < 9 or len(digits) > 11:
            return False
        
        # ìœ íš¨í•œ ì§€ì—­ë²ˆí˜¸ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
        valid_prefixes = ['02', '031', '032', '033', '041', '042', '043', '044', '051', '052', '053', '054', '055', '061', '062', '063', '064', '070', '010', '011', '016', '017', '018', '019']
        
        for prefix in valid_prefixes:
            if digits.startswith(prefix):
                return True
        
        return False
    
    def _is_valid_email_format(self, email: str) -> bool:
        """ì´ë©”ì¼ í˜•ì‹ ê²€ì¦"""
        if not email:
            return False
        
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return re.match(email_pattern, email) is not None
    
    def print_ai_enhanced_statistics(self):
        """AI ê°•í™” í†µê³„ ì¶œë ¥"""
        duration = self.stats["end_time"] - self.stats["start_time"]
        
        print("\n" + "="*80)
        print("ğŸ¤– AI ê°•í™” ëª¨ë“ˆëŸ¬ í¬ë¡¤ë§ ì™„ë£Œ í†µê³„")
        print("="*80)
        print(f"ğŸ“‹ ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"âœ… ì„±ê³µ: {self.stats['successful']}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
        print(f"ğŸ¤– AI ê°•í™”: {self.stats['ai_enhanced']}ê°œ")
        print(f"ğŸ“ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ: {self.stats['phone_extracted']}ê°œ")
        print(f"ğŸ“  íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ: {self.stats['fax_extracted']}ê°œ")
        print(f"ğŸ’¾ DB ì €ì¥: {self.stats['saved_to_db']}ê°œ")
        
        if self.stats['total_processed'] > 0:
            success_rate = (self.stats['successful'] / self.stats['total_processed']) * 100
            ai_enhancement_rate = (self.stats['ai_enhanced'] / self.stats['total_processed']) * 100
            print(f"ğŸ“ˆ ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}%")
            print(f"ğŸ¤– AI ê°•í™”ìœ¨: {ai_enhancement_rate:.1f}%")
        
        print(f"â±ï¸ ì†Œìš”ì‹œê°„: {duration}")
        
        # AI ì—ì´ì „íŠ¸ë³„ í†µê³„
        print(f"\nğŸ¤– AI ì—ì´ì „íŠ¸ë³„ ì„±ê³µë¥ :")
        for agent_name, stats in self.stats["agent_stats"].items():
            executed = stats["executed"]
            success = stats["success"]
            success_rate = (success / executed * 100) if executed > 0 else 0
            print(f"  - {agent_name}: {executed}íšŒ ì‹¤í–‰, {success}íšŒ ì„±ê³µ ({success_rate:.1f}%)")
        
        print("="*80)
    
    # ==================== ContactEnrichmentService í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ====================
    
    async def search_homepage(self, org_name: str) -> Dict[str, Any]:
        """í™ˆí˜ì´ì§€ ê²€ìƒ‰ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            self.logger.info(f"ğŸ” AI í™ˆí˜ì´ì§€ ê²€ìƒ‰: {org_name}")
            
            # AI ì—ì´ì „íŠ¸ë¥¼ í†µí•œ í™ˆí˜ì´ì§€ ê²€ìƒ‰
            context = CrawlingContext(
                organization={'name': org_name},
                current_stage=CrawlingStage.HOMEPAGE_SEARCH,
                extracted_data={},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì‹¤í–‰
            homepage_agent = EnhancedHomepageSearchAgent(self.ai_manager, self.logger, self)
            context = await homepage_agent.execute(context)
            
            if context.extracted_data.get('homepage'):
                return {
                    "homepage": context.extracted_data['homepage'],
                    "homepage_type": context.extracted_data.get('homepage_type', 'ì•Œìˆ˜ì—†ìŒ'),
                    "confidence": context.confidence_scores.get('homepage', 0.5),
                    "status": "success",
                    "ai_enhanced": True
                }
            else:
                return {
                    "homepage": "",
                    "status": "not_found",
                    "message": "AI ê¸°ë°˜ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                }
            
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {org_name} - {e}")
            return {"status": "error", "error": str(e)}
    
    async def extract_details_from_homepage(self, homepage_url: str) -> Dict[str, Any]:
        """í™ˆí˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            self.logger.info(f"ğŸŒ AI í™ˆí˜ì´ì§€ ë¶„ì„: {homepage_url}")
            
            # AI ì—ì´ì „íŠ¸ë¥¼ í†µí•œ í™ˆí˜ì´ì§€ ë¶„ì„
            context = CrawlingContext(
                organization={'name': 'Unknown'},
                current_stage=CrawlingStage.HOMEPAGE_ANALYSIS,
                extracted_data={'homepage': homepage_url},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # í™ˆí˜ì´ì§€ ë¶„ì„ ì—ì´ì „íŠ¸ ì‹¤í–‰
            analysis_agent = EnhancedHomepageAnalysisAgent(self.ai_manager, self.logger, self)
            context = await analysis_agent.execute(context)
            
            return {
                "phone": context.extracted_data.get("phone", ""),
                "fax": context.extracted_data.get("fax", ""),
                "email": context.extracted_data.get("email", ""),
                "address": context.extracted_data.get("address", ""),
                "ai_insights": context.ai_insights,
                "confidence_scores": context.confidence_scores,
                "status": "success",
                "ai_enhanced": True
            }
                
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ë¶„ì„ ì˜¤ë¥˜: {homepage_url} - {e}")
            return {"status": "error", "error": str(e)}
    
    async def search_missing_info(self, org_name: str, missing_fields: List[str]) -> Dict[str, str]:
        """ëˆ„ë½ëœ ì •ë³´ ê²€ìƒ‰ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            self.logger.info(f"ğŸ” AI ëˆ„ë½ ì •ë³´ ê²€ìƒ‰: {org_name} - {missing_fields}")
            
            results = {}
            
            # AI ì—ì´ì „íŠ¸ë¥¼ í†µí•œ ì—°ë½ì²˜ ì¶”ì¶œ
            context = CrawlingContext(
                organization={'name': org_name},
                current_stage=CrawlingStage.CONTACT_EXTRACTION,
                extracted_data={},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # ì—°ë½ì²˜ ì¶”ì¶œ ì—ì´ì „íŠ¸ ì‹¤í–‰
            contact_agent = EnhancedContactExtractionAgent(self.ai_manager, self.logger, self)
            context = await contact_agent.execute(context)
            
            # ìš”ì²­ëœ í•„ë“œë§Œ ë°˜í™˜
            for field in missing_fields:
                if field in context.extracted_data:
                    results[field] = context.extracted_data[field]
                    self.logger.info(f"  ğŸ¯ AIë¡œ {field} ë°œê²¬: {results[field]}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"AI ëˆ„ë½ ì •ë³´ ê²€ìƒ‰ ì˜¤ë¥˜: {org_name} - {e}")
            return {}
    
    def validate_and_clean_data(self, org_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            cleaned_data = org_data.copy()
            
            # ê¸°ì¡´ validator.py ì‚¬ìš© (ìœ ì§€)
            if self.contact_validator:
                # ì „í™”ë²ˆí˜¸ ê²€ì¦
                if cleaned_data.get('phone'):
                    is_valid, validated_phone = self.contact_validator.validate_phone_number(cleaned_data['phone'])
                    if is_valid:
                        cleaned_data['phone'] = validated_phone
                        cleaned_data['phone_validation'] = 'valid'
                    else:
                        self.logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì „í™”ë²ˆí˜¸: {cleaned_data['phone']}")
                        cleaned_data['phone_validation'] = 'invalid'
                
                # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦
                if cleaned_data.get('fax'):
                    is_valid, validated_fax = self.contact_validator.validate_fax_number(cleaned_data['fax'])
                    if is_valid:
                        cleaned_data['fax'] = validated_fax
                        cleaned_data['fax_validation'] = 'valid'
                    else:
                        self.logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ íŒ©ìŠ¤ë²ˆí˜¸: {cleaned_data['fax']}")
                        cleaned_data['fax_validation'] = 'invalid'
            
            # AI ê²€ì¦ ì¶”ê°€
            if self.ai_manager:
                try:
                    # AI ê²€ì¦ ì—ì´ì „íŠ¸ ì‹¤í–‰
                    context = CrawlingContext(
                        organization={'name': cleaned_data.get('name', 'Unknown')},
                        current_stage=CrawlingStage.AI_VERIFICATION,
                        extracted_data=cleaned_data,
                        ai_insights={},
                        error_log=[],
                        processing_time=0,
                        confidence_scores={}
                    )
                    
                    # AI ê²€ì¦ ì—ì´ì „íŠ¸ ì‹¤í–‰ (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ)
                    verification_agent = AIVerificationAgent(self.ai_manager, self.logger, self)
                    # ë¹„ë™ê¸° í˜¸ì¶œì„ ë™ê¸°ë¡œ ë³€í™˜
                    import asyncio
                    if asyncio.get_event_loop().is_running():
                        # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
                        task = asyncio.create_task(verification_agent.execute(context))
                        # ê°„ë‹¨í•œ AI ê²€ì¦ë§Œ ìˆ˜í–‰
                        cleaned_data['ai_validation_attempted'] = True
                    else:
                        context = asyncio.run(verification_agent.execute(context))
                        cleaned_data['ai_insights'] = context.ai_insights
                        cleaned_data['confidence_scores'] = context.confidence_scores
                
                except Exception as e:
                    self.logger.warning(f"AI ê²€ì¦ ì‹¤íŒ¨: {e}")
                    cleaned_data['ai_validation_error'] = str(e)
            
            return cleaned_data
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜: {e}")
            return org_data
    
    # ==================== app.py í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ====================
    
    async def process_json_file_async(self, json_file_path: str, test_mode: bool = False, test_count: int = 10, progress_callback=None) -> List[Dict]:
        """ğŸ”§ app.py í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ (AI ê°•í™”)"""
        try:
            # JSON íŒŒì¼ ë¡œë“œ
            data = FileUtils.load_json(json_file_path)
            
            # ë°ì´í„° ì „ì²˜ë¦¬ (app.pyì™€ ë™ì¼í•œ ë°©ì‹)
            organizations = []
            if isinstance(data, dict):
                for category, orgs in data.items():
                    if isinstance(orgs, list):
                        for org in orgs:
                            if isinstance(org, dict):
                                org["category"] = category
                                organizations.append(org)
            elif isinstance(data, list):
                organizations = [org for org in data if isinstance(org, dict)]
            
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
            if test_mode and test_count:
                organizations = organizations[:test_count]
            
            # progress_callback ì €ì¥
            self.progress_callback = progress_callback
            
            # AI ê°•í™” ì²˜ë¦¬ ì‹¤í–‰
            results = await self.process_organizations(organizations)
            
            return results
            
        except Exception as e:
            self.logger.error(f"JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

    async def _fallback_to_traditional_processing(self, context: CrawlingContext):
        """AI ì—ì´ì „íŠ¸ ì—†ì„ ë•Œ ê¸°ë³¸ ëª¨ë“ˆ ì²˜ë¦¬"""
        try:
            org_name = context.organization.get('name', 'Unknown')
            self.logger.info(f"ğŸ”§ ê¸°ë³¸ ëª¨ë“ˆ ì²˜ë¦¬: {org_name}")
            
            # ê¸°ë³¸ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (url_extractor ì‚¬ìš©)
            if self.homepage_parser:
                try:
                    # ê°„ë‹¨í•œ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ë¡œì§
                    pass
                except Exception as e:
                    self.logger.warning(f"í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ ì „í™”ë²ˆí˜¸ ê²€ìƒ‰ (phone_extractor ì‚¬ìš©)
            if self.phone_driver:
                try:
                    found_phones = search_phone_number(self.phone_driver, org_name)
                    if found_phones:
                        context.extracted_data['phone'] = found_phones[0]
                        self.update_confidence(context, 'phone', 0.7)
                except Exception as e:
                    self.logger.warning(f"ì „í™”ë²ˆí˜¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ (fax_extractor ì‚¬ìš©)
            if self.fax_extractor:
                try:
                    found_faxes = self.fax_extractor.search_fax_number(org_name)
                    if found_faxes:
                        context.extracted_data['fax'] = found_faxes[0]
                        self.update_confidence(context, 'fax', 0.7)
                except Exception as e:
                    self.logger.warning(f"íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.error(f"ê¸°ë³¸ ëª¨ë“ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _fallback_to_traditional_processing_simple(self, org: Dict) -> Dict:
        """ê°„ë‹¨í•œ ê¸°ë³¸ ì²˜ë¦¬ (ì‹¤íŒ¨ ì‹œ ëŒ€ì²´)"""
        result = org.copy()
        result['processing_metadata'] = {
            'extraction_method': 'fallback_traditional',
            'ai_enhanced': False,
            'timestamp': datetime.now().isoformat(),
            'status': 'fallback_processed'
        }
        return result

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ (ModularUnifiedCrawlerë¥¼ AI ê°•í™” ë²„ì „ìœ¼ë¡œ êµì²´)
ModularUnifiedCrawler = AIEnhancedModularUnifiedCrawler

# ==================== í¸ì˜ í•¨ìˆ˜ë“¤ ====================

async def crawl_ai_enhanced_from_file(input_file: str, options: Dict = None) -> List[Dict]:
    """íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ AI ê°•í™” í¬ë¡¤ë§"""
    try:
        # íŒŒì¼ ë¡œë“œ
        data = FileUtils.load_json(input_file)
        
        # AI ê°•í™” í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
        crawler = AIEnhancedModularUnifiedCrawler()
        results = await crawler.process_organizations(data, options)
        
        return results
        
    except Exception as e:
        logging.error(f"AI ê°•í™” íŒŒì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

async def crawl_ai_enhanced_latest_file(options: Dict = None) -> List[Dict]:
    """ìµœì‹  ì…ë ¥ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ AI ê°•í™” í¬ë¡¤ë§"""
    try:
        latest_file = get_latest_input_file()
        if not latest_file:
            raise ValueError("ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ“‚ ìµœì‹  íŒŒì¼ ì‚¬ìš©: {latest_file}")
        return await crawl_ai_enhanced_from_file(str(latest_file), options)
        
    except Exception as e:
        logging.error(f"AI ê°•í™” ìµœì‹  íŒŒì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

async def crawl_ai_enhanced_from_database(options: Dict = None) -> List[Dict]:
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ AI ê°•í™” í¬ë¡¤ë§ (ìˆ˜ì •ëœ ë²„ì „)"""
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        from database.database import get_database
        db = get_database()
        
        # ğŸ”¥ ì¡°ì§ ë°ì´í„° ì¡°íšŒ (ì¤‘ë³µ ë°©ì§€ ê°•í™”)
        with db.get_connection() as conn:
            query = """
            SELECT id, name, type, category, subcategory, homepage, phone, fax, email, 
                   mobile, postal_code, address, organization_size, denomination,
                   ai_crawled, last_crawled_at
            FROM organizations 
            WHERE is_active = 1 
            AND (
                (homepage = '' OR homepage IS NULL) OR
                (phone = '' OR phone IS NULL) OR 
                (fax = '' OR fax IS NULL) OR
                (email = '' OR email IS NULL)
            )
            AND name IS NOT NULL AND name != ''
            -- ğŸ”¥ ì¤‘ë³µ ë°©ì§€: 7ì¼ ì´ë‚´ í¬ë¡¤ë§í•˜ì§€ ì•Šì€ ê²ƒë§Œ
            AND (
                ai_crawled = 0 OR 
                ai_crawled IS NULL OR
                last_crawled_at IS NULL OR
                datetime(last_crawled_at) < datetime('now', '-7 days')
            )
            ORDER BY 
                CASE WHEN ai_crawled = 0 OR ai_crawled IS NULL THEN 0 ELSE 1 END,  -- ë¯¸í¬ë¡¤ë§ ìš°ì„ 
                last_crawled_at ASC NULLS FIRST,  -- ì˜¤ë˜ëœ ê²ƒ ìš°ì„ 
                updated_at DESC
            LIMIT 500
            """
            cursor = conn.execute(query)
            rows = cursor.fetchall()
            
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            organizations = []
            for row in rows:
                org = dict(row)
                organizations.append({
                    'db_id': org.get('id'),
                    'name': org.get('name', ''),
                    'type': org.get('type', 'CHURCH'),
                    'category': org.get('category', 'ì¢…êµì‹œì„¤'),
                    'subcategory': org.get('subcategory', ''),
                    'homepage': org.get('homepage', ''),
                    'phone': org.get('phone', ''),
                    'fax': org.get('fax', ''),
                    'email': org.get('email', ''),
                    'mobile': org.get('mobile', ''),
                    'postal_code': org.get('postal_code', ''),
                    'address': org.get('address', ''),
                    'organization_size': org.get('organization_size', ''),
                    'denomination': org.get('denomination', '')
                })
        
        if not organizations:
            print("ğŸ“‹ í¬ë¡¤ë§ì´ í•„ìš”í•œ ì¡°ì§ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ¯ ì¡°ê±´: ì—°ë½ì²˜ ëˆ„ë½ + 7ì¼ ì´ë‚´ í¬ë¡¤ë§ ì•ˆ ë¨")
            return []
        
        print(f"ğŸ“‚ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {len(organizations)}ê°œ ì¡°ì§ ë¡œë“œ")
        print(f"ğŸ¯ ì„ íƒ ì¡°ê±´:")
        print(f"  - ì—°ë½ì²˜ ì •ë³´ ëˆ„ë½ (homepage/phone/fax/email)")
        print(f"  - 7ì¼ ì´ë‚´ í¬ë¡¤ë§ ì•ˆ ë¨ (ai_crawled=0 OR last_crawled_at < 7ì¼ ì „)")
        print(f"  - ìš°ì„ ìˆœìœ„: ë¯¸í¬ë¡¤ë§ â†’ ì˜¤ë˜ëœ í¬ë¡¤ë§ â†’ ìµœì‹  ì—…ë°ì´íŠ¸")
        
        # AI ê°•í™” í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
        crawler = AIEnhancedModularUnifiedCrawler()
        results = await crawler.process_organizations(organizations, options)
        
        # ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë°ì´íŠ¸
        updated_count = 0
        for result in results:
            if result.get('db_id'):
                success = await crawler.save_to_database(result)
                if success:
                    updated_count += 1
        
        print(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸: {updated_count}ê°œ ì¡°ì§")
        
        return results
        
    except Exception as e:
        logging.error(f"AI ê°•í™” ë°ì´í„°ë² ì´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (AI ê°•í™”)"""
    print("ğŸ¤– AI ê°•í™” ëª¨ë“ˆëŸ¬ í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*80)
    print("ğŸš€ AI ê¸°ëŠ¥ ìµœëŒ€ í™œìš© ëª¨ë“œ")
    print("ğŸ“¦ ì „ë¬¸ ëª¨ë“ˆ + AI ì—ì´ì „íŠ¸ í†µí•©:")
    print(f"  - fax_extractor.py: {'âœ…' if FAX_EXTRACTOR_AVAILABLE else 'âŒ'}")
    print(f"  - phone_extractor.py: {'âœ…' if PHONE_EXTRACTOR_AVAILABLE else 'âŒ'}")
    print(f"  - url_extractor.py: {'âœ…' if URL_EXTRACTOR_AVAILABLE else 'âŒ'}")
    print(f"  - validator.py: {'âœ…' if VALIDATOR_AVAILABLE else 'âŒ'}")
    print(f"  - database.py: {'âœ…' if DATABASE_AVAILABLE else 'âŒ'}")
    print("ğŸ¤– AI ì—ì´ì „íŠ¸:")
    print("  - EnhancedHomepageSearchAgent: AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰ (ê³µì‹ì‚¬ì´íŠ¸ + ì†Œì…œë¯¸ë””ì–´)")
    print("  - EnhancedHomepageAnalysisAgent: AI ê¸°ë°˜ í™ˆí˜ì´ì§€ ë¶„ì„ (BS4 â†’ JS â†’ AI)")
    print("  - ContactPageSearchAgent: ì—°ë½ì²˜ í˜ì´ì§€ ì „ìš© AI ì—ì´ì „íŠ¸ (í˜ì‹ ì !)")
    print("  - EnhancedContactExtractionAgent: AI ê°•í™” ì—°ë½ì²˜ ì¶”ì¶œ (ì£¼ì†Œ ê¸°ë°˜)")
    print("  - AIVerificationAgent: AI ì¢…í•© ê²€ì¦")
    print("="*80)
    
    try:
        # í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
        initialize_project()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
        if not DATABASE_AVAILABLE:
            print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìš°ì„  ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í¬ë¡¤ë§ ì‹œë„
        print("ğŸ—ƒï¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ ì¡°íšŒ ì¤‘...")
        results = await crawl_ai_enhanced_from_database()
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ ì‹œë„
        if not results:
            print("ğŸ“ ì…ë ¥ íŒŒì¼ì—ì„œ í¬ë¡¤ë§ ì‹œë„ ì¤‘...")
            try:
                results = await crawl_ai_enhanced_latest_file()
            except Exception as e:
                print(f"âš ï¸ ì…ë ¥ íŒŒì¼ í¬ë¡¤ë§ë„ ì‹¤íŒ¨: {e}")
        
        if results:
            ai_enhanced_count = sum(1 for r in results if r.get('ai_enhanced'))
            print(f"\nâœ… AI ê°•í™” í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ì¡°ì§ ì²˜ë¦¬")
            print(f"ğŸ¤– AI ê°•í™”ëœ ì¡°ì§: {ai_enhanced_count}ê°œ")
            print(f"ğŸ“ˆ AI ê°•í™”ìœ¨: {(ai_enhanced_count/len(results)*100):.1f}%")
        else:
            print("\nâŒ AI ê°•í™” í¬ë¡¤ë§ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main())