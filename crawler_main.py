#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© í¬ë¡¤ë§ ì—”ì§„
ê¸°ì¡´ advcrawler.py + enhanced_detail_extractor.py í†µí•©
config.py ì„¤ì • í™œìš©
"""

import asyncio
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path

# í”„ë¡œì íŠ¸ ì„¤ì • import
from settings import *
from utils.logger_utils import LoggerUtils
from utils.file_utils import FileUtils
from utils.phone_utils import PhoneUtils
from ai_helpers import AIModelManager

class UnifiedCrawler:
    """í†µí•© í¬ë¡¤ë§ ì—”ì§„"""
    
    def __init__(self, config_override=None):
        """ì´ˆê¸°í™”"""
        self.config = config_override or CRAWLING_CONFIG
        self.logger = LoggerUtils.setup_crawler_logger()
        self.ai_manager = AIModelManager()
        self.phone_utils = PhoneUtils()
        
        # í†µê³„ ì •ë³´
        self.stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "start_time": None,
            "end_time": None
        }
        
        self.logger.info("ğŸš€ í†µí•© í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def process_organizations(self, organizations: List[Dict], options: Dict = None) -> List[Dict]:
        """ì¡°ì§/ê¸°ê´€ ëª©ë¡ ì²˜ë¦¬"""
        if not organizations:
            self.logger.warning("ì²˜ë¦¬í•  ì¡°ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        options = options or {}
        self.stats["start_time"] = datetime.now()
        self.stats["total_processed"] = len(organizations)
        
        self.logger.info(f"ğŸ“Š ì´ {len(organizations)}ê°œ ì¡°ì§ ì²˜ë¦¬ ì‹œì‘")
        
        results = []
        save_interval = self.config.get("save_interval", 50)
        
        for i, org in enumerate(organizations, 1):
            try:
                # ë‹¨ì¼ ì¡°ì§ ì²˜ë¦¬
                processed_org = await self.process_single_organization(org, i)
                results.append(processed_org)
                
                self.stats["successful"] += 1
                
                # ì¤‘ê°„ ì €ì¥
                if i % save_interval == 0:
                    await self.save_intermediate_results(results, i)
                
                # ë”œë ˆì´
                await asyncio.sleep(self.config.get("default_delay", 2))
                
            except Exception as e:
                self.logger.error(f"âŒ ì¡°ì§ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {org.get('name', 'Unknown')} - {e}")
                self.stats["failed"] += 1
                
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ì›ë³¸ ë°ì´í„°ëŠ” ìœ ì§€
                results.append(org)
        
        self.stats["end_time"] = datetime.now()
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        final_file = await self.save_final_results(results)
        
        # í†µê³„ ì¶œë ¥
        self.print_final_statistics()
        
        return results
    
    async def process_single_organization(self, org: Dict, index: int) -> Dict:
        """ë‹¨ì¼ ì¡°ì§ ì²˜ë¦¬"""
        org_name = org.get('name', 'Unknown')
        self.logger.info(f"ğŸ¢ ì²˜ë¦¬ ì¤‘ [{index}]: {org_name}")
        
        result = org.copy()
        
        try:
            # 1. í™ˆí˜ì´ì§€ URL ê²€ìƒ‰ (ì—†ëŠ” ê²½ìš°)
            if not result.get('homepage'):
                homepage = await self.search_homepage(org_name)
                if homepage:
                    result['homepage'] = homepage
                    self.logger.info(f"  âœ… í™ˆí˜ì´ì§€ ë°œê²¬: {homepage}")
            
            # 2. í™ˆí˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            if result.get('homepage'):
                details = await self.extract_details_from_homepage(result['homepage'])
                result.update(details)
            
            # 3. êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ëˆ„ë½ ì •ë³´ ë³´ì™„
            missing_fields = self.find_missing_fields(result)
            if missing_fields:
                google_results = await self.search_missing_info(org_name, missing_fields)
                result.update(google_results)
            
            # 4. ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
            result = self.validate_and_clean_data(result)
            
            self.logger.info(f"  âœ… ì²˜ë¦¬ ì™„ë£Œ: {org_name}")
            
        except Exception as e:
            self.logger.error(f"  âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {org_name} - {e}")
            result['processing_error'] = str(e)
        
        return result
    
    async def process_json_file_async(self, json_file_path: str, test_mode: bool = False, test_count: int = 10, progress_callback=None) -> List[Dict]:
        """ğŸ”§ app.py í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ"""
        try:
            # JSON íŒŒì¼ ë¡œë“œ
            data = FileUtils.load_json(json_file_path)
            
            # ë°ì´í„° ì „ì²˜ë¦¬ (app.pyì™€ ë™ì¼í•œ ë°©ì‹)
            organizations = []
            if isinstance(data, dict):
                for category, orgs in data.items():
                    if isinstance(orgs, list):
                        for org in orgs:
                            if isinstance(org, dict):
                                org["category"] = category
                                organizations.append(org)
            elif isinstance(data, list):
                organizations = [org for org in data if isinstance(org, dict)]
            
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
            if test_mode and test_count:
                organizations = organizations[:test_count]
            
            # progress_callback ì €ì¥
            self.progress_callback = progress_callback
            
            # ì²˜ë¦¬ ì‹¤í–‰
            results = await self.process_organizations_with_callback(organizations)
            
            return results
            
        except Exception as e:
            self.logger.error(f"JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    async def process_organizations_with_callback(self, organizations: List[Dict]) -> List[Dict]:
        """ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ì¡°ì§ ì²˜ë¦¬"""
        if not organizations:
            return []
        
        self.stats["start_time"] = datetime.now()
        self.stats["total_processed"] = len(organizations)
        
        results = []
        
        for i, org in enumerate(organizations, 1):
            try:
                # ì²˜ë¦¬ ì‹œì‘ ì½œë°±
                if hasattr(self, 'progress_callback') and self.progress_callback:
                    callback_data = {
                        'name': org.get('name', 'Unknown'),
                        'category': org.get('category', ''),
                        'homepage_url': org.get('homepage', ''),
                        'status': 'processing',
                        'current_step': f'{i}/{len(organizations)}',
                        'processing_time': 0
                    }
                    try:
                        self.progress_callback(callback_data)
                    except Exception as e:
                        self.logger.error(f"ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                # ì‹¤ì œ ì²˜ë¦¬
                start_time = time.time()
                processed_org = await self.process_single_organization(org, i)
                processing_time = time.time() - start_time
                
                results.append(processed_org)
                self.stats["successful"] += 1
                
                # ì²˜ë¦¬ ì™„ë£Œ ì½œë°±
                if hasattr(self, 'progress_callback') and self.progress_callback:
                    callback_data = {
                        'name': processed_org.get('name', 'Unknown'),
                        'category': processed_org.get('category', ''),
                        'homepage_url': processed_org.get('homepage', ''),
                        'status': 'completed',
                        'current_step': f'{i}/{len(organizations)}',
                        'processing_time': processing_time,
                        'phone': processed_org.get('phone', ''),
                        'fax': processed_org.get('fax', ''),
                        'email': processed_org.get('email', ''),
                        'address': processed_org.get('address', ''),
                        'extraction_method': processed_org.get('extraction_method', 'unified_crawler')
                    }
                    try:
                        self.progress_callback(callback_data)
                    except Exception as e:
                        self.logger.error(f"ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                # ë”œë ˆì´
                await asyncio.sleep(self.config.get("default_delay", 2))
                
            except Exception as e:
                self.logger.error(f"âŒ ì¡°ì§ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {org.get('name', 'Unknown')} - {e}")
                self.stats["failed"] += 1
                
                # ì‹¤íŒ¨ ì½œë°±
                if hasattr(self, 'progress_callback') and self.progress_callback:
                    callback_data = {
                        'name': org.get('name', 'Unknown'),
                        'status': 'failed',
                        'error_message': str(e)
                    }
                    try:
                        self.progress_callback(callback_data)
                    except Exception as e:
                        self.logger.error(f"ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                results.append(org)
        
        self.stats["end_time"] = datetime.now()
        return results
    
    async def search_homepage(self, org_name: str) -> Optional[str]:
        """í™ˆí˜ì´ì§€ URL ê²€ìƒ‰"""
        try:
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ êµ¬ê¸€ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
            # ì‹¤ì œë¡œëŠ” seleniumì´ë‚˜ requestsë¥¼ ì‚¬ìš©
            search_query = f"{org_name} í™ˆí˜ì´ì§€"
            self.logger.debug(f"ğŸ” í™ˆí˜ì´ì§€ ê²€ìƒ‰: {search_query}")
            
            # TODO: ì‹¤ì œ ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
            # í˜„ì¬ëŠ” None ë°˜í™˜
            return None
            
        except Exception as e:
            self.logger.error(f"í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_details_from_homepage(self, homepage_url: str) -> Dict:
        """í™ˆí˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            self.logger.debug(f"ğŸŒ í™ˆí˜ì´ì§€ ë¶„ì„: {homepage_url}")
            
            # TODO: ì‹¤ì œ ì›¹ í¬ë¡¤ë§ ë° AI ë¶„ì„ ë¡œì§ êµ¬í˜„
            # í˜„ì¬ëŠ” ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
            extracted_data = {
                "extraction_method": "homepage_crawling",
                "extraction_timestamp": datetime.now().isoformat(),
                "source_url": homepage_url
            }
            
            return extracted_data
            
        except Exception as e:
            self.logger.error(f"í™ˆí˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    async def search_missing_info(self, org_name: str, missing_fields: List[str]) -> Dict:
        """êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ëˆ„ë½ ì •ë³´ ê²€ìƒ‰"""
        try:
            self.logger.debug(f"ğŸ” ëˆ„ë½ ì •ë³´ ê²€ìƒ‰: {org_name} - {missing_fields}")
            
            results = {}
            
            for field in missing_fields:
                if field == "phone":
                    search_query = f"{org_name} ì „í™”ë²ˆí˜¸"
                elif field == "fax":
                    search_query = f"{org_name} íŒ©ìŠ¤ë²ˆí˜¸"
                elif field == "email":
                    search_query = f"{org_name} ì´ë©”ì¼"
                elif field == "address":
                    search_query = f"{org_name} ì£¼ì†Œ"
                else:
                    continue
                
                # TODO: ì‹¤ì œ êµ¬ê¸€ ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
                # í˜„ì¬ëŠ” ë¹ˆ ê°’ ë°˜í™˜
                
            return results
            
        except Exception as e:
            self.logger.error(f"ëˆ„ë½ ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {}
    
    def find_missing_fields(self, org: Dict) -> List[str]:
        """ëˆ„ë½ëœ í•„ë“œ ì°¾ê¸°"""
        required_fields = ["phone", "fax", "email", "address"]
        missing = []
        
        for field in required_fields:
            if not org.get(field) or str(org.get(field)).strip() == "":
                missing.append(field)
        
        return missing
    
    def validate_and_clean_data(self, org: Dict) -> Dict:
        """ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬"""
        try:
            # ì „í™”ë²ˆí˜¸ ê²€ì¦
            if org.get('phone'):
                org['phone'] = self.phone_utils.clean_phone_number(org['phone'])
                if not self.phone_utils.validate_phone_number(org['phone']):
                    org['phone_validation_error'] = "Invalid phone format"
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦
            if org.get('fax'):
                org['fax'] = self.phone_utils.clean_phone_number(org['fax'])
            
            # ì´ë©”ì¼ ê²€ì¦
            if org.get('email'):
                # ê°„ë‹¨í•œ ì´ë©”ì¼ ê²€ì¦
                import re
                email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
                if not re.match(email_pattern, org['email']):
                    org['email_validation_error'] = "Invalid email format"
            
            return org
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return org
    
    async def save_intermediate_results(self, results: List[Dict], count: int):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        try:
            filename = generate_output_filename("intermediate", OUTPUT_DIR, count=count)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {filename} ({count}ê°œ)")
            
        except Exception as e:
            self.logger.error(f"ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def save_final_results(self, results: List[Dict]) -> str:
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        try:
            filename = generate_output_filename("enhanced_final", OUTPUT_DIR)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ ìµœì¢… ì €ì¥ ì™„ë£Œ: {filename}")
            return str(filename)
            
        except Exception as e:
            self.logger.error(f"ìµœì¢… ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def print_final_statistics(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        duration = self.stats["end_time"] - self.stats["start_time"]
        
        print("\n" + "="*60)
        print("ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ í†µê³„")
        print("="*60)
        print(f"ğŸ“‹ ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"âœ… ì„±ê³µ: {self.stats['successful']}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {(self.stats['successful']/self.stats['total_processed']*100):.1f}%")
        print(f"â±ï¸ ì†Œìš”ì‹œê°„: {duration}")
        print(f"ğŸš€ í‰ê·  ì²˜ë¦¬ì‹œê°„: {duration.total_seconds()/self.stats['total_processed']:.2f}ì´ˆ/ê°œ")
        print("="*60)

# í¸ì˜ í•¨ìˆ˜ë“¤
async def crawl_from_file(input_file: str, options: Dict = None) -> List[Dict]:
    """íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ í¬ë¡¤ë§"""
    try:
        # íŒŒì¼ ë¡œë“œ
        data = FileUtils.load_json(input_file)
        if not data:
            raise ValueError(f"íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        
        # í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
        crawler = UnifiedCrawler()
        results = await crawler.process_organizations(data, options)
        
        return results
        
    except Exception as e:
        logging.error(f"íŒŒì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

async def crawl_latest_file(options: Dict = None) -> List[Dict]:
    """ìµœì‹  ì…ë ¥ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ í¬ë¡¤ë§"""
    try:
        latest_file = get_latest_input_file()
        if not latest_file:
            raise ValueError("ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ“‚ ìµœì‹  íŒŒì¼ ì‚¬ìš©: {latest_file}")
        return await crawl_from_file(str(latest_file), options)
        
    except Exception as e:
        logging.error(f"ìµœì‹  íŒŒì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*60)
    
    try:
        # í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
        initialize_project()
        
        # ìµœì‹  íŒŒì¼ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        results = await crawl_latest_file()
        
        if results:
            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ì¡°ì§ ì²˜ë¦¬")
        else:
            print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main()) 