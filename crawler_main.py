"""
AI ê°•í™” í†µí•© í¬ë¡¤ë§ ì—”ì§„ v4.0
âœ… ê¸°ì¡´ ì „ë¬¸ ëª¨ë“ˆ + AI Agentic Workflow í†µí•©
âœ… ìµœê³  í’ˆì§ˆì˜ AI ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ
âœ… ì‹ ë¢°ë„ ì ìˆ˜ ë° ë‹¤ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'test'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'cralwer'))

import asyncio
import json
import time
import re
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

# í”„ë¡œì íŠ¸ ì„¤ì • import
from utils.settings import *
from utils.logger_utils import LoggerUtils
from utils.file_utils import FileUtils
from utils.phone_utils import PhoneUtils
from utils.crawler_utils import CrawlerUtils
from utils.ai_helpers import AIModelManager

# ì „ë¬¸ ëª¨ë“ˆë“¤ import (ê¸°ì¡´ ìœ ì§€)
try:
    from cralwer.fax_extractor import GoogleContactCrawler as FaxExtractor
    FAX_EXTRACTOR_AVAILABLE = True
    print("âœ… fax_extractor.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ fax_extractor.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    FAX_EXTRACTOR_AVAILABLE = False

try:
    from cralwer.phone_extractor import extract_phone_numbers, search_phone_number, setup_driver
    PHONE_EXTRACTOR_AVAILABLE = True
    print("âœ… phone_extractor.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ phone_extractor.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    PHONE_EXTRACTOR_AVAILABLE = False

try:
    from cralwer.url_extractor import HomepageParser
    URL_EXTRACTOR_AVAILABLE = True
    print("âœ… url_extractor.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ url_extractor.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    URL_EXTRACTOR_AVAILABLE = False

try:
    from utils.validator import ContactValidator, AIValidator
    VALIDATOR_AVAILABLE = True
    print("âœ… validator.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ validator.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    VALIDATOR_AVAILABLE = False

try:
    from database.database import get_database
    DATABASE_AVAILABLE = True
    print("âœ… database.py ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ database.py ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    DATABASE_AVAILABLE = False

# ==================== AI Agentic Workflow ì‹œìŠ¤í…œ í†µí•© ====================

class CrawlingStage(Enum):
    """í¬ë¡¤ë§ ë‹¨ê³„ ì •ì˜"""
    INITIALIZATION = "ì´ˆê¸°í™”"
    HOMEPAGE_SEARCH = "í™ˆí˜ì´ì§€_ê²€ìƒ‰"
    HOMEPAGE_ANALYSIS = "í™ˆí˜ì´ì§€_ë¶„ì„"
    CONTACT_PAGE_SEARCH = "ì—°ë½ì²˜í˜ì´ì§€_ê²€ìƒ‰"
    CONTACT_EXTRACTION = "ì—°ë½ì²˜_ì¶”ì¶œ"
    FAX_SEARCH = "íŒ©ìŠ¤_ê²€ìƒ‰"
    AI_VERIFICATION = "AI_ê²€ì¦"
    DATA_VALIDATION = "ë°ì´í„°_ê²€ì¦"
    COMPLETION = "ì™„ë£Œ"

@dataclass
class CrawlingContext:
    """í¬ë¡¤ë§ ì»¨í…ìŠ¤íŠ¸ (AI Agent ê°„ ê³µìœ  ë°ì´í„°)"""
    organization: Dict[str, Any]
    current_stage: CrawlingStage
    extracted_data: Dict[str, Any]
    ai_insights: Dict[str, Any]
    error_log: List[str]
    processing_time: float
    confidence_scores: Dict[str, float]

class AIAgent:
    """AI ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler=None):
        self.name = name
        self.ai_manager = ai_manager
        self.logger = logger
        self.parent_crawler = parent_crawler  # Optionalë¡œ ë³€ê²½
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    async def should_execute(self, context: CrawlingContext) -> bool:
        """ì‹¤í–‰ ì¡°ê±´ í™•ì¸"""
        return True
    
    def update_confidence(self, context: CrawlingContext, field: str, score: float):
        """ì‹ ë¢°ë„ ì ìˆ˜ ì—…ë°ì´íŠ¸"""
        context.confidence_scores[field] = score

class EnhancedHomepageSearchAgent(AIAgent):
    """AI ê°•í™” í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì—ì´ì „íŠ¸"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler):
        super().__init__("EnhancedHomepageSearchAgent", ai_manager, logger, parent_crawler)
        self.crawler_utils = CrawlerUtils()
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰"""
        try:
            org_name = context.organization.get('name', '')
            category = context.organization.get('category', '')
            self.logger.info(f"ğŸ” [{self.name}] AI í™ˆí˜ì´ì§€ ê²€ìƒ‰: {org_name}")
            
            # ê¸°ì¡´ í™ˆí˜ì´ì§€ê°€ ìˆìœ¼ë©´ AIë¡œ ê²€ì¦
            existing_homepage = context.organization.get('homepage', '')
            if existing_homepage:
                verification_result = await self._verify_homepage_with_ai(existing_homepage, org_name, category)
                if verification_result['is_valid']:
                    context.extracted_data['homepage'] = existing_homepage
                    context.extracted_data['homepage_type'] = verification_result['type']
                    self.update_confidence(context, 'homepage', verification_result['confidence'])
                    context.current_stage = CrawlingStage.HOMEPAGE_ANALYSIS
                    return context
            
            # AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰
            search_results = await self._ai_comprehensive_homepage_search(org_name, category)
            if search_results:
                best_result = search_results[0]
                context.extracted_data['homepage'] = best_result['url']
                context.extracted_data['homepage_type'] = best_result['type']
                context.extracted_data['homepage_confidence'] = best_result['confidence']
                self.update_confidence(context, 'homepage', best_result['confidence'])
                self.logger.info(f"âœ… AI í™ˆí˜ì´ì§€ ë°œê²¬: {best_result['url']} ({best_result['type']})")
            
            context.current_stage = CrawlingStage.HOMEPAGE_ANALYSIS
            return context
            
        except Exception as e:
            context.error_log.append(f"EnhancedHomepageSearchAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _ai_comprehensive_homepage_search(self, org_name: str, category: str) -> List[Dict]:
        """AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰"""
        # ê¸°ì¡´ ëª¨ë“ˆì˜ homepage_parser í™œìš© + AI ê°•í™”
        if self.parent_crawler.homepage_parser:
            try:
                # url_extractorì˜ AI ê¸°ëŠ¥ í™œìš©
                ai_search_results = await self.parent_crawler.homepage_parser.ai_search_homepage(org_name, category)
                return ai_search_results
            except Exception as e:
                self.logger.warning(f"AI í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return []
    
    async def _verify_homepage_with_ai(self, url: str, org_name: str, category: str) -> Dict:
        """AIë¡œ í™ˆí˜ì´ì§€ ê´€ë ¨ì„± ê²€ì¦"""
        try:
            prompt = f"""
            ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™ˆí˜ì´ì§€ì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”:

            **ê¸°ê´€ ì •ë³´:**
            - ê¸°ê´€ëª…: {org_name}
            - ì¹´í…Œê³ ë¦¬: {category}
            - í™ˆí˜ì´ì§€ URL: {url}

            **íŒë‹¨ ê¸°ì¤€:**
            1. ë„ë©”ì¸ëª…ì´ ê¸°ê´€ëª…ê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€?
            2. URLì´ í•´ë‹¹ ê¸°ê´€ì˜ ê³µì‹ ë˜ëŠ” ê´€ë ¨ í˜ì´ì§€ì¸ê°€?
            3. ì†Œê·œëª¨ ê¸°ê´€ì˜ ê²½ìš° ë¸”ë¡œê·¸/ì¹´í˜/SNSë„ ê³µì‹ í˜ì´ì§€ë¡œ ì¸ì •
            4. ê¸°ê´€ì˜ ì„±ê²©ê³¼ URL íƒ€ì…ì´ ì í•©í•œê°€?

            **ì‘ë‹µ í˜•ì‹:**
            VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            TYPE: [ê³µì‹ì‚¬ì´íŠ¸/ë„¤ì´ë²„ë¸”ë¡œê·¸/ë„¤ì´ë²„ì¹´í˜/í˜ì´ìŠ¤ë¶/ì¸ìŠ¤íƒ€ê·¸ë¨/ìœ íŠœë¸Œ/ê¸°íƒ€]
            CONFIDENCE: [0.1-1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„ ì ìˆ˜]
            REASON: [íŒë‹¨ ì´ìœ  1-2ë¬¸ì¥]
            """
            
            response = await self.ai_manager.extract_with_gemini(url, prompt)
            return self._parse_verification_response(response)
            
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {'is_valid': False, 'type': 'ì•Œìˆ˜ì—†ìŒ', 'confidence': 0.0}
    
    def _parse_verification_response(self, response: str) -> Dict:
        """AI ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        result = {
            'is_valid': False,
            'type': 'ì•Œìˆ˜ì—†ìŒ',
            'confidence': 0.0,
            'reason': ''
        }
        
        try:
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('VALID:'):
                    result['is_valid'] = 'ì˜ˆ' in line
                elif line.startswith('TYPE:'):
                    result['type'] = line.replace('TYPE:', '').strip()
                elif line.startswith('CONFIDENCE:'):
                    confidence_text = line.replace('CONFIDENCE:', '').strip()
                    try:
                        result['confidence'] = float(confidence_text)
                    except:
                        result['confidence'] = 0.5
                elif line.startswith('REASON:'):
                    result['reason'] = line.replace('REASON:', '').strip()
        
        except Exception as e:
            self.logger.warning(f"ê²€ì¦ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return result

class EnhancedHomepageAnalysisAgent(AIAgent):
    """AI ê°•í™” í™ˆí˜ì´ì§€ ë¶„ì„ ì—ì´ì „íŠ¸ - BS4 â†’ JS â†’ AI ìˆœì„œ"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler):
        super().__init__("EnhancedHomepageAnalysisAgent", ai_manager, logger, parent_crawler)
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ë‹¨ê³„ë³„ í™ˆí˜ì´ì§€ ë¶„ì„: BS4 â†’ JS ë Œë”ë§ â†’ AI ë¶„ì„"""
        try:
            homepage_url = context.extracted_data.get('homepage')
            if not homepage_url:
                self.logger.info(f"ğŸ“‹ [{self.name}] í™ˆí˜ì´ì§€ê°€ ì—†ì–´ ë¶„ì„ ê±´ë„ˆë›°ê¸°")
                context.current_stage = CrawlingStage.CONTACT_EXTRACTION
                return context
            
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ” [{self.name}] ë‹¨ê³„ë³„ í™ˆí˜ì´ì§€ ë¶„ì„: {homepage_url}")
            
            # 1ë‹¨ê³„: BS4ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            extracted_text = await self._extract_with_bs4(homepage_url)
            if not extracted_text:
                # 2ë‹¨ê³„: JS ë Œë”ë§ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                extracted_text = await self._extract_with_selenium(homepage_url)
            
            if extracted_text:
                # 3ë‹¨ê³„: AIë¡œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
                contact_info = await self._extract_contacts_with_ai(extracted_text, org_name)
                if contact_info:
                    self._store_enhanced_contact_info(context, contact_info)
                    context.extracted_data['homepage_analyzed'] = True
                    self.logger.info(f"âœ… [{self.name}] AI í™ˆí˜ì´ì§€ ë¶„ì„ ì™„ë£Œ")
                else:
                    self.logger.warning(f"âš ï¸ [{self.name}] AIì—ì„œ ì—°ë½ì²˜ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•¨")
            else:
                context.error_log.append(f"í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {homepage_url}")
                self.logger.warning(f"âš ï¸ [{self.name}] í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            
            context.current_stage = CrawlingStage.CONTACT_EXTRACTION
            return context
            
        except Exception as e:
            context.error_log.append(f"EnhancedHomepageAnalysisAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _extract_with_bs4(self, url: str) -> Optional[str]:
        """1ë‹¨ê³„: BS4ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            import requests
            from bs4 import BeautifulSoup
            
            self.logger.info(f"ğŸ” BS4 í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„: {url}")
            
            headers = REQUEST_HEADERS
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
            for element in soup(["script", "style", "noscript", "meta", "link"]):
                element.decompose()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = soup.get_text()
            text = re.sub(r'\s+', ' ', text).strip()
            
            if len(text) > 500:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                self.logger.info(f"âœ… BS4 ì¶”ì¶œ ì„±ê³µ: {len(text)} chars")
                return text[:10000]  # ìµœëŒ€ 10,000ì
            else:
                self.logger.warning(f"âš ï¸ BS4 ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ: {len(text)} chars")
                return None
                
        except Exception as e:
            self.logger.warning(f"BS4 í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _extract_with_selenium(self, url: str) -> Optional[str]:
        """2ë‹¨ê³„: Seleniumìœ¼ë¡œ JS ë Œë”ë§ í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            self.logger.info(f"ğŸ” Selenium JS ë Œë”ë§ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„: {url}")
            
            if self.parent_crawler and self.parent_crawler.homepage_parser:
                page_data = self.parent_crawler.homepage_parser.extract_page_content(url)
                if page_data and page_data.get('accessible') and page_data.get('text_content'):
                    text = page_data['text_content']
                    self.logger.info(f"âœ… Selenium ì¶”ì¶œ ì„±ê³µ: {len(text)} chars")
                    return text
            
            self.logger.warning("âš ï¸ Selenium í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.warning(f"Selenium í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _extract_contacts_with_ai(self, text_content: str, org_name: str) -> Optional[Dict]:
        """3ë‹¨ê³„: AIë¡œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            self.logger.info(f"ğŸ¤– AI ì—°ë½ì²˜ ì¶”ì¶œ: {org_name}")
            
            prompt = f"""
            ë‹¤ìŒì€ '{org_name}' ê¸°ê´€ì˜ í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
            ì´ í…ìŠ¤íŠ¸ì—ì„œ ì—°ë½ì²˜ ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

            **í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸:**
            {text_content[:5000]}

            **ì¶”ì¶œí•  ì •ë³´:**
            1. ì „í™”ë²ˆí˜¸ (02-XXX-XXXX, 031-XXX-XXXX í˜•íƒœ)
            2. íŒ©ìŠ¤ë²ˆí˜¸ (ì „í™”ë²ˆí˜¸ì™€ ë‹¤ë¥¸ ë²ˆí˜¸)
            3. ì´ë©”ì¼ ì£¼ì†Œ
            4. ì£¼ì†Œ (ë„ë¡œëª…ì£¼ì†Œ ë˜ëŠ” ì§€ë²ˆì£¼ì†Œ)
            5. íœ´ëŒ€í°ë²ˆí˜¸ (010-XXXX-XXXX í˜•íƒœ)

            **ì‘ë‹µ í˜•ì‹ (JSON):**
            {{
                "phone": "ì „í™”ë²ˆí˜¸ ë˜ëŠ” null",
                "fax": "íŒ©ìŠ¤ë²ˆí˜¸ ë˜ëŠ” null", 
                "email": "ì´ë©”ì¼ ë˜ëŠ” null",
                "address": "ì£¼ì†Œ ë˜ëŠ” null",
                "mobile": "íœ´ëŒ€í° ë˜ëŠ” null"
            }}

            **ì£¼ì˜ì‚¬í•­:**
            - ì •í™•í•œ í˜•íƒœì˜ ì—°ë½ì²˜ë§Œ ì¶”ì¶œ
            - ì „í™”ë²ˆí˜¸ì™€ íŒ©ìŠ¤ë²ˆí˜¸ê°€ ê°™ìœ¼ë©´ íŒ©ìŠ¤ëŠ” null
            - ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ null ê°’ ì‚¬ìš©
            """
            
            if self.ai_manager and self.ai_manager.gemini_model:
                response = self.ai_manager.gemini_model.generate_content(prompt)
                response_text = response.text.strip()
                
                # JSON ì¶”ì¶œ ë° íŒŒì‹±
                contact_info = self._parse_ai_contact_response(response_text)
                
                if contact_info:
                    self.logger.info(f"âœ… AI ì—°ë½ì²˜ ì¶”ì¶œ ì„±ê³µ: {contact_info}")
                    return contact_info
                else:
                    self.logger.warning("âš ï¸ AI ì‘ë‹µì—ì„œ ì—°ë½ì²˜ ì •ë³´ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŒ")
                    return None
            else:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥")
                return None
                
        except Exception as e:
            self.logger.error(f"AI ì—°ë½ì²˜ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_ai_contact_response(self, response_text: str) -> Optional[Dict]:
        """AI ì‘ë‹µì—ì„œ ì—°ë½ì²˜ ì •ë³´ íŒŒì‹±"""
        try:
            import json
            
            # JSON ë¸”ë¡ ì°¾ê¸°
            if '```json' in response_text:
                json_part = response_text.split('```json')[1].split('```')[0].strip()
            elif '{' in response_text and '}' in response_text:
                start = response_text.find('{')
                end = response_text.rfind('}') + 1
                json_part = response_text[start:end]
            else:
                return None
            
            # JSON íŒŒì‹±
            contact_info = json.loads(json_part)
            
            # ê²°ê³¼ ì •ë¦¬ (null ê°’ ì œê±°)
            cleaned_info = {}
            for key, value in contact_info.items():
                if value and value.lower() != 'null' and value.strip():
                    cleaned_info[key] = value.strip()
            
            return cleaned_info if cleaned_info else None
            
        except Exception as e:
            self.logger.warning(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

class EnhancedContactExtractionAgent(AIAgent):
    """AI ê°•í™” ì—°ë½ì²˜ ì¶”ì¶œ ì—ì´ì „íŠ¸ - ì£¼ì†Œ ê¸°ë°˜ Selenium ê²€ìƒ‰"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler):
        super().__init__("EnhancedContactExtractionAgent", ai_manager, logger, parent_crawler)
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """ì£¼ì†Œ ê¸°ë°˜ Selenium ê²€ìƒ‰ â†’ AI ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            org_address = context.organization.get('address', '')
            
            self.logger.info(f"ğŸ“ [{self.name}] ì£¼ì†Œ ê¸°ë°˜ ì—°ë½ì²˜ ê²€ìƒ‰: {org_name}")
            
            # ì „í™”ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰
            if not context.extracted_data.get('phone'):
                phone_result = await self._search_phone_with_address(org_name, org_address)
                if phone_result:
                    # AIë¡œ ê²€ì¦
                    is_valid = await self._verify_contact_with_ai(phone_result, org_name, 'phone')
                    if is_valid:
                        context.extracted_data['phone'] = phone_result
                        context.extracted_data['phone_source'] = 'address_based_search'
                        self.update_confidence(context, 'phone', 0.8)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ (ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µ ë°©ì§€)
            if not context.extracted_data.get('fax'):
                fax_result = await self._search_fax_with_address(
                    org_name, org_address, context.extracted_data.get('phone')
                )
                if fax_result:
                    # AIë¡œ ê²€ì¦
                    is_valid = await self._verify_contact_with_ai(fax_result, org_name, 'fax')
                    if is_valid:
                        context.extracted_data['fax'] = fax_result
                        context.extracted_data['fax_source'] = 'address_based_search'
                        self.update_confidence(context, 'fax', 0.8)
            
            context.current_stage = CrawlingStage.AI_VERIFICATION
            return context
            
        except Exception as e:
            context.error_log.append(f"EnhancedContactExtractionAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _search_phone_with_address(self, org_name: str, address: str) -> Optional[str]:
        """ì£¼ì†Œ ê¸°ë°˜ ì „í™”ë²ˆí˜¸ ê²€ìƒ‰"""
        try:
            if not org_name:
                return None
            
            # ì§€ì—­ ì •ë³´ ì¶”ì¶œ
            region_info = self._extract_region_from_address(address)
            search_query = f"{org_name} ì „í™”ë²ˆí˜¸"
            
            if region_info:
                search_query = f"{region_info} {org_name} ì „í™”ë²ˆí˜¸"
            
            self.logger.info(f"ğŸ” ì „í™”ë²ˆí˜¸ ê²€ìƒ‰: {search_query}")
            
            # Seleniumìœ¼ë¡œ ê²€ìƒ‰
            if self.parent_crawler and self.parent_crawler.phone_driver:
                from cralwer.phone_extractor import search_phone_number
                found_phones = search_phone_number(self.parent_crawler.phone_driver, search_query)
                
                if found_phones:
                    # ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ë°˜í™˜ (ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²ƒìœ¼ë¡œ ê°€ì •)
                    phone = found_phones[0]
                    
                    # ì§€ì—­ë²ˆí˜¸ ê²€ì¦
                    if self._validate_phone_by_region(phone, address):
                        self.logger.info(f"âœ… ì „í™”ë²ˆí˜¸ ë°œê²¬: {phone}")
                        return phone
                    else:
                        self.logger.warning(f"âš ï¸ ì§€ì—­ë²ˆí˜¸ ë¶ˆì¼ì¹˜: {phone} (ì£¼ì†Œ: {address})")
            
            return None
            
        except Exception as e:
            self.logger.warning(f"ì „í™”ë²ˆí˜¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    async def _search_fax_with_address(self, org_name: str, address: str, existing_phone: str) -> Optional[str]:
        """ì£¼ì†Œ ê¸°ë°˜ íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ (ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µ ë°©ì§€)"""
        try:
            if not org_name:
                return None
            
            # ì§€ì—­ ì •ë³´ ì¶”ì¶œ
            region_info = self._extract_region_from_address(address)
            search_query = f"{org_name} íŒ©ìŠ¤ë²ˆí˜¸"
            
            if region_info:
                search_query = f"{region_info} {org_name} íŒ©ìŠ¤ë²ˆí˜¸"
            
            self.logger.info(f"ğŸ“  íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰: {search_query}")
            
            # íŒ©ìŠ¤ ì¶”ì¶œê¸°ë¡œ ê²€ìƒ‰
            if self.parent_crawler and self.parent_crawler.fax_extractor:
                found_faxes = self.parent_crawler.fax_extractor.search_fax_number(org_name)
                
                for fax in found_faxes:
                    # ì „í™”ë²ˆí˜¸ì™€ ì¤‘ë³µ ì²´í¬
                    if not self._is_duplicate_number(fax, existing_phone):
                        # ì§€ì—­ë²ˆí˜¸ ê²€ì¦
                        if self._validate_phone_by_region(fax, address):
                            self.logger.info(f"âœ… íŒ©ìŠ¤ë²ˆí˜¸ ë°œê²¬: {fax}")
                            return fax
                        else:
                            self.logger.warning(f"âš ï¸ íŒ©ìŠ¤ ì§€ì—­ë²ˆí˜¸ ë¶ˆì¼ì¹˜: {fax}")
                
                self.logger.info("ğŸ“  ì¤‘ë³µë˜ì§€ ì•ŠëŠ” íŒ©ìŠ¤ë²ˆí˜¸ ì—†ìŒ")
            
            return None
            
        except Exception as e:
            self.logger.warning(f"íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_region_from_address(self, address: str) -> Optional[str]:
        """ì£¼ì†Œì—ì„œ ì§€ì—­ ì •ë³´ ì¶”ì¶œ"""
        if not address:
            return None
        
        # settings.pyì˜ REGION_TO_AREA_CODE í™œìš©
        for region in REGION_TO_AREA_CODE.keys():
            if region in address:
                return region
        
        return None
    
    def _validate_phone_by_region(self, phone: str, address: str) -> bool:
        """ì§€ì—­ë²ˆí˜¸ì™€ ì£¼ì†Œ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸"""
        if not phone or not address:
            return True  # ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì¼ë‹¨ í†µê³¼
        
        # settings.pyì˜ validate_phone_by_region í™œìš©
        return validate_phone_by_region(phone, address)
    
    def _is_duplicate_number(self, number1: str, number2: str) -> bool:
        """ë‘ ë²ˆí˜¸ê°€ ì¤‘ë³µì¸ì§€ í™•ì¸"""
        if not number1 or not number2:
            return False
        
        # settings.pyì˜ is_phone_fax_duplicate í™œìš©
        return is_phone_fax_duplicate(number1, number2)
    
    async def _verify_contact_with_ai(self, contact: str, org_name: str, contact_type: str) -> bool:
        """AIë¡œ ì—°ë½ì²˜ ìœ íš¨ì„± ê²€ì¦"""
        try:
            prompt = f"""
            ë‹¤ìŒ ì •ë³´ê°€ ì˜¬ë°”ë¥¸ì§€ ê²€ì¦í•´ì£¼ì„¸ìš”:

            **ê¸°ê´€ëª…:** {org_name}
            **{contact_type}:** {contact}

            **ê²€ì¦ ê¸°ì¤€:**
            1. ë²ˆí˜¸ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€? (í•œêµ­ ì „í™”ë²ˆí˜¸ í˜•ì‹)
            2. ê¸°ê´€ëª…ê³¼ ê´€ë ¨ì„±ì´ ìˆì–´ ë³´ì´ëŠ”ê°€?
            3. ìœ íš¨í•œ ë²ˆí˜¸ë¡œ ë³´ì´ëŠ”ê°€?

            **ì‘ë‹µ í˜•ì‹:**
            VALID: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            CONFIDENCE: [0.1-1.0]
            REASON: [íŒë‹¨ ì´ìœ ]
            """
            
            if self.ai_manager and self.ai_manager.gemini_model:
                response = self.ai_manager.gemini_model.generate_content(prompt)
                response_text = response.text.strip()
                
                # ì‘ë‹µ íŒŒì‹±
                is_valid = self._parse_verification_response(response_text)
                self.logger.info(f"ğŸ¤– AI ê²€ì¦ ê²°ê³¼ ({contact_type}): {is_valid}")
                return is_valid
            
            return True  # AIê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
            
        except Exception as e:
            self.logger.warning(f"AI ê²€ì¦ ì‹¤íŒ¨: {e}")
            return True  # ì˜¤ë¥˜ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
    
    def _parse_verification_response(self, response_text: str) -> bool:
        """AI ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        try:
            if 'VALID:' in response_text:
                valid_line = [line for line in response_text.split('\n') if 'VALID:' in line][0]
                return 'ì˜ˆ' in valid_line or 'true' in valid_line.lower()
            
            # ë°±ì—…: ê¸ì •ì  í‚¤ì›Œë“œ ì²´í¬
            positive_keywords = ['ì˜ˆ', 'valid', 'true', 'ì˜¬ë°”', 'ìœ íš¨', 'ì ì ˆ']
            return any(keyword in response_text.lower() for keyword in positive_keywords)
            
        except Exception:
            return True

class AIVerificationAgent(AIAgent):
    """AI ê²€ì¦ ì—ì´ì „íŠ¸"""
    
    def __init__(self, ai_manager: AIModelManager, logger: logging.Logger, parent_crawler):
        super().__init__("AIVerificationAgent", ai_manager, logger, parent_crawler)
    
    async def execute(self, context: CrawlingContext) -> CrawlingContext:
        """AI ì¢…í•© ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            self.logger.info(f"ğŸ” [{self.name}] AI ì¢…í•© ê²€ì¦: {org_name}")
            
            # AI ì¢…í•© ê²€ì¦ ì‹¤í–‰
            verification_result = await self._ai_comprehensive_verification(context)
            
            # ê²€ì¦ ê²°ê³¼ ì ìš©
            context.ai_insights['verification'] = verification_result
            self._adjust_confidence_scores(context, verification_result)
            
            context.current_stage = CrawlingStage.COMPLETION
            return context
            
        except Exception as e:
            context.error_log.append(f"AIVerificationAgent ì˜¤ë¥˜: {str(e)}")
            self.logger.error(f"âŒ [{self.name}] ì˜¤ë¥˜: {e}")
            return context
    
    async def _ai_comprehensive_verification(self, context: CrawlingContext) -> Dict[str, Any]:
        """AI ì¢…í•© ê²€ì¦"""
        try:
            org_name = context.organization.get('name', '')
            extracted_data = context.extracted_data
            
            # ê²€ì¦í•  ë°ì´í„° ì¤€ë¹„
            verification_prompt = f"""
            ê¸°ê´€ëª…: {org_name}
            ì¶”ì¶œëœ ë°ì´í„°:
            - í™ˆí˜ì´ì§€: {extracted_data.get('homepage', 'ì—†ìŒ')}
            - ì „í™”ë²ˆí˜¸: {extracted_data.get('phone', 'ì—†ìŒ')}
            - íŒ©ìŠ¤ë²ˆí˜¸: {extracted_data.get('fax', 'ì—†ìŒ')}
            - ì´ë©”ì¼: {extracted_data.get('email', 'ì—†ìŒ')}
            
            ìœ„ ì •ë³´ë“¤ì´ í•´ë‹¹ ê¸°ê´€ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦í•´ì£¼ì„¸ìš”.
            ì‘ë‹µ í˜•ì‹:
            OVERALL_VALIDITY: [valid/invalid/uncertain]
            PHONE_VALIDITY: [valid/invalid/uncertain]
            FAX_VALIDITY: [valid/invalid/uncertain]
            HOMEPAGE_VALIDITY: [valid/invalid/uncertain]
            CONFIDENCE_SCORE: [0.0-1.0]
            """
            
            if self.ai_manager and self.ai_manager.gemini_model:
                response = self.ai_manager.gemini_model.generate_content(verification_prompt)
                response_text = response.text.strip()
                
                # ì‘ë‹µ íŒŒì‹±
                return self._parse_verification_response(response_text)
            else:
                return {
                    'overall_validity': 'uncertain',
                    'confidence_score': 0.5,
                    'verification_method': 'no_ai_available'
                }
                
        except Exception as e:
            self.logger.warning(f"AI ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                'overall_validity': 'uncertain',
                'confidence_score': 0.5,
                'verification_error': str(e)
            }
    
    def _parse_verification_response(self, response_text: str) -> Dict[str, Any]:
        """ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        result = {
            'overall_validity': 'uncertain',
            'phone_validity': 'uncertain',
            'fax_validity': 'uncertain',
            'homepage_validity': 'uncertain',
            'confidence_score': 0.5
        }
        
        try:
            lines = response_text.split('\n')
            for line in lines:
                line = line.strip().upper()
                
                if 'OVERALL_VALIDITY:' in line:
                    validity = line.split(':', 1)[1].strip().lower()
                    if validity in ['valid', 'invalid', 'uncertain']:
                        result['overall_validity'] = validity
                
                elif 'PHONE_VALIDITY:' in line:
                    validity = line.split(':', 1)[1].strip().lower()
                    if validity in ['valid', 'invalid', 'uncertain']:
                        result['phone_validity'] = validity
                
                elif 'FAX_VALIDITY:' in line:
                    validity = line.split(':', 1)[1].strip().lower()
                    if validity in ['valid', 'invalid', 'uncertain']:
                        result['fax_validity'] = validity
                
                elif 'HOMEPAGE_VALIDITY:' in line:
                    validity = line.split(':', 1)[1].strip().lower()
                    if validity in ['valid', 'invalid', 'uncertain']:
                        result['homepage_validity'] = validity
                
                elif 'CONFIDENCE_SCORE:' in line:
                    try:
                        score = float(line.split(':', 1)[1].strip())
                        if 0.0 <= score <= 1.0:
                            result['confidence_score'] = score
                    except ValueError:
                        pass
        
        except Exception as e:
            self.logger.warning(f"ê²€ì¦ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return result
    
    def _adjust_confidence_scores(self, context: CrawlingContext, verification: Dict[str, Any]):
        """ì‹ ë¢°ë„ ì ìˆ˜ ì¡°ì •"""
        try:
            adjustment_factor = verification.get('confidence_score', 0.5)
            
            # ì „ì²´ ìœ íš¨ì„±ì— ë”°ë¥¸ ì¡°ì •
            if verification.get('overall_validity') == 'valid':
                adjustment_factor *= 1.2
            elif verification.get('overall_validity') == 'invalid':
                adjustment_factor *= 0.5
            
            # ê° í•„ë“œë³„ ì‹ ë¢°ë„ ì¡°ì •
            for field in ['phone', 'fax', 'homepage']:
                if field in context.confidence_scores:
                    field_validity = verification.get(f'{field}_validity', 'uncertain')
                    if field_validity == 'valid':
                        context.confidence_scores[field] *= 1.1
                    elif field_validity == 'invalid':
                        context.confidence_scores[field] *= 0.6
                    
                    # ë²”ìœ„ ì œí•œ
                    context.confidence_scores[field] = min(1.0, max(0.0, context.confidence_scores[field]))
        
        except Exception as e:
            self.logger.warning(f"ì‹ ë¢°ë„ ì ìˆ˜ ì¡°ì • ì‹¤íŒ¨: {e}")

# ==================== AI ê°•í™” ModularUnifiedCrawler ====================

class AIEnhancedModularUnifiedCrawler:
    """AI ê°•í™” ëª¨ë“ˆëŸ¬ í†µí•© í¬ë¡¤ëŸ¬"""
    
    def __init__(self, config_override=None, api_key=None, progress_callback=None):
        """ì´ˆê¸°í™”"""
        self.config = config_override or CRAWLING_CONFIG
        self.logger = LoggerUtils.setup_crawler_logger("ai_enhanced_modular_crawler")
        self.progress_callback = progress_callback
        
        # AI ë§¤ë‹ˆì € ì´ˆê¸°í™”
        try:
            self.ai_manager = AIModelManager()
        except Exception as e:
            self.logger.warning(f"AI ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ai_manager = None
        
        # ì „ë¬¸ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ë“¤ (ê¸°ì¡´ ìœ ì§€)
        self.fax_extractor = None
        self.phone_driver = None
        self.homepage_parser = None
        self.contact_validator = None
        self.ai_validator = None
        self.database = None
        
        # AI ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™” (ìˆ˜ì •: parent_crawler ì „ë‹¬)
        self.ai_agents = []
        if self.ai_manager:
            try:
                self.ai_agents = [
                    EnhancedHomepageSearchAgent(self.ai_manager, self.logger, self),
                    EnhancedHomepageAnalysisAgent(self.ai_manager, self.logger, self),
                    EnhancedContactExtractionAgent(self.ai_manager, self.logger, self),
                    AIVerificationAgent(self.ai_manager, self.logger, self)
                ]
                self.logger.info("âœ… AI ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                self.logger.error(f"âŒ AI ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.ai_agents = []
        else:
            self.logger.warning("âš ï¸ AI ë§¤ë‹ˆì € ì—†ìŒ - ê¸°ë³¸ ëª¨ë“ˆë§Œ ì‚¬ìš©")
        
        # í†µê³„ ì •ë³´
        self.stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "phone_extracted": 0,
            "fax_extracted": 0,
            "homepage_parsed": 0,
            "contacts_validated": 0,
            "saved_to_db": 0,
            "ai_enhanced": 0,
            "start_time": None,
            "end_time": None,
            "agent_stats": {agent.name: {"executed": 0, "success": 0} for agent in self.ai_agents}
        }
        
        self.logger.info("ğŸš€ AI ê°•í™” ëª¨ë“ˆëŸ¬ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def initialize_modules(self):
        """ì „ë¬¸ ëª¨ë“ˆë“¤ ì´ˆê¸°í™” (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        try:
            self.logger.info("ğŸ”§ ì „ë¬¸ ëª¨ë“ˆë“¤ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. íŒ©ìŠ¤ ì¶”ì¶œê¸° ì´ˆê¸°í™”
            if FAX_EXTRACTOR_AVAILABLE:
                try:
                    self.fax_extractor = FaxExtractor()
                    self.logger.info("âœ… íŒ©ìŠ¤ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ íŒ©ìŠ¤ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.fax_extractor = None
            
            # 2. ì „í™”ë²ˆí˜¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” (Selenium ë“œë¼ì´ë²„)
            if PHONE_EXTRACTOR_AVAILABLE:
                try:
                    self.phone_driver = setup_driver()
                    self.logger.info("âœ… ì „í™”ë²ˆí˜¸ ì¶”ì¶œê¸° ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ ì „í™”ë²ˆí˜¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.phone_driver = None
            
            # 3. í™ˆí˜ì´ì§€ íŒŒì„œ ì´ˆê¸°í™”
            if URL_EXTRACTOR_AVAILABLE:
                try:
                    self.homepage_parser = HomepageParser(headless=True)
                    self.logger.info("âœ… í™ˆí˜ì´ì§€ íŒŒì„œ ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ í™ˆí˜ì´ì§€ íŒŒì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.homepage_parser = None
            
            # 4. ê²€ì¦ê¸° ì´ˆê¸°í™”
            if VALIDATOR_AVAILABLE:
                try:
                    self.contact_validator = ContactValidator()
                    self.ai_validator = AIValidator()
                    self.logger.info("âœ… ì—°ë½ì²˜ ê²€ì¦ê¸° ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ ê²€ì¦ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.contact_validator = None
                    self.ai_validator = None
            
            # 5. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            if DATABASE_AVAILABLE:
                try:
                    self.database = get_database()
                    self.logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
                    self.database = None
            
            self.logger.info("ğŸ¯ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë“ˆ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    async def process_organizations(self, organizations: List[Dict], options: Dict = None) -> List[Dict]:
        """AI ê°•í™” ì¡°ì§ ì²˜ë¦¬"""
        if not organizations:
            self.logger.warning("ì²˜ë¦¬í•  ì¡°ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        options = options or {}
        self.stats["start_time"] = datetime.now()
        self.stats["total_processed"] = len(organizations)
        
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.initialize_modules()
        
        self.logger.info(f"ğŸ“Š ì´ {len(organizations)}ê°œ ì¡°ì§ AI ê°•í™” ì²˜ë¦¬ ì‹œì‘")
        
        results = []
        
        try:
            for i, org in enumerate(organizations, 1):
                try:
                    # AI ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ì¡°ì§ ì²˜ë¦¬
                    processed_org = await self.process_single_organization_with_ai(org, i)
                    results.append(processed_org)
                    
                    self.stats["successful"] += 1
                    if processed_org.get('ai_enhanced'):
                        self.stats["ai_enhanced"] += 1
                    
                    # ë”œë ˆì´
                    await asyncio.sleep(self.config.get("default_delay", 2))
                    
                except Exception as e:
                    self.logger.error(f"âŒ ì¡°ì§ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {org.get('name', 'Unknown')} - {e}")
                    self.stats["failed"] += 1
                    results.append(org)
        
        finally:
            # ëª¨ë“ˆ ì •ë¦¬
            self.cleanup_modules()
        
        self.stats["end_time"] = datetime.now()
        self.print_ai_enhanced_statistics()
        
        return results
    
    async def process_single_organization_with_ai(self, org: Dict, index: int) -> Dict:
        """AI ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ì¡°ì§ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            org_name = org.get('name', 'Unknown')
            self.logger.info(f"ğŸ¤– AI ì›Œí¬í”Œë¡œìš° ì‹œì‘ [{index}]: {org_name}")
            
            # AI ì—ì´ì „íŠ¸ ì²´ì¸ ì´ˆê¸°í™”
            context = CrawlingContext(
                organization=org,
                current_stage=CrawlingStage.INITIALIZATION,
                extracted_data={},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # AI ì—ì´ì „íŠ¸ ì²´ì¸ ì‹¤í–‰
            agents = [
                EnhancedHomepageSearchAgent(self.ai_manager, self.logger, self),
                EnhancedHomepageAnalysisAgent(self.ai_manager, self.logger, self),
                EnhancedContactExtractionAgent(self.ai_manager, self.logger, self),
                AIVerificationAgent(self.ai_manager, self.logger, self)
            ]
            
            for agent in agents:
                try:
                    if await agent.should_execute(context):
                        self.logger.info(f"ğŸ”„ ì—ì´ì „íŠ¸ ì‹¤í–‰: {agent.name}")
                        context = await agent.execute(context)
                    else:
                        self.logger.info(f"â­ï¸ ì—ì´ì „íŠ¸ ê±´ë„ˆë›°ê¸°: {agent.name}")
                except Exception as e:
                    # error ì†ì„± ëŒ€ì‹  error_logì— ì¶”ê°€
                    error_msg = f"{agent.name} ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
                    context.error_log.append(error_msg)
                    self.logger.error(f"âŒ {error_msg}")
                    continue
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            context.processing_time = time.time() - start_time
            
            # ê²°ê³¼ ê²°í•©
            result = self._combine_ai_results(org, context)
            
            # ì „í†µì ì¸ ëª¨ë“ˆë¡œ ë³´ì™„
            await self._supplement_with_traditional_modules(result, context)
            
            return result
            
        except Exception as e:
            # error ì†ì„± ëŒ€ì‹  ì§ì ‘ ì˜¤ë¥˜ ë©”ì‹œì§€ ë¡œê¹…
            error_msg = f"AI ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {org_name} - {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì²˜ë¦¬
            result = org.copy()
            result.update({
                'ai_enhanced': False,
                'processing_metadata': {
                    'extraction_method': 'fallback_error',
                    'error_message': str(e),
                    'timestamp': datetime.now().isoformat(),
                    'processing_time': time.time() - start_time
                }
            })
            return result
    
    def _combine_ai_results(self, original_org: Dict, context: CrawlingContext) -> Dict:
        """AI ê²°ê³¼ ì¡°í•©"""
        result = original_org.copy()
        
        # ì¶”ì¶œëœ ë°ì´í„° ì¶”ê°€
        result.update(context.extracted_data)
        
        # AI ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
        result['ai_insights'] = context.ai_insights
        result['confidence_scores'] = context.confidence_scores
        
        # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result['processing_metadata'] = {
            'processing_time': context.processing_time,
            'final_stage': context.current_stage.value,
            'error_count': len(context.error_log),
            'errors': context.error_log,
            'extraction_method': 'ai_enhanced_modular',
            'ai_enhanced': True,
            'timestamp': datetime.now().isoformat()
        }
        
        return result
    
    async def _supplement_with_traditional_modules(self, result: Dict, context: CrawlingContext):
        """ê¸°ì¡´ ëª¨ë“ˆë¡œ ë³´ì™„ ì²˜ë¦¬"""
        try:
            org_name = result.get('name', 'Unknown')
            
            # ì „í™”ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ëª¨ë“ˆë¡œ ì¶”ê°€ ì‹œë„
            if not result.get('phone') and self.phone_driver:
                try:
                    found_phones = search_phone_number(self.phone_driver, org_name)
                    if found_phones:
                        result['phone'] = found_phones[0]
                        result['phone_source'] = 'traditional_module_supplement'
                        self.stats["phone_extracted"] += 1
                except Exception as e:
                    self.logger.warning(f"ì „í™”ë²ˆí˜¸ ë³´ì™„ ì‹¤íŒ¨: {e}")
            
            # íŒ©ìŠ¤ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ëª¨ë“ˆë¡œ ì¶”ê°€ ì‹œë„
            if not result.get('fax') and self.fax_extractor:
                try:
                    found_faxes = self.fax_extractor.search_fax_number(org_name)
                    if found_faxes:
                        result['fax'] = found_faxes[0]
                        result['fax_source'] = 'traditional_module_supplement'
                        self.stats["fax_extracted"] += 1
                except Exception as e:
                    self.logger.warning(f"íŒ©ìŠ¤ë²ˆí˜¸ ë³´ì™„ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.error(f"ê¸°ì¡´ ëª¨ë“ˆ ë³´ì™„ ì˜¤ë¥˜: {e}")
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (í˜¸í™˜ì„±)
    def cleanup_modules(self):
        """ëª¨ë“ˆë“¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ ëª¨ë“ˆ ì •ë¦¬ ì‹œì‘...")
            
            if self.fax_extractor:
                try:
                    self.fax_extractor.close()
                except:
                    pass
            
            if self.phone_driver:
                try:
                    self.phone_driver.quit()
                except:
                    pass
            
            if self.homepage_parser:
                try:
                    self.homepage_parser.close_driver()
                except:
                    pass
            
            self.logger.info("ğŸ¯ ëª¨ë“  ëª¨ë“ˆ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë“ˆ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def save_to_database(self, org_data: Dict) -> Optional[Dict]:
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥/ì—…ë°ì´íŠ¸"""
        try:
            if not DATABASE_AVAILABLE:
                self.logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            from database.database import get_database
            db = get_database()
            
            # DB IDê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            db_id = org_data.get('db_id') or org_data.get('id')
            
            # ì—…ë°ì´íŠ¸í•  ë°ì´í„° ì¤€ë¹„
            update_data = {}
            
            # í¬ë¡¤ë§ìœ¼ë¡œ ì–»ì€ ìƒˆë¡œìš´ ì •ë³´ë§Œ ì—…ë°ì´íŠ¸
            if org_data.get('homepage') and org_data['homepage'] != '':
                update_data['homepage'] = org_data['homepage']
            
            if org_data.get('phone') and org_data['phone'] != '':
                update_data['phone'] = org_data['phone']
            
            if org_data.get('fax') and org_data['fax'] != '':
                update_data['fax'] = org_data['fax']
            
            if org_data.get('email') and org_data['email'] != '':
                update_data['email'] = org_data['email']
            
            # í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„° ì €ì¥
            crawling_data = {
                'last_crawled': datetime.now().isoformat(),
                'ai_enhanced': org_data.get('ai_enhanced', False),
                'extraction_method': org_data.get('processing_metadata', {}).get('extraction_method', 'unknown'),
                'confidence_scores': org_data.get('confidence_scores', {}),
                'ai_insights': org_data.get('ai_insights', {})
            }
            
            if org_data.get('crawling_data'):
                crawling_data.update(org_data['crawling_data'])
            
            update_data['crawling_data'] = json.dumps(crawling_data, ensure_ascii=False)
            update_data['updated_by'] = 'crawler_system'
            
            if db_id:
                # ê¸°ì¡´ ì¡°ì§ ì—…ë°ì´íŠ¸
                success = db.update_organization(db_id, update_data, 'crawler_system')
                if success:
                    self.logger.info(f"âœ… ì¡°ì§ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {org_data.get('name')} (ID: {db_id})")
                    return {'action': 'updated', 'id': db_id}
                else:
                    self.logger.error(f"âŒ ì¡°ì§ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {org_data.get('name')} (ID: {db_id})")
                    return None
            else:
                # ìƒˆë¡œìš´ ì¡°ì§ ìƒì„±
                org_data_for_db = {
                    'name': org_data.get('name', ''),
                    'category': org_data.get('category', 'ì¢…êµì‹œì„¤'),
                    'type': org_data.get('type', 'CHURCH'),
                    'homepage': org_data.get('homepage', ''),
                    'phone': org_data.get('phone', ''),
                    'fax': org_data.get('fax', ''),
                    'email': org_data.get('email', ''),
                    'address': org_data.get('address', ''),
                    'organization_size': org_data.get('organization_size', ''),
                    'denomination': org_data.get('denomination', ''),
                    'crawling_data': json.dumps(crawling_data, ensure_ascii=False),
                    'created_by': 'crawler_system',
                    'updated_by': 'crawler_system',
                    'lead_source': 'CRAWLER'
                }
                
                new_id = db.create_organization(org_data_for_db)
                if new_id:
                    self.logger.info(f"âœ… ìƒˆ ì¡°ì§ ìƒì„± ì™„ë£Œ: {org_data.get('name')} (ID: {new_id})")
                    return {'action': 'created', 'id': new_id}
                else:
                    self.logger.error(f"âŒ ìƒˆ ì¡°ì§ ìƒì„± ì‹¤íŒ¨: {org_data.get('name')}")
                    return None
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
    
    def print_ai_enhanced_statistics(self):
        """AI ê°•í™” í†µê³„ ì¶œë ¥"""
        duration = self.stats["end_time"] - self.stats["start_time"]
        
        print("\n" + "="*80)
        print("ğŸ¤– AI ê°•í™” ëª¨ë“ˆëŸ¬ í¬ë¡¤ë§ ì™„ë£Œ í†µê³„")
        print("="*80)
        print(f"ğŸ“‹ ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        print(f"âœ… ì„±ê³µ: {self.stats['successful']}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
        print(f"ğŸ¤– AI ê°•í™”: {self.stats['ai_enhanced']}ê°œ")
        print(f"ğŸ“ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ: {self.stats['phone_extracted']}ê°œ")
        print(f"ğŸ“  íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ: {self.stats['fax_extracted']}ê°œ")
        print(f"ğŸ’¾ DB ì €ì¥: {self.stats['saved_to_db']}ê°œ")
        
        if self.stats['total_processed'] > 0:
            success_rate = (self.stats['successful'] / self.stats['total_processed']) * 100
            ai_enhancement_rate = (self.stats['ai_enhanced'] / self.stats['total_processed']) * 100
            print(f"ğŸ“ˆ ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}%")
            print(f"ğŸ¤– AI ê°•í™”ìœ¨: {ai_enhancement_rate:.1f}%")
        
        print(f"â±ï¸ ì†Œìš”ì‹œê°„: {duration}")
        
        # AI ì—ì´ì „íŠ¸ë³„ í†µê³„
        print(f"\nğŸ¤– AI ì—ì´ì „íŠ¸ë³„ ì„±ê³µë¥ :")
        for agent_name, stats in self.stats["agent_stats"].items():
            executed = stats["executed"]
            success = stats["success"]
            success_rate = (success / executed * 100) if executed > 0 else 0
            print(f"  - {agent_name}: {executed}íšŒ ì‹¤í–‰, {success}íšŒ ì„±ê³µ ({success_rate:.1f}%)")
        
        print("="*80)
    
    # ==================== ContactEnrichmentService í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ====================
    
    async def search_homepage(self, org_name: str) -> Dict[str, Any]:
        """í™ˆí˜ì´ì§€ ê²€ìƒ‰ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            self.logger.info(f"ğŸ” AI í™ˆí˜ì´ì§€ ê²€ìƒ‰: {org_name}")
            
            # AI ì—ì´ì „íŠ¸ë¥¼ í†µí•œ í™ˆí˜ì´ì§€ ê²€ìƒ‰
            context = CrawlingContext(
                organization={'name': org_name},
                current_stage=CrawlingStage.HOMEPAGE_SEARCH,
                extracted_data={},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì‹¤í–‰
            homepage_agent = EnhancedHomepageSearchAgent(self.ai_manager, self.logger, self)
            context = await homepage_agent.execute(context)
            
            if context.extracted_data.get('homepage'):
                return {
                    "homepage": context.extracted_data['homepage'],
                    "homepage_type": context.extracted_data.get('homepage_type', 'ì•Œìˆ˜ì—†ìŒ'),
                    "confidence": context.confidence_scores.get('homepage', 0.5),
                    "status": "success",
                    "ai_enhanced": True
                }
            else:
                return {
                    "homepage": "",
                    "status": "not_found",
                    "message": "AI ê¸°ë°˜ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                }
            
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {org_name} - {e}")
            return {"status": "error", "error": str(e)}
    
    async def extract_details_from_homepage(self, homepage_url: str) -> Dict[str, Any]:
        """í™ˆí˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            self.logger.info(f"ğŸŒ AI í™ˆí˜ì´ì§€ ë¶„ì„: {homepage_url}")
            
            # AI ì—ì´ì „íŠ¸ë¥¼ í†µí•œ í™ˆí˜ì´ì§€ ë¶„ì„
            context = CrawlingContext(
                organization={'name': 'Unknown'},
                current_stage=CrawlingStage.HOMEPAGE_ANALYSIS,
                extracted_data={'homepage': homepage_url},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # í™ˆí˜ì´ì§€ ë¶„ì„ ì—ì´ì „íŠ¸ ì‹¤í–‰
            analysis_agent = EnhancedHomepageAnalysisAgent(self.ai_manager, self.logger, self)
            context = await analysis_agent.execute(context)
            
            return {
                "phone": context.extracted_data.get("phone", ""),
                "fax": context.extracted_data.get("fax", ""),
                "email": context.extracted_data.get("email", ""),
                "address": context.extracted_data.get("address", ""),
                "ai_insights": context.ai_insights,
                "confidence_scores": context.confidence_scores,
                "status": "success",
                "ai_enhanced": True
            }
                
        except Exception as e:
            self.logger.error(f"AI í™ˆí˜ì´ì§€ ë¶„ì„ ì˜¤ë¥˜: {homepage_url} - {e}")
            return {"status": "error", "error": str(e)}
    
    async def search_missing_info(self, org_name: str, missing_fields: List[str]) -> Dict[str, str]:
        """ëˆ„ë½ëœ ì •ë³´ ê²€ìƒ‰ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            self.logger.info(f"ğŸ” AI ëˆ„ë½ ì •ë³´ ê²€ìƒ‰: {org_name} - {missing_fields}")
            
            results = {}
            
            # AI ì—ì´ì „íŠ¸ë¥¼ í†µí•œ ì—°ë½ì²˜ ì¶”ì¶œ
            context = CrawlingContext(
                organization={'name': org_name},
                current_stage=CrawlingStage.CONTACT_EXTRACTION,
                extracted_data={},
                ai_insights={},
                error_log=[],
                processing_time=0,
                confidence_scores={}
            )
            
            # ì—°ë½ì²˜ ì¶”ì¶œ ì—ì´ì „íŠ¸ ì‹¤í–‰
            contact_agent = EnhancedContactExtractionAgent(self.ai_manager, self.logger, self)
            context = await contact_agent.execute(context)
            
            # ìš”ì²­ëœ í•„ë“œë§Œ ë°˜í™˜
            for field in missing_fields:
                if field in context.extracted_data:
                    results[field] = context.extracted_data[field]
                    self.logger.info(f"  ğŸ¯ AIë¡œ {field} ë°œê²¬: {results[field]}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"AI ëˆ„ë½ ì •ë³´ ê²€ìƒ‰ ì˜¤ë¥˜: {org_name} - {e}")
            return {}
    
    def validate_and_clean_data(self, org_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬ - ContactEnrichmentService í˜¸í™˜ì„± (AI ê°•í™”)"""
        try:
            cleaned_data = org_data.copy()
            
            # ê¸°ì¡´ validator.py ì‚¬ìš© (ìœ ì§€)
            if self.contact_validator:
                # ì „í™”ë²ˆí˜¸ ê²€ì¦
                if cleaned_data.get('phone'):
                    is_valid, validated_phone = self.contact_validator.validate_phone_number(cleaned_data['phone'])
                    if is_valid:
                        cleaned_data['phone'] = validated_phone
                        cleaned_data['phone_validation'] = 'valid'
                    else:
                        self.logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì „í™”ë²ˆí˜¸: {cleaned_data['phone']}")
                        cleaned_data['phone_validation'] = 'invalid'
                
                # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ì¦
                if cleaned_data.get('fax'):
                    is_valid, validated_fax = self.contact_validator.validate_fax_number(cleaned_data['fax'])
                    if is_valid:
                        cleaned_data['fax'] = validated_fax
                        cleaned_data['fax_validation'] = 'valid'
                    else:
                        self.logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ íŒ©ìŠ¤ë²ˆí˜¸: {cleaned_data['fax']}")
                        cleaned_data['fax_validation'] = 'invalid'
            
            # AI ê²€ì¦ ì¶”ê°€
            if self.ai_manager:
                try:
                    # AI ê²€ì¦ ì—ì´ì „íŠ¸ ì‹¤í–‰
                    context = CrawlingContext(
                        organization={'name': cleaned_data.get('name', 'Unknown')},
                        current_stage=CrawlingStage.AI_VERIFICATION,
                        extracted_data=cleaned_data,
                        ai_insights={},
                        error_log=[],
                        processing_time=0,
                        confidence_scores={}
                    )
                    
                    # AI ê²€ì¦ ì—ì´ì „íŠ¸ ì‹¤í–‰ (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ)
                    verification_agent = AIVerificationAgent(self.ai_manager, self.logger, self)
                    # ë¹„ë™ê¸° í˜¸ì¶œì„ ë™ê¸°ë¡œ ë³€í™˜
                    import asyncio
                    if asyncio.get_event_loop().is_running():
                        # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
                        task = asyncio.create_task(verification_agent.execute(context))
                        # ê°„ë‹¨í•œ AI ê²€ì¦ë§Œ ìˆ˜í–‰
                        cleaned_data['ai_validation_attempted'] = True
                    else:
                        context = asyncio.run(verification_agent.execute(context))
                        cleaned_data['ai_insights'] = context.ai_insights
                        cleaned_data['confidence_scores'] = context.confidence_scores
                
                except Exception as e:
                    self.logger.warning(f"AI ê²€ì¦ ì‹¤íŒ¨: {e}")
                    cleaned_data['ai_validation_error'] = str(e)
            
            return cleaned_data
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜: {e}")
            return org_data
    
    # ==================== app.py í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ====================
    
    async def process_json_file_async(self, json_file_path: str, test_mode: bool = False, test_count: int = 10, progress_callback=None) -> List[Dict]:
        """ğŸ”§ app.py í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ (AI ê°•í™”)"""
        try:
            # JSON íŒŒì¼ ë¡œë“œ
            data = FileUtils.load_json(json_file_path)
            
            # ë°ì´í„° ì „ì²˜ë¦¬ (app.pyì™€ ë™ì¼í•œ ë°©ì‹)
            organizations = []
            if isinstance(data, dict):
                for category, orgs in data.items():
                    if isinstance(orgs, list):
                        for org in orgs:
                            if isinstance(org, dict):
                                org["category"] = category
                                organizations.append(org)
            elif isinstance(data, list):
                organizations = [org for org in data if isinstance(org, dict)]
            
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
            if test_mode and test_count:
                organizations = organizations[:test_count]
            
            # progress_callback ì €ì¥
            self.progress_callback = progress_callback
            
            # AI ê°•í™” ì²˜ë¦¬ ì‹¤í–‰
            results = await self.process_organizations(organizations)
            
            return results
            
        except Exception as e:
            self.logger.error(f"JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

    async def _fallback_to_traditional_processing(self, context: CrawlingContext):
        """AI ì—ì´ì „íŠ¸ ì—†ì„ ë•Œ ê¸°ë³¸ ëª¨ë“ˆ ì²˜ë¦¬"""
        try:
            org_name = context.organization.get('name', 'Unknown')
            self.logger.info(f"ğŸ”§ ê¸°ë³¸ ëª¨ë“ˆ ì²˜ë¦¬: {org_name}")
            
            # ê¸°ë³¸ í™ˆí˜ì´ì§€ ê²€ìƒ‰ (url_extractor ì‚¬ìš©)
            if self.homepage_parser:
                try:
                    # ê°„ë‹¨í•œ í™ˆí˜ì´ì§€ ê²€ìƒ‰ ë¡œì§
                    pass
                except Exception as e:
                    self.logger.warning(f"í™ˆí˜ì´ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ ì „í™”ë²ˆí˜¸ ê²€ìƒ‰ (phone_extractor ì‚¬ìš©)
            if self.phone_driver:
                try:
                    found_phones = search_phone_number(self.phone_driver, org_name)
                    if found_phones:
                        context.extracted_data['phone'] = found_phones[0]
                        self.update_confidence(context, 'phone', 0.7)
                except Exception as e:
                    self.logger.warning(f"ì „í™”ë²ˆí˜¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ (fax_extractor ì‚¬ìš©)
            if self.fax_extractor:
                try:
                    found_faxes = self.fax_extractor.search_fax_number(org_name)
                    if found_faxes:
                        context.extracted_data['fax'] = found_faxes[0]
                        self.update_confidence(context, 'fax', 0.7)
                except Exception as e:
                    self.logger.warning(f"íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.error(f"ê¸°ë³¸ ëª¨ë“ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _fallback_to_traditional_processing_simple(self, org: Dict) -> Dict:
        """ê°„ë‹¨í•œ ê¸°ë³¸ ì²˜ë¦¬ (ì‹¤íŒ¨ ì‹œ ëŒ€ì²´)"""
        result = org.copy()
        result['processing_metadata'] = {
            'extraction_method': 'fallback_traditional',
            'ai_enhanced': False,
            'timestamp': datetime.now().isoformat(),
            'status': 'fallback_processed'
        }
        return result

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ (ModularUnifiedCrawlerë¥¼ AI ê°•í™” ë²„ì „ìœ¼ë¡œ êµì²´)
ModularUnifiedCrawler = AIEnhancedModularUnifiedCrawler

# ==================== í¸ì˜ í•¨ìˆ˜ë“¤ ====================

async def crawl_ai_enhanced_from_file(input_file: str, options: Dict = None) -> List[Dict]:
    """íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ AI ê°•í™” í¬ë¡¤ë§"""
    try:
        # íŒŒì¼ ë¡œë“œ
        data = FileUtils.load_json(input_file)
        
        # AI ê°•í™” í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
        crawler = AIEnhancedModularUnifiedCrawler()
        results = await crawler.process_organizations(data, options)
        
        return results
        
    except Exception as e:
        logging.error(f"AI ê°•í™” íŒŒì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

async def crawl_ai_enhanced_latest_file(options: Dict = None) -> List[Dict]:
    """ìµœì‹  ì…ë ¥ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ AI ê°•í™” í¬ë¡¤ë§"""
    try:
        latest_file = get_latest_input_file()
        if not latest_file:
            raise ValueError("ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ“‚ ìµœì‹  íŒŒì¼ ì‚¬ìš©: {latest_file}")
        return await crawl_ai_enhanced_from_file(str(latest_file), options)
        
    except Exception as e:
        logging.error(f"AI ê°•í™” ìµœì‹  íŒŒì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

async def crawl_ai_enhanced_from_database(options: Dict = None) -> List[Dict]:
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ AI ê°•í™” í¬ë¡¤ë§ (ìˆ˜ì •ëœ ë²„ì „)"""
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        from database.database import get_database
        db = get_database()
        
        # ì¡°ì§ ë°ì´í„° ì¡°íšŒ (ë” ì •êµí•œ ì¡°ê±´)
        with db.get_connection() as conn:
            query = """
            SELECT id, name, type, category, subcategory, homepage, phone, fax, email, 
                   mobile, postal_code, address, organization_size, denomination
            FROM organizations 
            WHERE is_active = 1 
            AND (
                (homepage = '' OR homepage IS NULL) OR
                (phone = '' OR phone IS NULL) OR 
                (fax = '' OR fax IS NULL) OR
                (email = '' OR email IS NULL)
            )
            AND name IS NOT NULL AND name != ''
            ORDER BY updated_at DESC
            LIMIT 500
            """
            cursor = conn.execute(query)
            rows = cursor.fetchall()
            
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            organizations = []
            for row in rows:
                org = dict(row)
                organizations.append({
                    'db_id': org.get('id'),
                    'name': org.get('name', ''),
                    'type': org.get('type', 'CHURCH'),
                    'category': org.get('category', 'ì¢…êµì‹œì„¤'),
                    'subcategory': org.get('subcategory', ''),
                    'homepage': org.get('homepage', ''),
                    'phone': org.get('phone', ''),
                    'fax': org.get('fax', ''),
                    'email': org.get('email', ''),
                    'mobile': org.get('mobile', ''),
                    'postal_code': org.get('postal_code', ''),
                    'address': org.get('address', ''),
                    'organization_size': org.get('organization_size', ''),
                    'denomination': org.get('denomination', '')
                })
        
        if not organizations:
            print("ğŸ“‹ í¬ë¡¤ë§ì´ í•„ìš”í•œ ì¡°ì§ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ğŸ“‚ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {len(organizations)}ê°œ ì¡°ì§ ë¡œë“œ")
        
        # AI ê°•í™” í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
        crawler = AIEnhancedModularUnifiedCrawler()
        results = await crawler.process_organizations(organizations, options)
        
        # ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë°ì´íŠ¸
        updated_count = 0
        for result in results:
            if result.get('db_id'):
                success = await crawler.save_to_database(result)
                if success:
                    updated_count += 1
        
        print(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸: {updated_count}ê°œ ì¡°ì§")
        
        return results
        
    except Exception as e:
        logging.error(f"AI ê°•í™” ë°ì´í„°ë² ì´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (AI ê°•í™”)"""
    print("ğŸ¤– AI ê°•í™” ëª¨ë“ˆëŸ¬ í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*80)
    print("ğŸš€ AI ê¸°ëŠ¥ ìµœëŒ€ í™œìš© ëª¨ë“œ")
    print("ğŸ“¦ ì „ë¬¸ ëª¨ë“ˆ + AI ì—ì´ì „íŠ¸ í†µí•©:")
    print(f"  - fax_extractor.py: {'âœ…' if FAX_EXTRACTOR_AVAILABLE else 'âŒ'}")
    print(f"  - phone_extractor.py: {'âœ…' if PHONE_EXTRACTOR_AVAILABLE else 'âŒ'}")
    print(f"  - url_extractor.py: {'âœ…' if URL_EXTRACTOR_AVAILABLE else 'âŒ'}")
    print(f"  - validator.py: {'âœ…' if VALIDATOR_AVAILABLE else 'âŒ'}")
    print(f"  - database.py: {'âœ…' if DATABASE_AVAILABLE else 'âŒ'}")
    print("ğŸ¤– AI ì—ì´ì „íŠ¸:")
    print("  - EnhancedHomepageSearchAgent: AI ê¸°ë°˜ ì¢…í•© í™ˆí˜ì´ì§€ ê²€ìƒ‰")
    print("  - EnhancedHomepageAnalysisAgent: AI ê¸°ë°˜ í™ˆí˜ì´ì§€ ë¶„ì„")
    print("  - EnhancedContactExtractionAgent: AI ê°•í™” ì—°ë½ì²˜ ì¶”ì¶œ")
    print("  - AIVerificationAgent: AI ì¢…í•© ê²€ì¦")
    print("="*80)
    
    try:
        # í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
        initialize_project()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
        if not DATABASE_AVAILABLE:
            print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìš°ì„  ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í¬ë¡¤ë§ ì‹œë„
        print("ğŸ—ƒï¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ ì¡°íšŒ ì¤‘...")
        results = await crawl_ai_enhanced_from_database()
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ ì‹œë„
        if not results:
            print("ğŸ“ ì…ë ¥ íŒŒì¼ì—ì„œ í¬ë¡¤ë§ ì‹œë„ ì¤‘...")
            try:
                results = await crawl_ai_enhanced_latest_file()
            except Exception as e:
                print(f"âš ï¸ ì…ë ¥ íŒŒì¼ í¬ë¡¤ë§ë„ ì‹¤íŒ¨: {e}")
        
        if results:
            ai_enhanced_count = sum(1 for r in results if r.get('ai_enhanced'))
            print(f"\nâœ… AI ê°•í™” í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ì¡°ì§ ì²˜ë¦¬")
            print(f"ğŸ¤– AI ê°•í™”ëœ ì¡°ì§: {ai_enhanced_count}ê°œ")
            print(f"ğŸ“ˆ AI ê°•í™”ìœ¨: {(ai_enhanced_count/len(results)*100):.1f}%")
        else:
            print("\nâŒ AI ê°•í™” í¬ë¡¤ë§ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main())