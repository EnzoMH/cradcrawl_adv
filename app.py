#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì›¹ ê¸°ë°˜ í¬ë¡¤ë§ ì œì–´ ì‹œìŠ¤í…œ
FastAPI + HTML/CSS/JSë¥¼ í™œìš©í•œ í¬ë¡¤ë§ ê²°ê³¼ í™•ì¸ ë° ì œì–´
"""

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import json
import asyncio
import threading
import time
import logging
from datetime import datetime
from typing import List, Dict, Optional
import uvicorn
from pydantic import BaseModel

import os
from dotenv import load_dotenv
load_dotenv()
print(f"ğŸ”‘ .env ë¡œë“œ ì™„ë£Œ: GEMINI_API_KEY={'ì„¤ì •ë¨' if os.getenv('GEMINI_API_KEY') else 'ì—†ìŒ'}")

# âœ… ìˆ˜ì •: enhanced_detail_extractor ì‚¬ìš©
from crawler_main import CrawlerMain
# í†µê³„ ë¶„ì„ì€ ê¸°ì¡´ ì‚¬ìš©

from legacy.data_statistics import DataStatisticsAnalyzer

# âœ… ìˆ˜ì •: ìœ í‹¸ë¦¬í‹° í™œìš©
from utils.logger_utils import LoggerUtils
from utils.file_utils import FileUtils

# ğŸ†• ì¶”ê°€: ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ ëª¨ë¸
class CrawlingResult(BaseModel):
    """ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ ëª¨ë¸"""
    name: str
    category: str = ""
    phone: str = ""
    fax: str = ""
    email: str = ""
    postal_code: str = ""
    address: str = ""
    homepage_url: str = ""
    status: str = "processing"  # processing, completed, failed
    processed_at: str = ""
    error_message: str = ""
    current_step: str = ""
    processing_time: str = ""
    extraction_method: str = ""

class CrawlingProgress(BaseModel):
    """í¬ë¡¤ë§ ì§„í–‰ ìƒí™© ëª¨ë¸"""
    current_index: int
    total_count: int
    current_organization: str
    percentage: float
    latest_result: Optional[CrawlingResult] = None
    status: str = "idle"  # idle, running, completed, stopped, error

class CrawlingConfig(BaseModel):
    mode: str = "enhanced"  # "enhanced" ë˜ëŠ” "legacy"
    test_mode: bool = False
    test_count: int = 10
    use_ai: bool = True

# ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (âœ… ìˆ˜ì •: LoggerUtils í™œìš©)
logger = LoggerUtils.setup_app_logger()

app = FastAPI(title="í–¥ìƒëœ í¬ë¡¤ë§ ì œì–´ ì‹œìŠ¤í…œ")

# ì •ì  íŒŒì¼ ë° í…œí”Œë¦¿ ì„¤ì •
app.mount("/static/css", StaticFiles(directory="templates/css"), name="css")
app.mount("/static/js", StaticFiles(directory="templates/js"), name="js")
templates = Jinja2Templates(directory="templates/html")

# ê¸°íƒ€ ì •ì  íŒŒì¼ë“¤ (ë‹¤ìš´ë¡œë“œ íŒŒì¼ ë“±)ì„ ìœ„í•œ static ë””ë ‰í† ë¦¬
if os.path.exists("static"):
    app.mount("/static/files", StaticFiles(directory="static"), name="files")

# ğŸ”§ ìˆ˜ì •: ì „ì—­ ë³€ìˆ˜ ê°œì„ 
extractor_instance = None  # Enhanced Detail Extractor ì¸ìŠ¤í„´ìŠ¤
total_organizations = []
crawling_results = []  # ê¸°ì¡´ ê²°ê³¼ ì €ì¥ (í•˜ìœ„ í˜¸í™˜ì„±)
current_data_file = None  # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„° íŒŒì¼

# ğŸ†• ì¶”ê°€: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ê´€ë¦¬
crawling_progress = CrawlingProgress(
    current_index=0,
    total_count=0,
    current_organization="",
    percentage=0.0,
    status="idle"
)
real_time_results = []  # ì‹¤ì‹œê°„ ê²°ê³¼ ì €ì¥

# ê¸°ì¡´ crawling_status (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
crawling_status = {
    "is_running": False,
    "processed_count": 0,
    "total_count": 0,
    "current_organization": "",
    "extraction_mode": "enhanced"
}

def progress_callback(result: dict):
    """ğŸ†• ì¶”ê°€: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜"""
    global crawling_progress, real_time_results, crawling_status
    
    try:
        # ì‹¤ì‹œê°„ ê²°ê³¼ì— ì¶”ê°€
        crawling_result = CrawlingResult(**result)
        
        # ê¸°ì¡´ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€
        existing_index = -1
        for i, existing_result in enumerate(real_time_results):
            if existing_result.name == crawling_result.name:
                existing_index = i
                break
        
        if existing_index >= 0:
            real_time_results[existing_index] = crawling_result
        else:
            real_time_results.append(crawling_result)
        
        # ì™„ë£Œëœ ê¸°ê´€ë§Œ ì¹´ìš´íŠ¸
        if result.get("status") == "completed":
            crawling_progress.current_index = len([r for r in real_time_results if r.status == "completed"])
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        crawling_progress.current_organization = result.get("name", "")
        crawling_progress.percentage = (crawling_progress.current_index / crawling_progress.total_count) * 100 if crawling_progress.total_count > 0 else 0
        crawling_progress.latest_result = crawling_result
        
        # ê¸°ì¡´ crawling_statusë„ ì—…ë°ì´íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)
        crawling_status["processed_count"] = crawling_progress.current_index
        crawling_status["current_organization"] = result.get("name", "")
        
        logger.info(f"ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸: {result.get('name')} - {result.get('status')} - {result.get('current_step', '')}")
        
    except Exception as e:
        logger.error(f"ì§„í–‰ ìƒí™© ì½œë°± ì˜¤ë¥˜: {e}")

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€"""
    logger.info("ë©”ì¸ í˜ì´ì§€ ìš”ì²­")
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/api/status")
async def get_status():
    """í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    logger.debug(f"ìƒíƒœ ì¡°íšŒ ìš”ì²­: {crawling_status}")
    return crawling_status

@app.post("/api/start-enhanced-crawling")
async def start_enhanced_crawling(config: CrawlingConfig):
    """âœ… ìƒˆë¡œìš´: Enhanced Detail Extractor í¬ë¡¤ë§ ì‹œì‘"""
    global extractor_instance, total_organizations, crawling_status, crawling_results, crawling_progress, real_time_results, current_data_file
    
    logger.info(f"Enhanced í¬ë¡¤ë§ ì‹œì‘ ìš”ì²­: {config}")
    
    try:
        # âœ… ìˆ˜ì •: ë°ì´í„° íŒŒì¼ ë¡œë“œ
        logger.info("ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹œì‘")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸ ë° ë¡œë“œ
        if os.path.exists("raw_data_0530.json"):
            current_data_file = "raw_data_0530.json"
            data = FileUtils.load_json(current_data_file)
        elif os.path.exists("raw_data.json"):
            current_data_file = "raw_data.json"
            data = FileUtils.load_json(current_data_file)
        else:
            raise HTTPException(status_code=404, detail="raw_data.json ë˜ëŠ” raw_data_0530.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if not data:
            raise HTTPException(status_code=500, detail="ë°ì´í„° íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"ì‚¬ìš©í•  ë°ì´í„° íŒŒì¼: {current_data_file}")
        
        # ğŸ”§ í•µì‹¬ ìˆ˜ì •: ë°ì´í„° êµ¬ì¡° ìë™ ê°ì§€ ë° ì²˜ë¦¬
        total_organizations = []
        
        if isinstance(data, dict):
            # Dictionary êµ¬ì¡°: {"ì¹´í…Œê³ ë¦¬": [ê¸°ê´€ë“¤]}
            logger.info(f"Dictionary êµ¬ì¡° ë°ì´í„° ì²˜ë¦¬: {len(data)}ê°œ ì¹´í…Œê³ ë¦¬")
            for category, orgs in data.items():
                if isinstance(orgs, list):
                    for org in orgs:
                        if isinstance(org, dict):
                            org["category"] = category
                            total_organizations.append(org)
                        
        elif isinstance(data, list):
            # List êµ¬ì¡°: [{"name": "ê¸°ê´€", "category": "ì¹´í…Œê³ ë¦¬"}]
            logger.info(f"List êµ¬ì¡° ë°ì´í„° ì²˜ë¦¬: {len(data)}ê°œ ê¸°ê´€")
            total_organizations = [org for org in data if isinstance(org, dict)]
            
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° êµ¬ì¡°ì…ë‹ˆë‹¤. Dictionary ë˜ëŠ” List í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        logger.info(f"ì´ {len(total_organizations)}ê°œ ê¸°ê´€ ë¡œë“œ ì™„ë£Œ")
        
        # ğŸ”§ ìˆ˜ì •: Enhanced Detail Extractor ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì½œë°± í¬í•¨)
        logger.info("Enhanced Detail Extractor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        api_key = os.getenv('GEMINI_API_KEY') if config.use_ai else None
        extractor_instance = CrawlerMain(
            api_key=api_key, 
            progress_callback=progress_callback  # ğŸ†• ì¶”ê°€: ì½œë°± í•¨ìˆ˜ ì „ë‹¬
        )
        
        # ğŸ”§ ìˆ˜ì •: ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
        real_time_results = []
        crawling_progress = CrawlingProgress(
            current_index=0,
            total_count=len(total_organizations),
            current_organization="",
            percentage=0.0,
            status="running"
        )
        
        # ìƒíƒœ ì´ˆê¸°í™” (ê¸°ì¡´ í˜¸í™˜ì„±)
        crawling_results = []
        crawling_status.update({
            "is_running": True,
            "processed_count": 0,
            "total_count": len(total_organizations),
            "current_organization": "",
            "extraction_mode": "enhanced"
        })
        
        logger.info(f"í¬ë¡¤ë§ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ: ì´ {len(total_organizations)}ê°œ ê¸°ê´€")
        
        # âœ… ìˆ˜ì •: ë°±ê·¸ë¼ìš´ë“œì—ì„œ Enhanced í¬ë¡¤ë§ ì‹œì‘
        logger.info("ë°±ê·¸ë¼ìš´ë“œ Enhanced í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘")
        threading.Thread(target=run_enhanced_crawling, args=(config,), daemon=True).start()
        
        logger.info("Enhanced í¬ë¡¤ë§ ì‹œì‘ ì™„ë£Œ")
        return {
            "message": "Enhanced í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", 
            "total_count": len(total_organizations),
            "mode": "enhanced",
            "ai_enabled": config.use_ai
        }
        
    except Exception as e:
        logger.error(f"Enhanced í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")

def run_enhanced_crawling(config: CrawlingConfig):
    """âœ… ìƒˆë¡œìš´: Enhanced Detail Extractor ì‹¤í–‰"""
    global crawling_status, total_organizations, crawling_results, extractor_instance, crawling_progress, current_data_file
    
    logger.info(f"Enhanced í¬ë¡¤ë§ ì‹œì‘: ì´ê¸°ê´€ìˆ˜={len(total_organizations)}")
    
    try:
        # ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        # ì²˜ë¦¬í•  ê¸°ê´€ ìˆ˜ ê²°ì •
        process_count = config.test_count if config.test_mode else len(total_organizations)
        organizations_to_process = total_organizations[:process_count]
        
        # âœ… Enhanced Detail Extractor ë¹„ë™ê¸° ì‹¤í–‰
        results = loop.run_until_complete(
            extractor_instance.process_json_file_async(
                json_file_path=current_data_file,  # ğŸ”§ ìˆ˜ì •: ì‹¤ì œ íŒŒì¼ëª… ì‚¬ìš©
                test_mode=config.test_mode,
                test_count=config.test_count
            )
        )
        
        if results:
            crawling_results = results
            
            # âœ… ê²°ê³¼ ìë™ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_file = f"enhanced_results_{timestamp}.json"
            excel_file = f"enhanced_report_{timestamp}.xlsx"
            
            # JSON ì €ì¥
            extractor_instance.save_merged_results_to_json(results, json_file)
            
            # Excel ì €ì¥
            extractor_instance.create_final_excel_report(results, excel_file)
            
            logger.info(f"Enhanced í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ì €ì¥")
        
        # í¬ë¡¤ë§ ì™„ë£Œ
        crawling_status["is_running"] = False
        crawling_status["current_organization"] = ""
        crawling_progress.status = "completed"
        
        logger.info("Enhanced í¬ë¡¤ë§ ì™„ë£Œ")
        print("Enhanced í¬ë¡¤ë§ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"Enhanced í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"Enhanced í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        crawling_status["is_running"] = False
        crawling_status["current_organization"] = ""
        crawling_progress.status = "error"

# ğŸ†• ì¶”ê°€: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© API
@app.get("/api/progress")
async def get_progress():
    """ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì§„í–‰ ìƒí™© ì¡°íšŒ"""
    return crawling_progress

# ğŸ†• ì¶”ê°€: ì‹¤ì‹œê°„ ê²°ê³¼ API
@app.get("/api/real-time-results")
async def get_real_time_results(limit: int = 10):
    """ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ (ìµœì‹  Nê°œ)"""
    return {
        "results": real_time_results[-limit:] if real_time_results else [],
        "total_count": len(real_time_results),
        "progress": crawling_progress
    }

# ğŸ†• ì¶”ê°€: ì „ì²´ ì‹¤ì‹œê°„ ê²°ê³¼ ì¡°íšŒ
@app.get("/api/all-real-time-results")
async def get_all_real_time_results():
    """ëª¨ë“  ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    return {
        "results": real_time_results,
        "total_count": len(real_time_results),
        "completed_count": len([r for r in real_time_results if r.status == "completed"]),
        "failed_count": len([r for r in real_time_results if r.status == "failed"]),
        "progress": crawling_progress
    }

# ğŸ†• ì¶”ê°€: íŠ¹ì • ê¸°ê´€ ê²°ê³¼ ì¡°íšŒ
@app.get("/api/result/{organization_name}")
async def get_organization_result(organization_name: str):
    """íŠ¹ì • ê¸°ê´€ì˜ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    for result in real_time_results:
        if result.name == organization_name:
            return result
    raise HTTPException(status_code=404, detail="ê¸°ê´€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

@app.post("/api/start-crawling")
async def start_crawling(config: CrawlingConfig):
    """ê¸°ì¡´ í¬ë¡¤ë§ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    # Enhanced í¬ë¡¤ë§ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    return await start_enhanced_crawling(config)

@app.get("/api/results")
async def get_results():
    """ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    global crawling_results
    logger.debug(f"ì‹¤ì‹œê°„ ê²°ê³¼ ì¡°íšŒ: {len(crawling_results)}ê°œ í•­ëª©")
    return {"results": crawling_results, "count": len(crawling_results)}

@app.get("/api/latest-result")
async def get_latest_result():
    """ìµœì‹  í¬ë¡¤ë§ ê²°ê³¼ 1ê°œ ì¡°íšŒ"""
    global crawling_results
    if crawling_results:
        latest = crawling_results[-1]
        logger.debug(f"ìµœì‹  ê²°ê³¼ ì¡°íšŒ: {latest.get('ê¸°ê´€ëª…', 'Unknown')}")
        return {"result": latest, "index": len(crawling_results) - 1}
    return {"result": None, "index": -1}

@app.post("/api/stop-crawling")
async def stop_crawling():
    """í¬ë¡¤ë§ ì¤‘ì§€"""
    global extractor_instance, crawling_status, crawling_progress
    
    logger.info("í¬ë¡¤ë§ ì¤‘ì§€ ìš”ì²­")
    crawling_status["is_running"] = False
    crawling_progress.status = "stopped"
    
    # Enhanced Detail ExtractorëŠ” ë³„ë„ ì¢…ë£Œ ë¡œì§ ì—†ìŒ (ì„¸ì…˜ ê¸°ë°˜)
    logger.info("í¬ë¡¤ë§ ì¤‘ì§€ ì™„ë£Œ")
    return {"message": "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."}

@app.get("/api/download-results")
async def download_results():
    """ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ì œê³µ"""
    try:
        # âœ… ìˆ˜ì •: Enhanced ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
        enhanced_files = [f for f in os.listdir(".") if f.startswith("enhanced_results_")]
        legacy_files = [f for f in os.listdir(".") if f.startswith("raw_data_with_contacts_")]
        
        result_files = enhanced_files + legacy_files
        
        if not result_files:
            raise HTTPException(status_code=404, detail="ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        latest_file = max(result_files, key=lambda x: os.path.getctime(x))
        return {"filename": latest_file, "download_url": f"/static/files/{latest_file}"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/extractor-stats")
async def get_extractor_stats():
    """âœ… ìƒˆë¡œìš´: Enhanced Detail Extractor í†µê³„ ì¡°íšŒ"""
    global extractor_instance
    
    if not extractor_instance:
        return {"error": "Extractor ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    return {
        "stats": extractor_instance.stats,
        "ai_enabled": extractor_instance.use_ai,
        "total_processed": len(crawling_results),
        "real_time_count": len(real_time_results),
        "progress": crawling_progress
    }

# ê¸°ì¡´ í†µê³„ APIë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€...
@app.get('/api/statistics')
def get_statistics():
    """ë°ì´í„° í†µê³„ API (ê¸°ì¡´ ìœ ì§€)"""
    try:
        analyzer = DataStatisticsAnalyzer()
        files = analyzer.find_latest_files()
        
        if not files['json'] and not files['excel']:
            return JSONResponse({
                'status': 'error',
                'message': 'ë¶„ì„í•  ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'
            }, status_code=404)
        
        # JSON ë°ì´í„° ë¶„ì„
        json_stats = {}
        if files['json']:
            json_stats = analyzer.analyze_json_data(files['json'])
        
        # APIìš© ìš”ì•½ ë°ì´í„° ìƒì„±
        if json_stats:
            api_summary = analyzer.get_api_summary(json_stats)
            return JSONResponse({
                'status': 'success',
                'data': api_summary,
                'file_info': {
                    'json_file': files.get('json', ''),
                    'excel_file': files.get('excel', ''),
                    'analysis_time': datetime.now().isoformat()
                }
            })
        else:
            return JSONResponse({
                'status': 'error',
                'message': 'ë°ì´í„° ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
            }, status_code=500)
            
    except Exception as e:
        print(f"âŒ í†µê³„ API ì˜¤ë¥˜: {e}")
        return JSONResponse({
            'status': 'error',
            'message': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'
        }, status_code=500)

if __name__ == "__main__":
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("static", exist_ok=True)
    os.makedirs("templates/html", exist_ok=True)
    os.makedirs("templates/css", exist_ok=True)
    os.makedirs("templates/js", exist_ok=True)
    os.makedirs("logs", exist_ok=True)  # âœ… ì¶”ê°€: ë¡œê·¸ ë””ë ‰í† ë¦¬
    
    print("=" * 60)
    print("ğŸŒ í–¥ìƒëœ ì›¹ í¬ë¡¤ë§ ì œì–´ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 60)
    print("ğŸŒ ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8000 ì ‘ì†")
    print("ğŸ¤– Enhanced Detail Extractor ì§€ì›")
    print("ğŸ“ ëª¨ë“ˆí™”ëœ êµ¬ì¡°:")
    print("  ğŸ“„ HTML: templates/html/")
    print("  ğŸ¨ CSS: templates/css/")
    print("  âš¡ JS: templates/js/")
    print("  ğŸ“Š ë¡œê·¸: logs/")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8000) 