#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì›¹ ê¸°ë°˜ í¬ë¡¤ë§ ì œì–´ ì‹œìŠ¤í…œ - DB ê¸°ë°˜
FastAPI + SQLiteë¥¼ í™œìš©í•œ í¬ë¡¤ë§ ê²°ê³¼ ê´€ë¦¬
"""

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import json
import asyncio
import threading
import logging
from datetime import datetime
from typing import Optional
import uvicorn

import os
from dotenv import load_dotenv
load_dotenv()
print(f"ğŸ”‘ .env ë¡œë“œ ì™„ë£Œ: GEMINI_API_KEY={'ì„¤ì •ë¨' if os.getenv('GEMINI_API_KEY') else 'ì—†ìŒ'}")

# í•„ìš”í•œ ëª¨ë“ˆë“¤
from crawler_main import UnifiedCrawler
# from legacy.data_statistics import DataStatisticsAnalyzer
from utils.logger_utils import LoggerUtils
from utils.file_utils import FileUtils
from database.database import get_database

# ì „ì—­ ë³€ìˆ˜
db = get_database()  # âœ… DB ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
current_crawling_job_id: Optional[int] = None
extractor_instance = None
total_organizations = []
current_data_file = None
logger = LoggerUtils.setup_app_logger()

app = FastAPI(title="DB ê¸°ë°˜ í¬ë¡¤ë§ ì‹œìŠ¤í…œ", version="2.0.0")

# ì •ì  íŒŒì¼ ë° í…œí”Œë¦¿ ì„¤ì •
app.mount("/static/css", StaticFiles(directory="templates/css"), name="css")
app.mount("/static/js", StaticFiles(directory="templates/js"), name="js")
templates = Jinja2Templates(directory="templates/html")

if os.path.exists("static"):
    app.mount("/static/files", StaticFiles(directory="static"), name="files")

# progress_callback í•¨ìˆ˜
def progress_callback(result: dict):
    """DB ê¸°ë°˜ ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜"""
    global current_crawling_job_id
    
    if not current_crawling_job_id:
        logger.error("í¬ë¡¤ë§ ì‘ì—… IDê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # DBì— ê²°ê³¼ ì €ì¥
        result_data = {
            'job_id': current_crawling_job_id,
            'organization_name': result.get('name', ''),
            'category': result.get('category', ''),
            'homepage_url': result.get('homepage_url', ''),
            'status': result.get('status', 'PROCESSING'),
            'current_step': result.get('current_step', ''),
            'processing_time': result.get('processing_time', 0),
            'extraction_method': result.get('extraction_method', ''),
            'phone': result.get('phone', ''),
            'fax': result.get('fax', ''),
            'email': result.get('email', ''),
            'mobile': result.get('mobile', ''),
            'address': result.get('address', ''),
            'crawling_details': {
                'homepage_parsed': result.get('homepage_parsed'),
                'ai_summary': result.get('ai_summary'),
                'meta_info': result.get('meta_info'),
                'contact_info_extracted': result.get('contact_info_extracted')
            },
            'error_message': result.get('error_message', '')
        }
        
        db.add_crawling_result(result_data)
        
        # ì‘ì—… ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        if result.get('status') == 'completed':
            progress = db.get_crawling_progress(current_crawling_job_id)
            completed_count = progress['processed_count'] + 1
            
            db.update_crawling_job(current_crawling_job_id, {
                'processed_count': completed_count
            })
        
        logger.info(f"DB ì €ì¥ ì™„ë£Œ: {result.get('name')} - {result.get('status')}")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {e}")

# ì›¹ í˜ì´ì§€ ë¼ìš°íŠ¸
@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€"""
    logger.info("ë©”ì¸ í˜ì´ì§€ ìš”ì²­")
    return templates.TemplateResponse("index.html", {"request": request})

# API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/api/start-enhanced-crawling")
async def start_enhanced_crawling(request: Request):
    """DB ê¸°ë°˜ Enhanced í¬ë¡¤ë§ ì‹œì‘"""
    global extractor_instance, total_organizations, current_crawling_job_id, current_data_file
    
    try:
        config_data = await request.json()
    except:
        config_data = {"test_mode": False, "test_count": 10, "use_ai": True}
    
    logger.info(f"Enhanced í¬ë¡¤ë§ ì‹œì‘ ìš”ì²­: {config_data}")
    
    try:
        # 121-128 ë¼ì¸ ìˆ˜ì •
        if os.path.exists("data/json/merged_church_data_20250618_174032.json"):
            current_data_file = "data/json/merged_church_data_20250618_174032.json"
            data = FileUtils.load_json(current_data_file)
        elif os.path.exists("raw_data_0530.json"):
            current_data_file = "raw_data_0530.json"
            data = FileUtils.load_json(current_data_file)
        elif os.path.exists("raw_data.json"):
            current_data_file = "raw_data.json"
            data = FileUtils.load_json(current_data_file)
        else:
            raise HTTPException(status_code=404, detail="ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° ì²˜ë¦¬
        total_organizations = []
        if isinstance(data, dict):
            for category, orgs in data.items():
                if isinstance(orgs, list):
                    for org in orgs:
                        if isinstance(org, dict):
                            org["category"] = category
                            total_organizations.append(org)
        elif isinstance(data, list):
            total_organizations = [org for org in data if isinstance(org, dict)]
        
        # DBì— í¬ë¡¤ë§ ì‘ì—… ìƒì„±
        job_data = {
            'job_name': f"Enhanced Crawling {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            'total_count': config_data.get('test_count', 10) if config_data.get('test_mode', False) else len(total_organizations),
            'started_by': 'SYSTEM',
            'config': config_data
        }
        
        current_crawling_job_id = db.create_crawling_job(job_data)
        logger.info(f"í¬ë¡¤ë§ ì‘ì—… ìƒì„±: ID {current_crawling_job_id}")
        
        # Enhanced Detail Extractor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        api_key = os.getenv('GEMINI_API_KEY') if config_data.get('use_ai', True) else None
        extractor_instance = UnifiedCrawler(
            api_key=api_key, 
            progress_callback=progress_callback
        )
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
        threading.Thread(target=run_enhanced_crawling_db, args=(config_data,), daemon=True).start()
        
        return {
            "message": "Enhanced í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", 
            "job_id": current_crawling_job_id,
            "total_count": job_data['total_count']
        }
        
    except Exception as e:
        logger.error(f"Enhanced í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")

def run_enhanced_crawling_db(config_data: dict):
    """DB ê¸°ë°˜ í¬ë¡¤ë§ ì‹¤í–‰"""
    global current_crawling_job_id, extractor_instance, current_data_file
    
    logger.info(f"DB ê¸°ë°˜ Enhanced í¬ë¡¤ë§ ì‹œì‘: Job ID {current_crawling_job_id}")
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        test_mode = config_data.get('test_mode', False)
        test_count = config_data.get('test_count', 10)
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        results = loop.run_until_complete(
            extractor_instance.process_json_file_async(
                json_file_path=current_data_file,
                test_mode=test_mode,
                test_count=test_count
            )
        )
        
        # ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
        db.update_crawling_job(current_crawling_job_id, {
            'status': 'COMPLETED',
            'completed_at': datetime.now().isoformat()
        })
        
        logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: Job ID {current_crawling_job_id}")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        if current_crawling_job_id:
            db.update_crawling_job(current_crawling_job_id, {
                'status': 'ERROR',
                'error_message': str(e),
                'completed_at': datetime.now().isoformat()
            })

@app.get("/api/progress")
async def get_progress():
    """DB ê¸°ë°˜ í¬ë¡¤ë§ ì§„í–‰ ìƒí™© ì¡°íšŒ"""
    if not current_crawling_job_id:
        return {"status": "idle", "message": "ì§„í–‰ ì¤‘ì¸ í¬ë¡¤ë§ì´ ì—†ìŠµë‹ˆë‹¤."}
    
    try:
        progress = db.get_crawling_progress(current_crawling_job_id)
        return progress
    except Exception as e:
        logger.error(f"ì§„í–‰ ìƒí™© ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/api/real-time-results")
async def get_real_time_results(limit: int = 10):
    """DB ê¸°ë°˜ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    if not current_crawling_job_id:
        return {"results": [], "total_count": 0}
    
    try:
        results = db.get_crawling_results(current_crawling_job_id, limit)
        progress = db.get_crawling_progress(current_crawling_job_id)
        
        return {
            "results": results,
            "total_count": progress.get('processed_count', 0),
            "progress": progress
        }
    except Exception as e:
        logger.error(f"ì‹¤ì‹œê°„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"results": [], "total_count": 0, "error": str(e)}

@app.get("/api/all-real-time-results")
async def get_all_real_time_results():
    """DB ê¸°ë°˜ ëª¨ë“  í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    if not current_crawling_job_id:
        return {"results": [], "total_count": 0}
    
    try:
        results = db.get_crawling_results(current_crawling_job_id, limit=10000)
        progress = db.get_crawling_progress(current_crawling_job_id)
        
        completed_count = len([r for r in results if r['status'] == 'COMPLETED'])
        failed_count = len([r for r in results if r['status'] == 'FAILED'])
        
        return {
            "results": results,
            "total_count": len(results),
            "completed_count": completed_count,
            "failed_count": failed_count,
            "progress": progress
        }
    except Exception as e:
        logger.error(f"ì „ì²´ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"results": [], "total_count": 0, "error": str(e)}

@app.get("/api/results")
async def get_results():
    """DBì—ì„œ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    job = db.get_latest_crawling_job()
    if not job:
        return {"results": [], "count": 0}
    
    results = db.get_crawling_results(job['id'], limit=1000)
    return {"results": results, "count": len(results)}

# ê¸°ì¡´ í†µê³„ API (ìœ ì§€)
@app.get('/api/statistics')
def get_statistics():
    """ê°„ë‹¨í•œ ë°ì´í„° í†µê³„ API - DB ê¸°ë°˜"""
    try:
        # DBì—ì„œ ì§ì ‘ í†µê³„ ì¡°íšŒ
        stats = db.get_dashboard_stats()
        
        return JSONResponse({
            'status': 'success',
            'data': {
                'total_organizations': stats['total_organizations'],
                'total_users': stats['total_users'],
                'recent_activities': stats['recent_activities'],
                'crawling_jobs': stats.get('crawling_jobs', 0),
                'analysis_time': datetime.now().isoformat()
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ í†µê³„ API ì˜¤ë¥˜: {e}")
        return JSONResponse({
            'status': 'error',
            'message': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'
        }, status_code=500)

# Organizations API ìˆ˜ì • (ê¸°ì¡´ ì½”ë“œ êµì²´)
@app.get("/api/organizations")
async def get_organizations(
    page: int = 1,
    per_page: int = 50,  # 50ê°œë¡œ ì¦ê°€
    search: str = None,
    category: str = None,
    status: str = None
):
    """ê¸°ê´€ ëª©ë¡ ì¡°íšŒ (ë¬´í•œ ìŠ¤í¬ë¡¤ìš©)"""
    try:
        filters = {}
        if category:
            filters['category'] = category
        if status:
            filters['contact_status'] = status
            
        result = db.get_organizations_paginated(
            page=page,
            per_page=per_page,
            search=search,
            filters=filters
        )
        
        return JSONResponse({
            "organizations": result["organizations"],
            "pagination": {
                "current_page": result["pagination"]["page"],
                "per_page": result["pagination"]["per_page"], 
                "total_count": result["pagination"]["total_count"],
                "total_pages": result["pagination"]["total_pages"],
                "has_next": result["pagination"]["page"] < result["pagination"]["total_pages"]
            }
        })
        
    except Exception as e:
        logger.error(f"ê¸°ê´€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
@app.get("/api/organizations/{org_id}")
async def get_organization_detail(org_id: int):
    """ê¸°ê´€ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    try:
        organization = db.get_organization_detail(org_id)
        if not organization:
            raise HTTPException(status_code=404, detail="ê¸°ê´€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return {"organization": organization}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ê¸°ê´€ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/organizations/{org_id}")
async def update_organization(org_id: int, request: Request):
    """ê¸°ê´€ ì •ë³´ ì—…ë°ì´íŠ¸"""
    try:
        updates = await request.json()
        success = db.update_organization(org_id, updates, updated_by="SYSTEM")
        
        if success:
            return {"message": "ê¸°ê´€ ì •ë³´ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            raise HTTPException(status_code=404, detail="ê¸°ê´€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ê¸°ê´€ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/organizations/{org_id}")
async def delete_organization(org_id: int):
    """ê¸°ê´€ ì‚­ì œ"""
    try:
        success = db.delete_organization(org_id)
        
        if success:
            return {"message": "ê¸°ê´€ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            raise HTTPException(status_code=404, detail="ê¸°ê´€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ê¸°ê´€ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("static", exist_ok=True)
    os.makedirs("templates/html", exist_ok=True)
    os.makedirs("templates/css", exist_ok=True)
    os.makedirs("templates/js", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    print("=" * 80)
    print("ğŸŒ DB ê¸°ë°˜ í¬ë¡¤ë§ ì‹œìŠ¤í…œ v2.0 ì‹œì‘")
    print("=" * 80)
    print("ğŸ¤– í¬ë¡¤ë§ ê¸°ëŠ¥:")
    print("  ğŸ“„ Enhanced Detail Extractor")
    print("  ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§")
    print("  ğŸ—„ï¸  SQLite DB ì €ì¥")
    print()
    print("ğŸŒ ì ‘ì† ì •ë³´:")
    print("  ğŸ“± ë¸Œë¼ìš°ì €: http://localhost:8000")
    print("=" * 80)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
    try:
        stats = db.get_dashboard_stats()
        print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸: {stats['total_organizations']:,}ê°œ ê¸°ê´€")
    except Exception as e:
        print(f"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸ ì‹¤íŒ¨: {e}")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)