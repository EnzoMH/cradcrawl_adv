#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìƒˆë¡œìš´ í†µí•© CRM ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ì„ ìœ„í•œ í—¤ë” ë¶„ì„ ë° AI ê¸°ë°˜ í•©ë³‘ ë°©ì•ˆ ì œì‹œ
Version 2: êµíšŒ ë°ì´í„° 3í–‰ í—¤ë” ì²˜ë¦¬ + ìƒˆë¡œìš´ SimpleAI í´ë¼ì´ì–¸íŠ¸
"""

import pandas as pd
import json
import os
import sys
from pathlib import Path
import asyncio
from typing import Dict, List, Any, Optional

# ìƒëŒ€ê²½ë¡œ importë¥¼ ìœ„í•œ path ì„¤ì •
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.append(str(parent_dir))

# Gemini AI ëª¨ë“ˆ import
try:
    import google.generativeai as genai
    from settings import GEMINI_API_KEY, GEMINI_MODEL_TEXT
    GEMINI_AVAILABLE = True and bool(GEMINI_API_KEY)
    print(f"âœ… Gemini AI ì‚¬ìš© ê°€ëŠ¥: {GEMINI_AVAILABLE}")
except ImportError as e:
    print(f"âš ï¸ Gemini AI ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    GEMINI_AVAILABLE = False
    genai = None
    GEMINI_API_KEY = None
    GEMINI_MODEL_TEXT = "gemini-1.5-flash"

# ê¸°ì¡´ ai_helpersë„ ì‹œë„ (fallbackìš©)
try:
    from ai_helpers import AIModelManager, extract_with_gemini_text
    AI_HELPERS_AVAILABLE = True
except ImportError:
    print("âš ï¸ AI helpers ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    AI_HELPERS_AVAILABLE = False

class SimpleAIClient:
    """ê°„ë‹¨í•œ Gemini AI í´ë¼ì´ì–¸íŠ¸ (ai_helpersì™€ ë³„ë„)"""
    
    def __init__(self):
        self.model = None
        self.available = GEMINI_AVAILABLE
        
        if self.available and genai and GEMINI_API_KEY:
            try:
                genai.configure(api_key=GEMINI_API_KEY)
                self.model = genai.GenerativeModel(GEMINI_MODEL_TEXT)
                print(f"âœ… SimpleAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {GEMINI_MODEL_TEXT})")
            except Exception as e:
                print(f"âŒ SimpleAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.available = False
                self.model = None
        else:
            print("âš ï¸ SimpleAI í´ë¼ì´ì–¸íŠ¸ ë¹„í™œì„±í™” (API í‚¤ ë˜ëŠ” ëª¨ë“ˆ ì—†ìŒ)")
    
    def generate_response(self, prompt: str) -> str:
        """AI ì‘ë‹µ ìƒì„± (ë™ê¸° ë°©ì‹)"""
        if not self.available or not self.model:
            return "AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
        
        try:
            print("ğŸ¤– Gemini AI ì‘ë‹µ ìƒì„± ì¤‘...")
            response = self.model.generate_content(prompt)
            response_text = response.text.strip()
            print(f"âœ… AI ì‘ë‹µ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(response_text)} chars)")
            return response_text
            
        except Exception as e:
            print(f"âŒ AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return f"AI ì˜¤ë¥˜ë¡œ ì¸í•œ ë”ë¯¸ ì‘ë‹µ: {str(e)}"


class DataHeaderAnalyzer:
    """ë°ì´í„° íŒŒì¼ í—¤ë” ë¶„ì„ ë° AI ê¸°ë°˜ í•©ë³‘ ë°©ì•ˆ ì œì‹œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ë‹¤ì¤‘ AI ë°©ì‹ ì„¤ì • (ìš°ì„ ìˆœìœ„: SimpleAI â†’ ai_helpers â†’ ìˆ˜ë™ë¶„ì„)
        self.simple_ai = SimpleAIClient()
        
        if AI_HELPERS_AVAILABLE:
            self.ai_manager = AIModelManager()
        else:
            self.ai_manager = None
        
        self.data_files = {
            'academy': '../í•™ì›êµìŠµì†Œì •ë³´_2024ë…„10ì›”31ì¼ê¸°ì¤€20250617ìˆ˜ì •.xlsx',
            'church': '../data/excel/êµíšŒ_ì›ë³¸_ìˆ˜ì •01.xlsx',
            'filtered_json': '../data/json/filtered_data_updated_20250613_013541.json'
        }
        self.headers = {}
        self.sample_data = {}
        
    def analyze_church_excel_with_header_row_3(self, file_path: str) -> Dict[str, Any]:
        """êµíšŒ ì—‘ì…€ íŒŒì¼ ë¶„ì„ (3í–‰ì´ í—¤ë”)"""
        try:
            abs_path = Path(current_dir) / file_path
            print(f"ğŸ“Š êµíšŒ íŒŒì¼ ë¶„ì„ ì¤‘ (3í–‰ í—¤ë”): {abs_path}")
            
            # 3í–‰ì„ í—¤ë”ë¡œ ì§€ì • (pandasì—ì„œëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ 2)
            df = pd.read_excel(abs_path, header=2, nrows=10)
            headers = df.columns.tolist()
            sample_data = df.head(3).to_dict('records')
            
            # NaN ê°’ì´ ë§ì€ í—¤ë”ë“¤ ì •ë¦¬
            cleaned_headers = []
            for i, h in enumerate(headers):
                if pd.isna(h) or str(h).startswith('Unnamed'):
                    # ì‹¤ì œ ë°ì´í„°ì—ì„œ ì²« ë²ˆì§¸ ê°’ì„ í—¤ë”ë¡œ ì‚¬ìš©
                    try:
                        first_val = df[h].dropna().iloc[0] if not df[h].dropna().empty else f"Column_{i}"
                        cleaned_headers.append(str(first_val))
                    except:
                        cleaned_headers.append(f"Column_{i}")
                else:
                    cleaned_headers.append(str(h))
            
            analysis = {
                'file_path': str(abs_path),
                'file_type': 'church',
                'headers': cleaned_headers,
                'original_headers': headers,
                'header_count': len(cleaned_headers),
                'sample_data': sample_data,
                'data_shape': df.shape,
                'note': '3í–‰ì„ í—¤ë”ë¡œ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ë¨'
            }
            
            print(f"âœ… êµíšŒ ë°ì´í„° í—¤ë” ê°œìˆ˜: {len(cleaned_headers)}")
            print(f"ğŸ“ ì •ë¦¬ëœ í—¤ë” ëª©ë¡: {cleaned_headers[:10]}{'...' if len(cleaned_headers) > 10 else ''}")
            
            return analysis
            
        except Exception as e:
            print(f"âŒ êµíšŒ íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return None
        
    def analyze_excel_headers(self, file_path: str, file_type: str) -> Dict[str, Any]:
        """ì—‘ì…€ íŒŒì¼ì˜ í—¤ë” ë¶„ì„ (ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì‹œë„)"""
        try:
            abs_path = Path(current_dir) / file_path
            print(f"ğŸ“Š {file_type} íŒŒì¼ ë¶„ì„ ì¤‘: {abs_path}")
            
            # ë¨¼ì € ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì‹œë„
            try:
                df = pd.read_excel(abs_path, nrows=10)
                headers = df.columns.tolist()
                sample_data = df.head(3).to_dict('records')
                
                # í—¤ë”ê°€ 'Unnamed'ì¸ ê²½ìš° ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì‹œë„
                if any('Unnamed' in str(h) for h in headers):
                    print(f"âš ï¸ {file_type} íŒŒì¼ì—ì„œ Unnamed í—¤ë” ë°œê²¬. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì‹œë„...")
                    
                    # í—¤ë”ê°€ 2ë²ˆì§¸ ë˜ëŠ” 3ë²ˆì§¸ í–‰ì— ìˆì„ ìˆ˜ ìˆìŒ
                    for header_row in [1, 2, 3]:
                        try:
                            df_alt = pd.read_excel(abs_path, header=header_row, nrows=10)
                            headers_alt = df_alt.columns.tolist()
                            if not any('Unnamed' in str(h) for h in headers_alt):
                                headers = headers_alt
                                sample_data = df_alt.head(3).to_dict('records')
                                print(f"âœ… {file_type} í—¤ë”ë¥¼ {header_row+1}ë²ˆì§¸ í–‰ì—ì„œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                                break
                        except:
                            continue
                            
            except Exception as e:
                print(f"âŒ {file_type} íŒŒì¼ ê¸°ë³¸ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
                return None
            
            analysis = {
                'file_path': str(abs_path),
                'file_type': file_type,
                'headers': headers,
                'header_count': len(headers),
                'sample_data': sample_data,
                'data_shape': df.shape if 'df' in locals() else (0, 0)
            }
            
            print(f"âœ… {file_type} í—¤ë” ê°œìˆ˜: {len(headers)}")
            print(f"ğŸ“ í—¤ë” ëª©ë¡: {headers[:10]}{'...' if len(headers) > 10 else ''}")
            
            return analysis
            
        except Exception as e:
            print(f"âŒ {file_type} íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def analyze_json_headers(self, file_path: str, file_type: str) -> Dict[str, Any]:
        """JSON íŒŒì¼ì˜ í—¤ë” ë¶„ì„"""
        try:
            abs_path = Path(current_dir) / file_path
            print(f"ğŸ“Š {file_type} íŒŒì¼ ë¶„ì„ ì¤‘: {abs_path}")
            
            with open(abs_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not data:
                return None
            
            # ì²« ë²ˆì§¸ í•­ëª©ì—ì„œ í‚¤ ì¶”ì¶œ
            sample_item = data[0] if isinstance(data, list) else data
            headers = list(sample_item.keys())
            
            # ìƒ˜í”Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì²˜ìŒ 3ê°œ)
            sample_data = data[:3] if isinstance(data, list) else [data]
            
            analysis = {
                'file_path': str(abs_path),
                'file_type': file_type,
                'headers': headers,
                'header_count': len(headers),
                'sample_data': sample_data,
                'data_count': len(data) if isinstance(data, list) else 1
            }
            
            print(f"âœ… {file_type} í—¤ë” ê°œìˆ˜: {len(headers)}")
            print(f"ğŸ“ í—¤ë” ëª©ë¡: {headers}")
            
            return analysis
            
        except Exception as e:
            print(f"âŒ {file_type} íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def analyze_all_files(self):
        """ëª¨ë“  íŒŒì¼ì˜ í—¤ë” ë¶„ì„"""
        print("ğŸ” ë°ì´í„° íŒŒì¼ í—¤ë” ë¶„ì„ ì‹œì‘...")
        
        # í•™ì› ë°ì´í„° ë¶„ì„
        academy_analysis = self.analyze_excel_headers(
            self.data_files['academy'], 'academy'
        )
        if academy_analysis:
            self.headers['academy'] = academy_analysis
        
        # êµíšŒ ë°ì´í„° ë¶„ì„ (3í–‰ í—¤ë” ì‚¬ìš©)
        print("ğŸ“‹ êµíšŒ ë°ì´í„°ëŠ” 3í–‰ì´ ì‹¤ì œ í—¤ë”ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        church_analysis = self.analyze_church_excel_with_header_row_3(
            self.data_files['church']
        )
        if church_analysis:
            self.headers['church'] = church_analysis
        
        # JSON ë°ì´í„° ë¶„ì„
        json_analysis = self.analyze_json_headers(
            self.data_files['filtered_json'], 'filtered_json'
        )
        if json_analysis:
            self.headers['filtered_json'] = json_analysis
        
        print(f"\nğŸ“Š ì´ {len(self.headers)}ê°œ íŒŒì¼ ë¶„ì„ ì™„ë£Œ")
        return self.headers
    
    def identify_common_fields(self) -> Dict[str, List[str]]:
        """ê³µí†µ í•„ë“œ ì‹ë³„"""
        common_mapping = {
            'ìƒí˜¸ëª…': [],
            'ì£¼ì†Œ': [],
            'ì „í™”ë²ˆí˜¸': [],
            'ëª¨ë°”ì¼ë²ˆí˜¸': [],
            'ì¹´í…Œê³ ë¦¬': []
        }
        
        # ê° ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ê³µí†µ í•„ë“œ ë§¤í•‘
        for source_name, analysis in self.headers.items():
            headers = analysis['headers']
            
            # ìƒí˜¸ëª… ë§¤í•‘
            name_fields = [h for h in headers if any(keyword in str(h).lower() for keyword in ['name', 'ëª…', 'ìƒí˜¸', 'í•™ì›ëª…', 'êµíšŒ'])]
            if name_fields:
                common_mapping['ìƒí˜¸ëª…'].append(f"{source_name}: {name_fields}")
            
            # ì£¼ì†Œ ë§¤í•‘
            address_fields = [h for h in headers if any(keyword in str(h).lower() for keyword in ['address', 'ì£¼ì†Œ', 'ì†Œì¬ì§€', 'ìœ„ì¹˜'])]
            if address_fields:
                common_mapping['ì£¼ì†Œ'].append(f"{source_name}: {address_fields}")
            
            # ì „í™”ë²ˆí˜¸ ë§¤í•‘
            phone_fields = [h for h in headers if any(keyword in str(h).lower() for keyword in ['phone', 'ì „í™”', 'ì—°ë½ì²˜', 'tel'])]
            if phone_fields:
                common_mapping['ì „í™”ë²ˆí˜¸'].append(f"{source_name}: {phone_fields}")
            
            # ëª¨ë°”ì¼ë²ˆí˜¸ ë§¤í•‘
            mobile_fields = [h for h in headers if any(keyword in str(h).lower() for keyword in ['mobile', 'íœ´ëŒ€í°', 'í•¸ë“œí°', 'ëª¨ë°”ì¼'])]
            if mobile_fields:
                common_mapping['ëª¨ë°”ì¼ë²ˆí˜¸'].append(f"{source_name}: {mobile_fields}")
            
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_fields = [h for h in headers if any(keyword in str(h).lower() for keyword in ['category', 'ì¹´í…Œê³ ë¦¬', 'ë¶„ë¥˜', 'ìœ í˜•', 'ì¢…ë¥˜'])]
            if category_fields:
                common_mapping['ì¹´í…Œê³ ë¦¬'].append(f"{source_name}: {category_fields}")
        
        return common_mapping
    
    def create_manual_merge_strategy(self) -> Dict[str, Any]:
        """ìˆ˜ë™ìœ¼ë¡œ í•©ë³‘ ì „ëµ ìƒì„±"""
        common_fields = self.identify_common_fields()
        
        strategy = {
            "analysis_timestamp": pd.Timestamp.now().isoformat(),
            "data_sources": {
                source: {
                    "file_path": analysis['file_path'],
                    "header_count": analysis['header_count'],
                    "headers": analysis['headers']
                }
                for source, analysis in self.headers.items()
            },
            "common_fields_mapping": common_fields,
            "recommended_unified_schema": {
                "table_name": "unified_crm_database",
                "primary_columns": [
                    {"name": "id", "type": "INTEGER PRIMARY KEY AUTOINCREMENT", "description": "ê³ ìœ  ì‹ë³„ì"},
                    {"name": "business_name", "type": "TEXT NOT NULL", "description": "ìƒí˜¸ëª…/ê¸°ê´€ëª…"},
                    {"name": "full_address", "type": "TEXT", "description": "ì „ì²´ ì£¼ì†Œ"},
                    {"name": "phone_number", "type": "TEXT", "description": "ëŒ€í‘œ ì „í™”ë²ˆí˜¸"},
                    {"name": "mobile_number", "type": "TEXT", "description": "ëª¨ë°”ì¼ ë²ˆí˜¸"},
                    {"name": "category", "type": "TEXT NOT NULL", "description": "ì¹´í…Œê³ ë¦¬ (ì¢…êµ/êµìŠµì†Œ/í•™ì›/ê³µê³µê¸°ê´€)"},
                    {"name": "data_source", "type": "TEXT NOT NULL", "description": "ë°ì´í„° ì¶œì²˜"},
                    {"name": "additional_info", "type": "JSON", "description": "ì¶”ê°€ ì •ë³´ (JSON í˜•íƒœ)"},
                    {"name": "created_at", "type": "TIMESTAMP DEFAULT CURRENT_TIMESTAMP", "description": "ìƒì„±ì¼ì‹œ"},
                    {"name": "updated_at", "type": "TIMESTAMP DEFAULT CURRENT_TIMESTAMP", "description": "ìˆ˜ì •ì¼ì‹œ"}
                ]
            },
            "merge_implementation_steps": [
                "1. ê° ë°ì´í„° ì†ŒìŠ¤ë³„ë¡œ í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë°ì´í„° ì¶”ì¶œ",
                "2. ê³µí†µ í•„ë“œ ë§¤í•‘ ê·œì¹™ì— ë”°ë¼ ë°ì´í„° ë³€í™˜",
                "3. ì¤‘ë³µ ë°ì´í„° ì‹ë³„ ë° ì œê±° (ìƒí˜¸ëª… + ì£¼ì†Œ ê¸°ì¤€)",
                "4. ì¹´í…Œê³ ë¦¬ í‘œì¤€í™” (ì¢…êµì‹œì„¤, êµìœ¡ê¸°ê´€, ê³µê³µê¸°ê´€ ë“±)",
                "5. í†µí•© ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±",
                "6. ë³€í™˜ëœ ë°ì´í„° ì‚½ì…",
                "7. ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ë³´ì™„"
            ],
            "data_quality_considerations": [
                "ì „í™”ë²ˆí˜¸ í˜•ì‹ í‘œì¤€í™” (í•˜ì´í”ˆ ì¶”ê°€/ì œê±°)",
                "ì£¼ì†Œ í‘œì¤€í™” (ë„ë¡œëª… ì£¼ì†Œ ìš°ì„ )",
                "ì¤‘ë³µ ì—…ì²´ ì‹ë³„ ë¡œì§ (ì´ë¦„ ìœ ì‚¬ë„ + ì£¼ì†Œ ìœ ì‚¬ë„)",
                "ë¹ˆ ê°’ ì²˜ë¦¬ ë°©ì•ˆ",
                "íŠ¹ìˆ˜ë¬¸ì ë° ì¸ì½”ë”© ë¬¸ì œ í•´ê²°"
            ]
        }
        
        return strategy
    
    def get_ai_merge_strategy_simple(self) -> Optional[Dict[str, Any]]:
        """SimpleAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•©ë³‘ ì „ëµ ìƒì„± (ë™ê¸° ë°©ì‹)"""
        try:
            print("\nğŸ¤– SimpleAIë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° í•©ë³‘ ì „ëµ ìƒì„± ì¤‘...")
            
            # ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            prompt = self.create_advanced_ai_prompt()
            
            # AI ì‘ë‹µ ë°›ê¸°
            ai_response = self.simple_ai.generate_response(prompt)
            
            if "AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in ai_response or "ì˜¤ë¥˜" in ai_response:
                print("âš ï¸ SimpleAI ì‚¬ìš© ë¶ˆê°€. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
                return None
            
            print("âœ… SimpleAI í•©ë³‘ ì „ëµ ìƒì„± ì™„ë£Œ")
            
            # í…ìŠ¤íŠ¸ ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜
            strategy = self.create_manual_merge_strategy()
            strategy["ai_analysis"] = ai_response
            strategy["ai_method"] = "SimpleAI (Gemini)"
            
            return strategy
                
        except Exception as e:
            print(f"âŒ SimpleAI í•©ë³‘ ì „ëµ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
    
    async def get_ai_merge_strategy_helpers(self) -> Optional[Dict[str, Any]]:
        """AI helpersë¥¼ ì‚¬ìš©í•˜ì—¬ í•©ë³‘ ì „ëµ ìƒì„± (ë¹„ë™ê¸° ë°©ì‹)"""
        if not AI_HELPERS_AVAILABLE or not self.ai_manager:
            print("âš ï¸ AI helpersë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            print("\nğŸ¤– AI helpersë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° í•©ë³‘ ì „ëµ ìƒì„± ì¤‘...")
            
            # AI ëª¨ë¸ ì´ˆê¸°í™”
            await self.ai_manager.setup_models()
            
            prompt = self.create_ai_prompt_for_merge_strategy()
            
            # AI ì‘ë‹µ ë°›ê¸°
            ai_response = await extract_with_gemini_text(prompt, "")
            
            print("âœ… AI helpers í•©ë³‘ ì „ëµ ìƒì„± ì™„ë£Œ")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                strategy = json.loads(ai_response)
                strategy["ai_method"] = "AI helpers"
                return strategy
            except json.JSONDecodeError:
                print("âš ï¸ AI ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # í…ìŠ¤íŠ¸ ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜
                strategy = self.create_manual_merge_strategy()
                strategy["ai_analysis"] = ai_response
                strategy["ai_method"] = "AI helpers (text)"
                return strategy
                
        except Exception as e:
            print(f"âŒ AI helpers í•©ë³‘ ì „ëµ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
    
    async def get_ai_merge_strategy(self) -> Optional[Dict[str, Any]]:
        """ë‹¤ì¤‘ AI ë°©ì‹ìœ¼ë¡œ í•©ë³‘ ì „ëµ ìƒì„± (ìš°ì„ ìˆœìœ„: SimpleAI â†’ ai_helpers â†’ ìˆ˜ë™ë¶„ì„)"""
        print("\nğŸ”„ ë‹¤ì¤‘ AI ë°©ì‹ìœ¼ë¡œ ë°ì´í„° í•©ë³‘ ì „ëµ ìƒì„± ì‹œë„...")
        
        # ë°©ë²• 1: SimpleAI ì‹œë„
        strategy = self.get_ai_merge_strategy_simple()
        if strategy:
            return strategy
        
        # ë°©ë²• 2: AI helpers ì‹œë„
        strategy = await self.get_ai_merge_strategy_helpers()
        if strategy:
            return strategy
        
        # ë°©ë²• 3: ìˆ˜ë™ ë¶„ì„ (ìµœì¢… fallback)
        print("ğŸ“ ëª¨ë“  AI ë°©ì‹ ì‹¤íŒ¨. ìˆ˜ë™ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        strategy = self.create_manual_merge_strategy()
        strategy["ai_method"] = "Manual analysis (fallback)"
        return strategy
    
    def create_advanced_ai_prompt(self) -> str:
        """ê³ ê¸‰ AI í”„ë¡¬í”„íŠ¸ ìƒì„± (urlextractor_2.py ìŠ¤íƒ€ì¼ ì°¸ê³ )"""
        prompt = f"""
# í†µí•© CRM ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ì„ ìœ„í•œ ì „ë¬¸ê°€ ë¶„ì„ ìš”ì²­

ì•ˆë…•í•˜ì„¸ìš”. ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ 3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤ë¥¼ í•˜ë‚˜ì˜ í†µí•© CRM ì‹œìŠ¤í…œìœ¼ë¡œ í•©ë³‘í•˜ëŠ” ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

## ğŸ“Š ë°ì´í„° ì†ŒìŠ¤ ìƒì„¸ ë¶„ì„
"""
        
        for source_name, analysis in self.headers.items():
            prompt += f"""
### ã€{source_name.upper()} ë°ì´í„°ã€‘
- **íŒŒì¼ ê²½ë¡œ**: {analysis['file_path']}
- **ì´ ì»¬ëŸ¼ ìˆ˜**: {analysis['header_count']}ê°œ
- **ë°ì´í„° í¬ê¸°**: {analysis.get('data_shape', 'N/A')}
- **íŠ¹ë³„ ì‚¬í•­**: {analysis.get('note', 'ì¼ë°˜ ì²˜ë¦¬')}
- **ì „ì²´ ì»¬ëŸ¼ ëª©ë¡**: {', '.join(analysis['headers'])}

**ìƒ˜í”Œ ë°ì´í„° (ì²« ë²ˆì§¸ ë ˆì½”ë“œ)**:
```json
{json.dumps(analysis['sample_data'][0] if analysis['sample_data'] else {}, ensure_ascii=False, indent=2)}
```
"""
        
        # ê³µí†µ í•„ë“œ ë§¤í•‘ ì •ë³´ ì¶”ê°€
        common_fields = self.identify_common_fields()
        prompt += """

## ğŸ”— í˜„ì¬ ì‹ë³„ëœ ê³µí†µ í•„ë“œ ë§¤í•‘
"""
        for field_name, mappings in common_fields.items():
            prompt += f"\n**{field_name}**:\n"
            if mappings:
                for mapping in mappings:
                    prompt += f"  - {mapping}\n"
            else:
                prompt += "  - ë§¤í•‘ëœ í•„ë“œ ì—†ìŒ\n"
        
        prompt += """

## ğŸ¯ ë¶„ì„ ìš”ì²­ì‚¬í•­

ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì „ë¬¸ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ì „ëµì„ ì œì‹œí•´ì£¼ì„¸ìš”:

1. **ê³µí†µ í•„ë“œ ë§¤í•‘ ì „ëµ**
   - ìƒí˜¸ëª…/ê¸°ê´€ëª… í†µí•© ë°©ì•ˆ
   - ì£¼ì†Œ ì •ë³´ í‘œì¤€í™” ë°©ì•ˆ
   - ì „í™”ë²ˆí˜¸/ëª¨ë°”ì¼ë²ˆí˜¸ í†µí•© ì •ì±…
   - ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì²´ê³„ ì„¤ê³„

2. **í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„**
   - ê¸°ë³¸ í…Œì´ë¸” êµ¬ì¡°
   - ì¸ë±ìŠ¤ ì „ëµ
   - í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡° ì„¤ê³„
   - ë©”íƒ€ë°ì´í„° ê´€ë¦¬ ë°©ì•ˆ

3. **ë°ì´í„° í’ˆì§ˆ ë° ì¤‘ë³µ ê´€ë¦¬**
   - ì¤‘ë³µ ë°ì´í„° ì‹ë³„ ì•Œê³ ë¦¬ì¦˜
   - ë°ì´í„° ì •í•©ì„± ê²€ì¦ ë°©ë²•
   - ì˜¤ë¥˜ ë°ì´í„° ì²˜ë¦¬ ì „ëµ
   - ë°ì´í„° í‘œì¤€í™” ê·œì¹™

4. **êµ¬í˜„ ê³„íš**
   - ë‹¨ê³„ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš
   - ì„±ëŠ¥ ìµœì í™” ë°©ì•ˆ
   - ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦ ë°©ë²•
   - ë¡¤ë°± ì „ëµ

## ğŸ“‹ ì‘ë‹µ í˜•ì‹

ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

**FIELD_MAPPING**: ê° ê³µí†µ í•„ë“œë³„ ë§¤í•‘ ì „ëµ
**UNIFIED_SCHEMA**: í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ
**QUALITY_STRATEGY**: ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì „ëµ
**IMPLEMENTATION**: êµ¬í˜„ ê³„íš ë° ìˆœì„œ
**ADDITIONAL_CONSIDERATIONS**: ì¶”ê°€ ê³ ë ¤ì‚¬í•­

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.
"""
        return prompt
    
    def create_ai_prompt_for_merge_strategy(self) -> str:
        """AI ëª¨ë¸ì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„± (ê¸°ì¡´ ë²„ì „ ìœ ì§€)"""
        prompt = """
ë‹¤ìŒì€ 3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤ì˜ í—¤ë” ì •ë³´ì™€ ìƒ˜í”Œ ë°ì´í„°ì…ë‹ˆë‹¤.
ì´ ë°ì´í„°ë“¤ì„ í•˜ë‚˜ì˜ í†µí•© CRM ë°ì´í„°ë² ì´ìŠ¤ë¡œ í•©ë³‘í•˜ëŠ” ì „ëµì„ ì œì‹œí•´ì£¼ì„¸ìš”.

=== ë°ì´í„° ì†ŒìŠ¤ ë¶„ì„ ===
"""
        
        for source_name, analysis in self.headers.items():
            prompt += f"\nã€{source_name.upper()}ã€‘\n"
            prompt += f"- íŒŒì¼ ê²½ë¡œ: {analysis['file_path']}\n"
            prompt += f"- í—¤ë” ê°œìˆ˜: {analysis['header_count']}\n"
            prompt += f"- í—¤ë” ëª©ë¡: {analysis['headers']}\n"
            prompt += f"- ìƒ˜í”Œ ë°ì´í„° (ì²« ë²ˆì§¸ í•­ëª©): {analysis['sample_data'][0] if analysis['sample_data'] else 'N/A'}\n"
        
        prompt += """

=== ìš”êµ¬ì‚¬í•­ ===
1. ê³µí†µ í•„ë“œ ì‹ë³„: ìƒí˜¸ëª…, ì£¼ì†Œ, ì „í™”ë²ˆí˜¸, ëª¨ë°”ì¼ë²ˆí˜¸, ì¹´í…Œê³ ë¦¬(ì¢…êµ/êµìŠµì†Œ/í•™ì›/ê³µê³µê¸°ê´€)
2. ê° ë°ì´í„° ì†ŒìŠ¤ì˜ ê³ ìœ  í•„ë“œë“¤ë„ ë³´ì¡´
3. ë°ì´í„° í’ˆì§ˆ ë° ì¤‘ë³µ ì œê±° ì „ëµ
4. ìƒˆë¡œìš´ í†µí•© ìŠ¤í‚¤ë§ˆ ì„¤ê³„

=== ë‹µë³€ í˜•ì‹ ===
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

{
    "common_fields_mapping": {
        "ìƒí˜¸ëª…": ["academy_field", "church_field", "json_field"],
        "ì£¼ì†Œ": ["academy_field", "church_field", "json_field"],
        "ì „í™”ë²ˆí˜¸": ["academy_field", "church_field", "json_field"],
        "ëª¨ë°”ì¼ë²ˆí˜¸": ["academy_field", "church_field", "json_field"],
        "ì¹´í…Œê³ ë¦¬": ["academy_field", "church_field", "json_field"]
    },
    "unified_schema": {
        "table_name": "unified_crm",
        "columns": [
            {"name": "id", "type": "INTEGER PRIMARY KEY AUTOINCREMENT"},
            {"name": "business_name", "type": "TEXT"},
            {"name": "address", "type": "TEXT"},
            {"name": "phone", "type": "TEXT"},
            {"name": "mobile", "type": "TEXT"},
            {"name": "category", "type": "TEXT"},
            {"name": "additional_fields", "type": "JSON"}
        ]
    },
    "merge_strategy": {
        "deduplication_rules": ["rule1", "rule2"],
        "data_transformation": ["transformation1", "transformation2"],
        "quality_checks": ["check1", "check2"]
    },
    "implementation_plan": [
        "step1: ë°ì´í„° ì¶”ì¶œ ë° ì •ê·œí™”",
        "step2: ê³µí†µ í•„ë“œ ë§¤í•‘",
        "step3: ì¤‘ë³µ ì œê±°",
        "step4: í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"
    ]
}
"""
        return prompt
    
    def save_analysis_results(self, merge_strategy: Dict[str, Any]):
        """ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            results = {
                'timestamp': pd.Timestamp.now().isoformat(),
                'header_analysis': self.headers,
                'merge_strategy': merge_strategy
            }
            
            output_file = Path(current_dir) / 'database_merge_analysis.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
    
    def print_detailed_summary(self):
        """ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š ë°ì´í„° íŒŒì¼ ìƒì„¸ í—¤ë” ë¶„ì„ ê²°ê³¼")
        print("="*80)
        
        for source_name, analysis in self.headers.items():
            print(f"\nã€{source_name.upper()}ã€‘")
            print(f"  ğŸ“‚ íŒŒì¼: {os.path.basename(analysis['file_path'])}")
            print(f"  ğŸ“ˆ í—¤ë” ê°œìˆ˜: {analysis['header_count']}")
            print(f"  ğŸ“ ë°ì´í„° í¬ê¸°: {analysis.get('data_shape', 'N/A')}")
            print(f"  ğŸ“ ì „ì²´ í—¤ë” ëª©ë¡:")
            for i, header in enumerate(analysis['headers'], 1):
                print(f"    {i:2d}. {header}")
            
            # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
            if analysis['sample_data']:
                print(f"  ğŸ” ìƒ˜í”Œ ë°ì´í„° (ì²« ë²ˆì§¸ í•­ëª©):")
                sample = analysis['sample_data'][0]
                for key, value in sample.items():
                    if len(str(value)) > 50:
                        value = str(value)[:50] + "..."
                    print(f"    - {key}: {value}")
        
        # ê³µí†µ í•„ë“œ ë§¤í•‘ ê²°ê³¼
        print(f"\n" + "="*80)
        print("ğŸ”— ê³µí†µ í•„ë“œ ë§¤í•‘ ë¶„ì„")
        print("="*80)
        
        common_fields = self.identify_common_fields()
        for field_name, mappings in common_fields.items():
            print(f"\nğŸ“Œ {field_name}:")
            if mappings:
                for mapping in mappings:
                    print(f"  - {mapping}")
            else:
                print("  - ë§¤í•‘ëœ í•„ë“œ ì—†ìŒ")
    
    async def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ í†µí•© CRM ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ì„ ìœ„í•œ ìƒì„¸ ë¶„ì„ ì‹œì‘")
        print("="*80)
        
        # 1. í—¤ë” ë¶„ì„
        self.analyze_all_files()
        
        # 2. ìƒì„¸ ìš”ì•½ ì¶œë ¥
        self.print_detailed_summary()
        
        # 3. AI í•©ë³‘ ì „ëµ ìƒì„± (ë˜ëŠ” ìˆ˜ë™ ë¶„ì„)
        merge_strategy = await self.get_ai_merge_strategy()
        
        # 4. ê²°ê³¼ ì €ì¥
        if merge_strategy:
            self.save_analysis_results(merge_strategy)
            
            # ì „ëµ ì¶œë ¥
            print("\n" + "="*80)
            print("ğŸ“‹ ë°ì´í„°ë² ì´ìŠ¤ í•©ë³‘ ì „ëµ")
            print("="*80)
            
            # AI ë°©ì‹ ì •ë³´ ì¶œë ¥
            ai_method = merge_strategy.get('ai_method', 'Unknown')
            print(f"ğŸ¤– ì‚¬ìš©ëœ AI ë°©ì‹: {ai_method}")
            
            # AI ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶œë ¥
            if 'ai_analysis' in merge_strategy:
                print(f"\nğŸ“ AI ë¶„ì„ ê²°ê³¼:")
                print("-" * 50)
                print(merge_strategy['ai_analysis'][:2000] + ("..." if len(merge_strategy['ai_analysis']) > 2000 else ""))
                print("-" * 50)
            
            # êµ¬ì¡°í™”ëœ ì „ëµ ì¶œë ¥
            if 'raw_response' in merge_strategy:
                print(merge_strategy['raw_response'])
            else:
                # AI ë¶„ì„ ì œì™¸í•˜ê³  ì¶œë ¥ (ë„ˆë¬´ ê¸¸ì–´ì„œ)
                display_strategy = {k: v for k, v in merge_strategy.items() if k != 'ai_analysis'}
                print(json.dumps(display_strategy, ensure_ascii=False, indent=2))
        
        print("\nâœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ!")
        print("ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ë°ì´í„° í†µí•© ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = DataHeaderAnalyzer()
    await analyzer.run_full_analysis()


if __name__ == "__main__":
    asyncio.run(main())
