#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ
ê¸°ì¡´ jsontoexcel.py + exceltojson.py + combined.py í†µí•©
config.py ì„¤ì • í™œìš©
"""

import json
import pandas as pd
import os
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from pathlib import Path

# í”„ë¡œì íŠ¸ ì„¤ì • import
from utils.settings import *
from utils.logger_utils import LoggerUtils
from utils.file_utils import FileUtils

class DataProcessor:
    """í†µí•© ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.logger = LoggerUtils.setup_logger("data_processor")
        self.conversion_config = CONVERSION_CONFIG
        self.logger.info("ğŸ“Š ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ===== JSON â†” Excel ë³€í™˜ =====
    
    def json_to_excel(self, json_file: Union[str, Path], excel_file: Optional[str] = None, 
                     exclude_fields: Optional[List[str]] = None) -> str:
        """JSON íŒŒì¼ì„ Excelë¡œ ë³€í™˜"""
        try:
            self.logger.info(f"ğŸ“„ JSON â†’ Excel ë³€í™˜ ì‹œì‘: {json_file}")
            
            # íŒŒì¼ ë¡œë“œ
            data = FileUtils.load_json(json_file)
            if not data:
                raise ValueError(f"JSON íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file}")
            
            # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
            if not excel_file:
                excel_file = generate_output_filename("excel_filtered", EXCEL_DIR)
            
            # ì œì™¸ í•„ë“œ ì„¤ì •
            exclude_fields = exclude_fields or self.conversion_config["exclude_fields"]
            
            # ë°ì´í„° í•„í„°ë§
            filtered_data = self._filter_excluded_fields(data, exclude_fields)
            
            # DataFrame ìƒì„±
            df = pd.DataFrame(filtered_data)
            
            # ì»¬ëŸ¼ ìˆœì„œ ì¡°ì •
            df = self._reorder_columns(df)
            
            # Excel íŒŒì¼ë¡œ ì €ì¥
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                # ë©”ì¸ ë°ì´í„° ì‹œíŠ¸
                df.to_excel(writer, sheet_name='ë°ì´í„°', index=False)
                
                # í†µê³„ ì‹œíŠ¸ ìƒì„±
                stats_df = self._create_statistics_sheet(df)
                stats_df.to_excel(writer, sheet_name='í†µê³„', index=False)
            
            self.logger.info(f"âœ… Excel ë³€í™˜ ì™„ë£Œ: {excel_file}")
            self.logger.info(f"ğŸ“Š ë°ì´í„°: {len(df)}í–‰, {len(df.columns)}ì—´")
            
            return str(excel_file)
            
        except Exception as e:
            self.logger.error(f"âŒ JSON â†’ Excel ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def excel_to_json(self, excel_file: Union[str, Path], json_file: Optional[str] = None,
                     sheet_name: str = None) -> str:
        """Excel íŒŒì¼ì„ JSONìœ¼ë¡œ ë³€í™˜"""
        try:
            self.logger.info(f"ğŸ“Š Excel â†’ JSON ë³€í™˜ ì‹œì‘: {excel_file}")
            
            # Excel íŒŒì¼ ì½ê¸°
            if sheet_name:
                df = pd.read_excel(excel_file, sheet_name=sheet_name)
            else:
                df = pd.read_excel(excel_file)
            
            # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
            if not json_file:
                json_file = generate_output_filename("converted_json", JSON_DIR)
            
            # ë°ì´í„° ë³€í™˜
            json_data = []
            for _, row in df.iterrows():
                item = {
                    "name": self._get_value_from_row(row, ['name', 'ì´ë¦„', 'ì—…ì²´ëª…', 'íšŒì‚¬ëª…', 'ê¸°ê´€ëª…']),
                    "category": self._get_value_from_row(row, ['category', 'ì¹´í…Œê³ ë¦¬', 'ì—…ì¢…', 'ë¶„ë¥˜']),
                    "homepage": self._get_value_from_row(row, ['homepage', 'í™ˆí˜ì´ì§€', 'ì›¹ì‚¬ì´íŠ¸', 'website']),
                    "phone": self._get_value_from_row(row, ['phone', 'ì „í™”ë²ˆí˜¸', 'ì „í™”', 'tel']),
                    "fax": self._get_value_from_row(row, ['fax', 'íŒ©ìŠ¤', 'facsimile']),
                    "email": self._get_value_from_row(row, ['email', 'ì´ë©”ì¼', 'mail']),
                    "mobile": self._get_value_from_row(row, ['mobile', 'íœ´ëŒ€í°', 'í•¸ë“œí°', 'ëª¨ë°”ì¼']),
                    "postal_code": self._get_value_from_row(row, ['postal_code', 'ìš°í¸ë²ˆí˜¸', 'zipcode']),
                    "address": self._get_value_from_row(row, ['address', 'ì£¼ì†Œ', 'addr', 'ì†Œì¬ì§€'])
                }
                json_data.append(item)
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            FileUtils.save_json(json_data, json_file)
            
            self.logger.info(f"âœ… JSON ë³€í™˜ ì™„ë£Œ: {json_file}")
            self.logger.info(f"ğŸ“Š ë°ì´í„°: {len(json_data)}ê°œ í•­ëª©")
            
            return str(json_file)
            
        except Exception as e:
            self.logger.error(f"âŒ Excel â†’ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    # ===== ë°ì´í„° ê²°í•© ë° ë³‘í•© =====
    
    def combine_datasets(self, file_paths: List[Union[str, Path]], 
                        output_file: Optional[str] = None) -> str:
        """ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ ê²°í•©"""
        try:
            self.logger.info(f"ğŸ”— ë°ì´í„°ì…‹ ê²°í•© ì‹œì‘: {len(file_paths)}ê°œ íŒŒì¼")
            
            combined_data = []
            
            for file_path in file_paths:
                self.logger.info(f"  ğŸ“‚ ë¡œë”©: {file_path}")
                
                if str(file_path).endswith('.json'):
                    data = FileUtils.load_json(file_path)
                elif str(file_path).endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(file_path)
                    data = df.to_dict('records')
                else:
                    self.logger.warning(f"  âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
                    continue
                
                if isinstance(data, list):
                    combined_data.extend(data)
                elif isinstance(data, dict):
                    # ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì¸ ê²½ìš° ê°’ë“¤ì„ ì¶”ì¶œ
                    for key, value in data.items():
                        if isinstance(value, list):
                            combined_data.extend(value)
                
                self.logger.info(f"  âœ… ë¡œë”© ì™„ë£Œ: {len(data) if isinstance(data, list) else '?'}ê°œ í•­ëª©")
            
            # ì¤‘ë³µ ì œê±°
            unique_data = self._remove_duplicates(combined_data)
            
            # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
            if not output_file:
                output_file = generate_output_filename("combined", JSON_DIR)
            
            # ê²°í•©ëœ ë°ì´í„° ì €ì¥
            FileUtils.save_json(unique_data, output_file)
            
            self.logger.info(f"âœ… ë°ì´í„°ì…‹ ê²°í•© ì™„ë£Œ: {output_file}")
            self.logger.info(f"ğŸ“Š ì´ {len(unique_data)}ê°œ í•­ëª© (ì¤‘ë³µ ì œê±° í›„)")
            
            return str(output_file)
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„°ì…‹ ê²°í•© ì‹¤íŒ¨: {e}")
            raise
    
    def merge_with_validation(self, primary_file: Union[str, Path], 
                            secondary_file: Union[str, Path],
                            output_file: Optional[str] = None) -> str:
        """ê²€ì¦ì„ í†µí•œ ë°ì´í„° ë³‘í•©"""
        try:
            self.logger.info(f"ğŸ” ê²€ì¦ ë³‘í•© ì‹œì‘")
            self.logger.info(f"  ğŸ“‚ ì£¼ íŒŒì¼: {primary_file}")
            self.logger.info(f"  ğŸ“‚ ë³´ì¡° íŒŒì¼: {secondary_file}")
            
            # ë°ì´í„° ë¡œë“œ
            primary_data = FileUtils.load_json(primary_file)
            secondary_data = FileUtils.load_json(secondary_file)
            
            if not primary_data or not secondary_data:
                raise ValueError("íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë³‘í•© ë¡œì§
            merged_data = []
            
            for primary_item in primary_data:
                # ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­
                primary_name = primary_item.get('name', '').strip().lower()
                
                # ë³´ì¡° ë°ì´í„°ì—ì„œ ë§¤ì¹­ë˜ëŠ” í•­ëª© ì°¾ê¸°
                matched_item = None
                for secondary_item in secondary_data:
                    secondary_name = secondary_item.get('name', '').strip().lower()
                    if primary_name == secondary_name:
                        matched_item = secondary_item
                        break
                
                # ë³‘í•©
                if matched_item:
                    merged_item = self._merge_items(primary_item, matched_item)
                    merged_item['merge_status'] = 'matched'
                else:
                    merged_item = primary_item.copy()
                    merged_item['merge_status'] = 'primary_only'
                
                merged_data.append(merged_item)
            
            # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
            if not output_file:
                output_file = generate_output_filename("merged", JSON_DIR)
            
            # ë³‘í•©ëœ ë°ì´í„° ì €ì¥
            FileUtils.save_json(merged_data, output_file)
            
            # í†µê³„ ì¶œë ¥
            matched_count = len([item for item in merged_data if item.get('merge_status') == 'matched'])
            self.logger.info(f"âœ… ê²€ì¦ ë³‘í•© ì™„ë£Œ: {output_file}")
            self.logger.info(f"ğŸ“Š ì´ {len(merged_data)}ê°œ í•­ëª©, {matched_count}ê°œ ë§¤ì¹­")
            
            return str(output_file)
            
        except Exception as e:
            self.logger.error(f"âŒ ê²€ì¦ ë³‘í•© ì‹¤íŒ¨: {e}")
            raise
    
    # ===== ë°ì´í„° ë¶„ì„ ë° í†µê³„ =====
    
    def analyze_data(self, data_file: Union[str, Path]) -> Dict[str, Any]:
        """ë°ì´í„° ë¶„ì„"""
        try:
            self.logger.info(f"ğŸ“ˆ ë°ì´í„° ë¶„ì„ ì‹œì‘: {data_file}")
            
            # ë°ì´í„° ë¡œë“œ
            data = FileUtils.load_json(data_file)
            if not data:
                raise ValueError(f"ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
            
            analysis = {
                "basic_stats": self._get_basic_stats(data),
                "field_analysis": self._analyze_fields(data),
                "quality_metrics": self._calculate_quality_metrics(data),
                "analysis_timestamp": datetime.now().isoformat()
            }
            
            self.logger.info(f"âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
            return analysis
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    def generate_report(self, data_file: Union[str, Path], 
                       output_file: Optional[str] = None) -> str:
        """ë°ì´í„° ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            # ë¶„ì„ ì‹¤í–‰
            analysis = self.analyze_data(data_file)
            
            # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
            if not output_file:
                output_file = generate_output_filename("report", OUTPUT_DIR)
                output_file = str(output_file).replace('.json', '_report.json')
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            FileUtils.save_json(analysis, output_file)
            
            self.logger.info(f"ğŸ“‹ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_file}")
            return str(output_file)
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    # ===== ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œë“¤ =====
    
    def _filter_excluded_fields(self, data: List[Dict], exclude_fields: List[str]) -> List[Dict]:
        """ì œì™¸ í•„ë“œ í•„í„°ë§"""
        if not exclude_fields:
            return data
        
        filtered_data = []
        for item in data:
            filtered_item = {k: v for k, v in item.items() if k not in exclude_fields}
            filtered_data.append(filtered_item)
        
        return filtered_data
    
    def _reorder_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬"""
        priority_columns = self.conversion_config["priority_columns"]
        
        # ì¡´ì¬í•˜ëŠ” ìš°ì„ ìˆœìœ„ ì»¬ëŸ¼ë“¤ë§Œ í•„í„°ë§
        existing_priority = [col for col in priority_columns if col in df.columns]
        remaining_columns = [col for col in df.columns if col not in existing_priority]
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        new_column_order = existing_priority + remaining_columns
        return df[new_column_order]
    
    def _create_statistics_sheet(self, df: pd.DataFrame) -> pd.DataFrame:
        """í†µê³„ ì‹œíŠ¸ ìƒì„±"""
        stats_data = []
        
        for column in df.columns:
            non_null_count = df[column].notna().sum()
            non_empty_count = df[column].astype(str).str.strip().ne('').sum()
            
            stats_data.append({
                'í•„ë“œëª…': column,
                'ì „ì²´_ë ˆì½”ë“œìˆ˜': len(df),
                'ë¹„ì–´ìˆì§€ì•Šì€_ë ˆì½”ë“œìˆ˜': non_empty_count,
                'ì±„ì›€ë¥ _í¼ì„¼íŠ¸': round((non_empty_count / len(df)) * 100, 1),
                'ìƒ˜í”Œ_ë°ì´í„°': str(df[column].dropna().iloc[0] if not df[column].dropna().empty else '')[:50]
            })
        
        return pd.DataFrame(stats_data)
    
    def _get_value_from_row(self, row, column_names: List[str]) -> str:
        """í–‰ì—ì„œ ê°’ ì¶”ì¶œ"""
        for col_name in column_names:
            if col_name in row.index and pd.notna(row[col_name]):
                value = str(row[col_name]).strip()
                if value and value.lower() not in ['nan', 'none', 'null', '']:
                    return value
        return ""
    
    def _remove_duplicates(self, data: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ì œê±°"""
        seen_names = set()
        unique_data = []
        
        for item in data:
            name = item.get('name', '').strip().lower()
            if name and name not in seen_names:
                seen_names.add(name)
                unique_data.append(item)
        
        return unique_data
    
    def _merge_items(self, primary: Dict, secondary: Dict) -> Dict:
        """ë‘ í•­ëª© ë³‘í•©"""
        merged = primary.copy()
        
        # ë¹ˆ í•„ë“œë¥¼ ë³´ì¡° ë°ì´í„°ë¡œ ì±„ìš°ê¸°
        for key, value in secondary.items():
            if key not in merged or not str(merged.get(key, '')).strip():
                merged[key] = value
        
        return merged
    
    def _get_basic_stats(self, data: List[Dict]) -> Dict[str, Any]:
        """ê¸°ë³¸ í†µê³„"""
        return {
            "total_records": len(data),
            "unique_names": len(set(item.get('name', '') for item in data if item.get('name'))),
            "categories": list(set(item.get('category', '') for item in data if item.get('category'))),
            "has_homepage": len([item for item in data if item.get('homepage')]),
            "has_phone": len([item for item in data if item.get('phone')]),
            "has_email": len([item for item in data if item.get('email')])
        }
    
    def _analyze_fields(self, data: List[Dict]) -> Dict[str, Any]:
        """í•„ë“œ ë¶„ì„"""
        if not data:
            return {}
        
        all_fields = set()
        for item in data:
            all_fields.update(item.keys())
        
        field_analysis = {}
        for field in all_fields:
            non_empty_count = len([item for item in data if item.get(field) and str(item.get(field)).strip()])
            field_analysis[field] = {
                "fill_rate": (non_empty_count / len(data)) * 100,
                "non_empty_count": non_empty_count,
                "total_count": len(data)
            }
        
        return field_analysis
    
    def _calculate_quality_metrics(self, data: List[Dict]) -> Dict[str, Any]:
        """í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
        if not data:
            return {}
        
        # í•„ìˆ˜ í•„ë“œ ì±„ì›€ë¥ 
        required_fields = ["name", "category"]
        optional_fields = ["phone", "fax", "email", "address"]
        
        required_score = 0
        for field in required_fields:
            filled = len([item for item in data if item.get(field) and str(item.get(field)).strip()])
            required_score += (filled / len(data)) * 100
        
        optional_score = 0
        for field in optional_fields:
            filled = len([item for item in data if item.get(field) and str(item.get(field)).strip()])
            optional_score += (filled / len(data)) * 100
        
        return {
            "required_fields_score": required_score / len(required_fields),
            "optional_fields_score": optional_score / len(optional_fields),
            "overall_score": (required_score / len(required_fields) * 0.7 + 
                            optional_score / len(optional_fields) * 0.3)
        }

# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_json_to_excel(json_file: str, excel_file: str = None) -> str:
    """ë¹ ë¥¸ JSON â†’ Excel ë³€í™˜"""
    processor = DataProcessor()
    return processor.json_to_excel(json_file, excel_file)

def quick_excel_to_json(excel_file: str, json_file: str = None) -> str:
    """ë¹ ë¥¸ Excel â†’ JSON ë³€í™˜"""
    processor = DataProcessor()
    return processor.excel_to_json(excel_file, json_file)

def quick_combine(file_paths: List[str], output_file: str = None) -> str:
    """ë¹ ë¥¸ ë°ì´í„° ê²°í•©"""
    processor = DataProcessor()
    return processor.combine_datasets(file_paths, output_file)

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“Š í†µí•© ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ")
    print("="*60)
    
    try:
        # í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
        initialize_project()
        
        # ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„±
        processor = DataProcessor()
        
        # ìµœì‹  JSON íŒŒì¼ ì°¾ê¸°
        latest_json = get_latest_input_file()
        if latest_json:
            print(f"ğŸ“‚ ìµœì‹  JSON íŒŒì¼: {latest_json}")
            
            # Excelë¡œ ë³€í™˜
            excel_file = processor.json_to_excel(latest_json)
            print(f"âœ… Excel ë³€í™˜ ì™„ë£Œ: {excel_file}")
            
            # ë°ì´í„° ë¶„ì„
            analysis = processor.analyze_data(latest_json)
            print(f"ğŸ“ˆ ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
            print(f"  - ì´ ë ˆì½”ë“œ: {analysis['basic_stats']['total_records']}ê°œ")
            print(f"  - í’ˆì§ˆ ì ìˆ˜: {analysis['quality_metrics']['overall_score']:.1f}ì ")
        else:
            print("âŒ ì²˜ë¦¬í•  JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main() 