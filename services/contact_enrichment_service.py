#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì—°ë½ì²˜ ì •ë³´ ë³´ê°• ì„œë¹„ìŠ¤
CRM DBì—ì„œ ëˆ„ë½ëœ phone/homepage/address/fax ì •ë³´ë¥¼ crawler_main.pyë¡œ ìë™ ê²€ìƒ‰
"""

import asyncio
import logging
import json
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass

from database.database import get_database
from crawler_main import ModularUnifiedCrawler
from utils.logger_utils import LoggerUtils

@dataclass
class EnrichmentRequest:
    """ì—°ë½ì²˜ ë³´ê°• ìš”ì²­"""
    org_id: int
    org_name: str
    missing_fields: List[str]
    priority: str = "MEDIUM"
    requested_by: str = "SYSTEM"

@dataclass
class EnrichmentResult:
    """ì—°ë½ì²˜ ë³´ê°• ê²°ê³¼"""
    org_id: int
    success: bool
    found_data: Dict[str, str]
    missing_fields: List[str]
    error_message: Optional[str] = None
    processing_time: float = 0.0

class ContactEnrichmentService:
    """ì—°ë½ì²˜ ì •ë³´ ë³´ê°• ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.db = get_database()
        self.logger = LoggerUtils.setup_logger(name="contact_enrichment", file_logging=False)
        self.crawler = None
        
        # í†µê³„
        self.stats = {
            "total_processed": 0,
            "successful_enrichments": 0,
            "failed_enrichments": 0,
            "fields_found": {
                "phone": 0,
                "fax": 0,
                "email": 0,
                "homepage": 0,
                "address": 0
            }
        }
        
        self.logger.info("ğŸ” ì—°ë½ì²˜ ë³´ê°• ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_crawler(self) -> ModularUnifiedCrawler:
        """í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ê°œì„ ëœ ë²„ì „)"""
        if not self.crawler:
            try:
                # ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜ ì •ì˜
                def progress_callback(data):
                    try:
                        status = data.get('status', 'unknown')
                        name = data.get('name', 'Unknown')
                        self.logger.info(f"ğŸ”„ í¬ë¡¤ë§ ì§„í–‰: {name} - {status}")
                    except Exception as e:
                        self.logger.debug(f"ì½œë°± ì˜¤ë¥˜: {e}")
                
                # ModularUnifiedCrawler ì´ˆê¸°í™” (ì½œë°± í¬í•¨)
                self.crawler = ModularUnifiedCrawler(progress_callback=progress_callback)
                self.logger.info("ğŸ¤– ModularUnifiedCrawler ì´ˆê¸°í™” ì„±ê³µ (ì½œë°± í¬í•¨)")
                
            except Exception as e:
                self.logger.error(f"âŒ ModularUnifiedCrawler ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise
        
        return self.crawler
    
    def find_organizations_with_missing_contacts(self, limit: int = 100) -> List[EnrichmentRequest]:
        """ëˆ„ë½ëœ ì—°ë½ì²˜ ì •ë³´ê°€ ìˆëŠ” ê¸°ê´€ë“¤ ì°¾ê¸°"""
        try:
            self.logger.info(f"ğŸ” ëˆ„ë½ëœ ì—°ë½ì²˜ ì •ë³´ ê¸°ê´€ ê²€ìƒ‰ (ìµœëŒ€ {limit}ê°œ)")
            
            query = """
            SELECT id, name, homepage, phone, fax, email, address, type, category
            FROM organizations 
            WHERE is_active = 1
            AND (
                phone IS NULL OR phone = '' OR
                fax IS NULL OR fax = '' OR
                email IS NULL OR email = '' OR
                homepage IS NULL OR homepage = '' OR
                address IS NULL OR address = ''
            )
            ORDER BY 
                CASE 
                    WHEN priority = 'HIGH' THEN 1
                    WHEN priority = 'MEDIUM' THEN 2
                    ELSE 3
                END,
                updated_at DESC
            LIMIT ?
            """
            
            with self.db.get_connection() as conn:
                cursor = conn.execute(query, (limit,))
                organizations = cursor.fetchall()
            
            requests = []
            for org in organizations:
                missing_fields = []
                
                # ê° í•„ë“œë³„ ëˆ„ë½ í™•ì¸
                if not org['phone'] or org['phone'].strip() == '':
                    missing_fields.append('phone')
                if not org['fax'] or org['fax'].strip() == '':
                    missing_fields.append('fax')
                if not org['email'] or org['email'].strip() == '':
                    missing_fields.append('email')
                if not org['homepage'] or org['homepage'].strip() == '':
                    missing_fields.append('homepage')
                if not org['address'] or org['address'].strip() == '':
                    missing_fields.append('address')
                
                if missing_fields:
                    requests.append(EnrichmentRequest(
                        org_id=org['id'],
                        org_name=org['name'],
                        missing_fields=missing_fields
                    ))
            
            self.logger.info(f"âœ… {len(requests)}ê°œ ê¸°ê´€ì— ëˆ„ë½ëœ ì—°ë½ì²˜ ì •ë³´ ë°œê²¬")
            return requests
            
        except Exception as e:
            self.logger.error(f"âŒ ëˆ„ë½ ê¸°ê´€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def enrich_single_organization(self, request: EnrichmentRequest) -> EnrichmentResult:
        """ë‹¨ì¼ ê¸°ê´€ì˜ ì—°ë½ì²˜ ì •ë³´ ë³´ê°• - crawler_main.py ì™„ì „ í†µí•©"""
        start_time = datetime.now()
        
        try:
            self.logger.info(f"ğŸ” ì—°ë½ì²˜ ë³´ê°• ì‹œì‘: {request.org_name} (ID: {request.org_id})")
            self.logger.info(f"  ğŸ“‹ ëˆ„ë½ í•„ë“œ: {', '.join(request.missing_fields)}")
            
            # ğŸ”§ ìˆ˜ì •: ê¸°ì¡´ í¬ë¡¤ëŸ¬ ì‚¬ìš© (ë§¤ë²ˆ ìƒˆë¡œ ë§Œë“¤ì§€ ì•ŠìŒ)
            crawler = self.get_crawler()
            
            # ğŸ”§ ìˆ˜ì •: ëª¨ë“ˆ ì´ˆê¸°í™” í™•ì¸ ë° ì‹¤í–‰
            if not hasattr(crawler, '_modules_initialized') or not crawler._modules_initialized:
                crawler.initialize_modules()
                crawler._modules_initialized = True
                self.logger.info("ğŸ”§ í¬ë¡¤ëŸ¬ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ê¸°ê´€ ì •ë³´ë¥¼ í¬ë¡¤ëŸ¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            org_data = {
                "name": request.org_name,
                "category": "ê¸°ê´€",
                "homepage": "",
                "phone": "",
                "fax": "",
                "email": "",
                "address": ""
            }
            
            # ğŸš€ ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰ - crawler_main.pyì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ í™œìš©
            try:
                # 1. í™ˆí˜ì´ì§€ ê²€ìƒ‰
                if not org_data.get('homepage'):
                    homepage_result = await crawler.search_homepage(request.org_name)
                    if homepage_result and homepage_result.get('homepage'):
                        org_data['homepage'] = homepage_result['homepage']
                        self.logger.info(f"  ğŸŒ í™ˆí˜ì´ì§€ ë°œê²¬: {homepage_result['homepage']}")
                
                # 2. í™ˆí˜ì´ì§€ì—ì„œ ì—°ë½ì²˜ ì¶”ì¶œ
                if org_data.get('homepage'):
                    homepage_details = await crawler.extract_details_from_homepage(org_data['homepage'])
                    
                    # ê²°ê³¼ ë³‘í•©
                    for field in ['phone', 'fax', 'email', 'address']:
                        if homepage_details.get(field) and not org_data.get(field):
                            org_data[field] = homepage_details[field]
                            self.logger.info(f"  âœ… í™ˆí˜ì´ì§€ì—ì„œ {field} ë°œê²¬: {homepage_details[field]}")
                
                # 3. êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ëˆ„ë½ ì •ë³´ ë³´ì™„
                missing_fields = [field for field in request.missing_fields 
                                if not org_data.get(field) or org_data[field].strip() == ""]
                
                if missing_fields:
                    self.logger.info(f"  ğŸ” êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ëˆ„ë½ ì •ë³´ ê²€ìƒ‰: {missing_fields}")
                    google_results = await crawler.search_missing_info(request.org_name, missing_fields)
                    
                    # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ë³‘í•©
                    for field, value in google_results.items():
                        if value and value.strip() and not org_data.get(field):
                            org_data[field] = value
                            self.logger.info(f"  âœ… êµ¬ê¸€ ê²€ìƒ‰ì—ì„œ {field} ë°œê²¬: {value}")
                
                # 4. ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
                org_data = crawler.validate_and_clean_data(org_data)
                
            except Exception as crawl_error:
                self.logger.error(f"  âŒ í¬ë¡¤ë§ ê³¼ì • ì˜¤ë¥˜: {crawl_error}")
                # í¬ë¡¤ë§ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            
            # ê²°ê³¼ì—ì„œ ì°¾ì€ ë°ì´í„° ì¶”ì¶œ
            found_data = {}
            still_missing = []
            
            for field in request.missing_fields:
                value = org_data.get(field, "")
                if value and str(value).strip() and str(value).strip() != "":
                    found_data[field] = str(value).strip()
                    self.stats["fields_found"][field] += 1
                    self.logger.info(f"  âœ… {field} ë°œê²¬: {value}")
                else:
                    still_missing.append(field)
            
            # DB ì—…ë°ì´íŠ¸
            if found_data:
                success = self.update_organization_contacts(request.org_id, found_data, request.requested_by)
                if success:
                    self.logger.info(f"  ğŸ’¾ DB ì—…ë°ì´íŠ¸ ì„±ê³µ: {len(found_data)}ê°œ í•„ë“œ")
                else:
                    self.logger.warning(f"  âš ï¸ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = EnrichmentResult(
                org_id=request.org_id,
                success=len(found_data) > 0,
                found_data=found_data,
                missing_fields=still_missing,
                processing_time=processing_time
            )
            
            if result.success:
                self.stats["successful_enrichments"] += 1
                self.logger.info(f"  ğŸ‰ ë³´ê°• ì„±ê³µ: {len(found_data)}ê°œ í•„ë“œ ë°œê²¬")
            else:
                self.stats["failed_enrichments"] += 1
                self.logger.warning(f"  âš ï¸ ë³´ê°• ì‹¤íŒ¨: ì—°ë½ì²˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            self.stats["total_processed"] += 1
            
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            self.logger.error(f"âŒ ì—°ë½ì²˜ ë³´ê°• ì‹¤íŒ¨: {request.org_name} - {e}")
            
            self.stats["failed_enrichments"] += 1
            self.stats["total_processed"] += 1
            
            return EnrichmentResult(
                org_id=request.org_id,
                success=False,
                found_data={},
                missing_fields=request.missing_fields,
                error_message=str(e),
                processing_time=processing_time
            )
            
    def update_organization_contacts(self, org_id: int, contact_data: Dict[str, str], updated_by: str) -> bool:
        """ê¸°ê´€ì˜ ì—°ë½ì²˜ ì •ë³´ ì—…ë°ì´íŠ¸"""
        try:
            # ì—…ë°ì´íŠ¸í•  í•„ë“œ ì¤€ë¹„
            updates = contact_data.copy()
            updates['updated_by'] = updated_by
            updates['updated_at'] = datetime.now().isoformat()
            
            # ğŸ”¥ í¬ë¡¤ë§ ìƒíƒœ í•„ë“œ ì¶”ê°€
            updates['ai_crawled'] = True
            updates['last_crawled_at'] = datetime.now().isoformat()
            
            # í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            crawling_metadata = {
                "last_enrichment": datetime.now().isoformat(),
                "enrichment_source": "ModularUnifiedCrawler",
                "found_fields": list(contact_data.keys())
            }
            
            # ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë³‘í•©
            with self.db.get_connection() as conn:
                cursor = conn.execute("SELECT crawling_data FROM organizations WHERE id = ?", (org_id,))
                row = cursor.fetchone()
                
                if row and row['crawling_data']:
                    try:
                        existing_data = json.loads(row['crawling_data'])
                        existing_data.update(crawling_metadata)
                        updates['crawling_data'] = json.dumps(existing_data, ensure_ascii=False)
                    except:
                        updates['crawling_data'] = json.dumps(crawling_metadata, ensure_ascii=False)
                else:
                    updates['crawling_data'] = json.dumps(crawling_metadata, ensure_ascii=False)
            
            # DB ì—…ë°ì´íŠ¸
            success = self.db.update_organization(org_id, updates, updated_by)
            
            if success:
                self.logger.info(f"âœ… ê¸°ê´€ ID {org_id} ì—°ë½ì²˜ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                self.logger.info(f"ğŸ¯ í¬ë¡¤ë§ ìƒíƒœ: ai_crawled=True, last_crawled_at={updates['last_crawled_at']}")
            
            return success
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ê´€ ì—°ë½ì²˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    async def enrich_multiple_organizations(self, requests: List[EnrichmentRequest], 
                                          max_concurrent: int = 3) -> List[EnrichmentResult]:
        """ì—¬ëŸ¬ ê¸°ê´€ì˜ ì—°ë½ì²˜ ì •ë³´ ì¼ê´„ ë³´ê°•"""
        try:
            self.logger.info(f"ğŸš€ ì¼ê´„ ì—°ë½ì²˜ ë³´ê°• ì‹œì‘: {len(requests)}ê°œ ê¸°ê´€")
            
            # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def enrich_with_semaphore(request):
                async with semaphore:
                    return await self.enrich_single_organization(request)
            
            # ë³‘ë ¬ ì²˜ë¦¬
            tasks = [enrich_with_semaphore(req) for req in requests]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ì˜ˆì™¸ ì²˜ë¦¬
            final_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"âŒ ê¸°ê´€ {requests[i].org_name} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {result}")
                    final_results.append(EnrichmentResult(
                        org_id=requests[i].org_id,
                        success=False,
                        found_data={},
                        missing_fields=requests[i].missing_fields,
                        error_message=str(result)
                    ))
                else:
                    final_results.append(result)
            
            # í†µê³„ ì¶œë ¥
            self.print_enrichment_statistics(final_results)
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"âŒ ì¼ê´„ ì—°ë½ì²˜ ë³´ê°• ì‹¤íŒ¨: {e}")
            return []
    
    async def auto_enrich_missing_contacts(self, limit: int = 50, max_concurrent: int = 3) -> Dict[str, Any]:
        """ëˆ„ë½ëœ ì—°ë½ì²˜ ì •ë³´ ìë™ ë³´ê°•"""
        try:
            self.logger.info(f"ğŸ¤– ìë™ ì—°ë½ì²˜ ë³´ê°• ì‹œì‘ (ìµœëŒ€ {limit}ê°œ ê¸°ê´€)")
            
            # 1. ëˆ„ë½ëœ ì—°ë½ì²˜ê°€ ìˆëŠ” ê¸°ê´€ë“¤ ì°¾ê¸°
            requests = self.find_organizations_with_missing_contacts(limit)
            
            if not requests:
                self.logger.info("âœ… ë³´ê°•ì´ í•„ìš”í•œ ê¸°ê´€ì´ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "completed",
                    "message": "ë³´ê°•ì´ í•„ìš”í•œ ê¸°ê´€ì´ ì—†ìŠµë‹ˆë‹¤.",
                    "processed_count": 0,
                    "successful_count": 0,
                    "results": []
                }
            
            # 2. ì¼ê´„ ë³´ê°• ì‹¤í–‰
            results = await self.enrich_multiple_organizations(requests, max_concurrent)
            
            # 3. ê²°ê³¼ ìš”ì•½
            successful_count = sum(1 for r in results if r.success)
            
            summary = {
                "status": "completed",
                "message": f"{successful_count}/{len(results)}ê°œ ê¸°ê´€ ì—°ë½ì²˜ ë³´ê°• ì™„ë£Œ",
                "processed_count": len(results),
                "successful_count": successful_count,
                "total_fields_found": sum(len(r.found_data) for r in results),
                "results": results,
                "statistics": self.stats.copy()
            }
            
            self.logger.info(f"ğŸ‰ ìë™ ì—°ë½ì²˜ ë³´ê°• ì™„ë£Œ: {successful_count}/{len(results)}ê°œ ì„±ê³µ")
            
            return summary
            
        except Exception as e:
            self.logger.error(f"âŒ ìë™ ì—°ë½ì²˜ ë³´ê°• ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "message": f"ìë™ ë³´ê°• ì‹¤íŒ¨: {str(e)}",
                "processed_count": 0,
                "successful_count": 0,
                "results": []
            }
    
    async def start_auto_enrichment(self, limit: int = 100, max_concurrent: int = 3) -> Dict[str, Any]:
        """ìë™ ë³´ê°• ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìš©) - crm_app.py í˜¸ì¶œìš©"""
        try:
            import uuid
            from datetime import datetime
            
            # ì‘ì—… ID ìƒì„±
            job_id = f"auto_enrichment_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"
            
            self.logger.info(f"ğŸš€ ìë™ ë³´ê°• ì‘ì—… ì‹œì‘: {job_id} (ìµœëŒ€ {limit}ê°œ ê¸°ê´€)")
            
            # ì‹¤ì œ ë³´ê°• ì‹¤í–‰ (ê¸°ì¡´ ë©”ì„œë“œ í™œìš©)
            result = await self.auto_enrich_missing_contacts(limit, max_concurrent)
            
            # ì‘ì—… ID ì¶”ê°€í•˜ì—¬ ë°˜í™˜
            result["job_id"] = job_id
            result["estimated_time"] = limit * 2  # ê¸°ê´€ë‹¹ í‰ê·  2ì´ˆ ì˜ˆìƒ
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ìë™ ë³´ê°• ì‹œì‘ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "message": f"ìë™ ë³´ê°• ì‹œì‘ ì‹¤íŒ¨: {str(e)}",
                "job_id": None,
                "processed_count": 0,
                "successful_count": 0
            }
    
    def print_enrichment_statistics(self, results: List[EnrichmentResult]):
        """ë³´ê°• í†µê³„ ì¶œë ¥"""
        successful = [r for r in results if r.success]
        failed = [r for r in results if not r.success]
        
        total_fields_found = sum(len(r.found_data) for r in successful)
        avg_processing_time = sum(r.processing_time for r in results) / len(results) if results else 0
        
        print("\n" + "="*60)
        print("ğŸ“Š ì—°ë½ì²˜ ë³´ê°• í†µê³„")
        print("="*60)
        print(f"ğŸ“‹ ì´ ì²˜ë¦¬: {len(results)}ê°œ ê¸°ê´€")
        print(f"âœ… ì„±ê³µ: {len(successful)}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {len(failed)}ê°œ")
        print(f"ğŸ“ ë°œê²¬ëœ ì—°ë½ì²˜: {total_fields_found}ê°œ")
        print(f"â±ï¸ í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_processing_time:.2f}ì´ˆ")
        
        if total_fields_found > 0:
            print("\nğŸ“ˆ í•„ë“œë³„ ë°œê²¬ í†µê³„:")
            for field, count in self.stats["fields_found"].items():
                if count > 0:
                    print(f"  {field}: {count}ê°œ")
        
        print("="*60)
    
    def get_enrichment_history(self, org_id: int) -> List[Dict[str, Any]]:
        """ê¸°ê´€ì˜ ì—°ë½ì²˜ ë³´ê°• ì´ë ¥ ì¡°íšŒ"""
        try:
            with self.db.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT crawling_data, updated_at, updated_by
                    FROM organizations 
                    WHERE id = ?
                """, (org_id,))
                
                row = cursor.fetchone()
                if row and row['crawling_data']:
                    try:
                        crawling_data = json.loads(row['crawling_data'])
                        return [{
                            "enrichment_date": crawling_data.get('last_enrichment'),
                            "source": crawling_data.get('enrichment_source'),
                            "found_fields": crawling_data.get('found_fields', []),
                            "updated_by": row['updated_by'],
                            "updated_at": row['updated_at']
                        }]
                    except:
                        pass
                
                return []
                
        except Exception as e:
            self.logger.error(f"âŒ ë³´ê°• ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

# í¸ì˜ í•¨ìˆ˜ë“¤
async def auto_enrich_contacts(limit: int = 50) -> Dict[str, Any]:
    """ì—°ë½ì²˜ ìë™ ë³´ê°• í¸ì˜ í•¨ìˆ˜"""
    service = ContactEnrichmentService()
    return await service.auto_enrich_missing_contacts(limit)

async def enrich_organization_by_id(org_id: int) -> Optional[EnrichmentResult]:
    """íŠ¹ì • ê¸°ê´€ ì—°ë½ì²˜ ë³´ê°• í¸ì˜ í•¨ìˆ˜"""
    try:
        service = ContactEnrichmentService()
        db = get_database()
        
        # ê¸°ê´€ ì •ë³´ ì¡°íšŒ
        with db.get_connection() as conn:
            cursor = conn.execute("SELECT id, name FROM organizations WHERE id = ?", (org_id,))
            org = cursor.fetchone()
            
            if not org:
                return None
        
        # ë³´ê°• ìš”ì²­ ìƒì„±
        request = EnrichmentRequest(
            org_id=org['id'],
            org_name=org['name'],
            missing_fields=['phone', 'fax', 'email', 'homepage', 'address']  # ëª¨ë“  í•„ë“œ ì²´í¬
        )
        
        # ë³´ê°• ì‹¤í–‰
        result = await service.enrich_single_organization(request)
        return result
        
    except Exception as e:
        logging.error(f"âŒ ê¸°ê´€ ID {org_id} ì—°ë½ì²˜ ë³´ê°• ì‹¤íŒ¨: {e}")
        return None 