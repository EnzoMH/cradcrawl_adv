#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í¬ë¡¤ë§ ì„œë¹„ìŠ¤
íŒŒì¼ ê¸°ë°˜ í¬ë¡¤ë§ê³¼ DB ê¸°ë°˜ í¬ë¡¤ë§ì„ í†µí•© ê´€ë¦¬
"""

import asyncio
import json
import os
import threading
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass

from database.database import get_database
from utils.file_utils import FileUtils
from utils.logger_utils import LoggerUtils

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

logger = LoggerUtils.setup_logger(name="crawling_service", file_logging=True)

@dataclass
class CrawlingJobConfig:
    """í¬ë¡¤ë§ ì‘ì—… ì„¤ì •"""
    test_mode: bool = False
    test_count: int = 10
    use_ai: bool = True
    max_concurrent: int = 3
    job_name: Optional[str] = None
    started_by: str = "SYSTEM"

@dataclass
class CrawlingJobStatus:
    """í¬ë¡¤ë§ ì‘ì—… ìƒíƒœ"""
    job_id: Optional[int] = None
    status: str = "IDLE"  # IDLE, RUNNING, COMPLETED, ERROR
    total_count: int = 0
    processed_count: int = 0
    failed_count: int = 0
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    error_message: Optional[str] = None
    data_file: Optional[str] = None

class CrawlingService:
    """í¬ë¡¤ë§ ì„œë¹„ìŠ¤ í†µí•© ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.db = get_database()
        self.logger = logger
        
        # í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬
        self.current_job: Optional[CrawlingJobStatus] = None
        self.extractor_instance = None
        self.total_organizations = []
        
        # ì§€ì›í•˜ëŠ” ë°ì´í„° íŒŒì¼ ê²½ë¡œë“¤
        self.data_file_paths = [
            "data/json/merged_church_data_20250618_174032.json",
            "raw_data_0530.json", 
            "raw_data.json"
        ]
    
    def find_data_file(self) -> Optional[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ì°¾ê¸°"""
        for file_path in self.data_file_paths:
            if os.path.exists(file_path):
                self.logger.info(f"âœ… ë°ì´í„° íŒŒì¼ ë°œê²¬: {file_path}")
                return file_path
        
        self.logger.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    def load_organizations_from_file(self, file_path: str) -> List[Dict[str, Any]]:
        """íŒŒì¼ì—ì„œ ì¡°ì§ ë°ì´í„° ë¡œë“œ"""
        try:
            data = FileUtils.load_json(file_path)
            organizations = []
            
            if isinstance(data, dict):
                for category, orgs in data.items():
                    if isinstance(orgs, list):
                        for org in orgs:
                            if isinstance(org, dict):
                                org["category"] = category
                                organizations.append(org)
            elif isinstance(data, list):
                organizations = [org for org in data if isinstance(org, dict)]
            
            self.logger.info(f"âœ… ì¡°ì§ ë°ì´í„° ë¡œë“œ: {len(organizations)}ê°œ")
            return organizations
            
        except Exception as e:
            self.logger.error(f"âŒ ì¡°ì§ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def create_crawling_job(self, config: CrawlingJobConfig, organizations: List[Dict]) -> int:
        """í¬ë¡¤ë§ ì‘ì—… ìƒì„±"""
        try:
            job_name = config.job_name or f"Crawling Job {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            total_count = config.test_count if config.test_mode else len(organizations)
            
            job_data = {
                'job_name': job_name,
                'total_count': total_count,
                'started_by': config.started_by,
                'config_data': json.dumps(config.__dict__)
            }
            
            job_id = self.db.create_crawling_job(job_data)
            self.logger.info(f"âœ… í¬ë¡¤ë§ ì‘ì—… ìƒì„±: ID {job_id}, ì´ {total_count}ê°œ")
            
            return job_id
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ë¡¤ë§ ì‘ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def create_progress_callback(self, job_id: int) -> Callable:
        """ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜ ìƒì„±"""
        def progress_callback(result: dict):
            """í¬ë¡¤ë§ ì§„í–‰ ìƒí™© ì½œë°±"""
            try:
                result_data = {
                    'job_id': job_id,
                    'organization_name': result.get('name', ''),
                    'category': result.get('category', ''),
                    'homepage_url': result.get('homepage_url', ''),
                    'status': result.get('status', 'PROCESSING'),
                    'current_step': result.get('current_step', ''),
                    'processing_time': result.get('processing_time', 0),
                    'extraction_method': result.get('extraction_method', ''),
                    'phone': result.get('phone', ''),
                    'fax': result.get('fax', ''),
                    'email': result.get('email', ''),
                    'mobile': result.get('mobile', ''),
                    'address': result.get('address', ''),
                    'crawling_details': json.dumps({
                        'homepage_parsed': result.get('homepage_parsed'),
                        'ai_summary': result.get('ai_summary'),
                        'meta_info': result.get('meta_info'),
                        'contact_info_extracted': result.get('contact_info_extracted')
                    }),
                    'error_message': result.get('error_message', '')
                }
                
                self.db.add_crawling_result(result_data)
                
                # ì™„ë£Œëœ ê²½ìš° ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                if result.get('status') == 'COMPLETED':
                    progress = self.db.get_crawling_progress(job_id)
                    completed_count = progress.get('processed_count', 0) + 1
                    self.db.update_crawling_job(job_id, {
                        'processed_count': completed_count
                    })
                
                self.logger.debug(f"âœ… ì§„í–‰ ìƒí™© ì €ì¥: {result.get('name')} - {result.get('status')}")
                
            except Exception as e:
                self.logger.error(f"âŒ ì§„í–‰ ìƒí™© ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return progress_callback
    
    async def start_file_crawling(self, config: CrawlingJobConfig) -> Dict[str, Any]:
        """íŒŒì¼ ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘"""
        try:
            # 1. ë°ì´í„° íŒŒì¼ ì°¾ê¸°
            data_file = self.find_data_file()
            if not data_file:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # 2. ì¡°ì§ ë°ì´í„° ë¡œë“œ
            organizations = self.load_organizations_from_file(data_file)
            if not organizations:
                raise ValueError("ì¡°ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # 3. í¬ë¡¤ë§ ì‘ì—… ìƒì„±
            job_id = self.create_crawling_job(config, organizations)
            
            # 4. í¬ë¡¤ë§ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.current_job = CrawlingJobStatus(
                job_id=job_id,
                status="RUNNING",
                total_count=config.test_count if config.test_mode else len(organizations),
                started_at=datetime.now().isoformat(),
                data_file=data_file
            )
            
            # 5. í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            from crawler_main import AIEnhancedModularUnifiedCrawler
            api_key = os.getenv('GEMINI_API_KEY') if config.use_ai else None
            progress_callback = self.create_progress_callback(job_id)
            
            self.extractor_instance = AIEnhancedModularUnifiedCrawler(
                api_key=api_key,
                progress_callback=progress_callback
            )
            
            # 6. ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
            def run_crawling():
                """ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹¤í–‰"""
                try:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    # í¬ë¡¤ë§ ì‹¤í–‰
                    results = loop.run_until_complete(
                        self.extractor_instance.process_json_file_async(
                            json_file_path=data_file,
                            test_mode=config.test_mode,
                            test_count=config.test_count
                        )
                    )
                    
                    # ì„±ê³µ ì™„ë£Œ
                    self.db.update_crawling_job(job_id, {
                        'status': 'COMPLETED',
                        'completed_at': datetime.now().isoformat()
                    })
                    
                    if self.current_job:
                        self.current_job.status = "COMPLETED"
                        self.current_job.completed_at = datetime.now().isoformat()
                    
                    self.logger.info(f"âœ… íŒŒì¼ ê¸°ë°˜ í¬ë¡¤ë§ ì™„ë£Œ: Job ID {job_id}")
                    
                except Exception as e:
                    # ì˜¤ë¥˜ ì²˜ë¦¬
                    self.logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                    
                    self.db.update_crawling_job(job_id, {
                        'status': 'ERROR',
                        'error_message': str(e),
                        'completed_at': datetime.now().isoformat()
                    })
                    
                    if self.current_job:
                        self.current_job.status = "ERROR"
                        self.current_job.error_message = str(e)
                        self.current_job.completed_at = datetime.now().isoformat()
            
            # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
            threading.Thread(target=run_crawling, daemon=True).start()
            
            return {
                "status": "success",
                "message": "íŒŒì¼ ê¸°ë°˜ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "job_id": job_id,
                "total_count": self.current_job.total_count,
                "data_file": data_file,
                "config": config.__dict__
            }
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì¼ ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨: {e}")
            raise
    
    async def start_db_crawling(self, config: CrawlingJobConfig) -> Dict[str, Any]:
        """DB ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘ (crawler_main.py í†µí•©)"""
        try:
            from crawler_main import crawl_ai_enhanced_from_database
            
            # DB í¬ë¡¤ë§ ì‹¤í–‰
            self.logger.info("ğŸš€ DB ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘")
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
            def run_db_crawling():
                try:
                    # ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œìš©)
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    # async í•¨ìˆ˜ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‹¤í–‰
                    results = loop.run_until_complete(crawl_ai_enhanced_from_database())
                    self.logger.info(f"âœ… DB ê¸°ë°˜ í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ì²˜ë¦¬")
                    
                    # ì´ë²¤íŠ¸ ë£¨í”„ ì •ë¦¬
                    loop.close()
                    
                except Exception as e:
                    self.logger.error(f"âŒ DB ê¸°ë°˜ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            
            threading.Thread(target=run_db_crawling, daemon=True).start()
            
            return {
                "status": "success",
                "message": "DB ê¸°ë°˜ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "config": config.__dict__
            }
            
        except Exception as e:
            self.logger.error(f"âŒ DB ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨: {e}")
            raise
    
    def get_crawling_progress(self) -> Dict[str, Any]:
        """í˜„ì¬ í¬ë¡¤ë§ ì§„í–‰ ìƒí™© ì¡°íšŒ"""
        if not self.current_job or not self.current_job.job_id:
            return {
                "status": "idle",
                "message": "ì§„í–‰ ì¤‘ì¸ í¬ë¡¤ë§ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        try:
            progress = self.db.get_crawling_progress(self.current_job.job_id)
            
            # í˜„ì¬ ì‘ì—… ìƒíƒœì™€ DB ìƒíƒœ ë™ê¸°í™”
            if self.current_job:
                progress.update({
                    "current_job_status": self.current_job.status,
                    "data_file": self.current_job.data_file,
                    "started_at": self.current_job.started_at,
                    "completed_at": self.current_job.completed_at,
                    "error_message": self.current_job.error_message
                })
            
            return progress
            
        except Exception as e:
            self.logger.error(f"âŒ ì§„í–‰ ìƒí™© ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"status": "error", "message": str(e)}
    
    def get_crawling_results(self, limit: int = 10, all_results: bool = False) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
        job_id = self.current_job.job_id if self.current_job else None
        
        if not job_id:
            return {"results": [], "total_count": 0}
        
        try:
            if all_results:
                results = self.db.get_crawling_results(job_id, limit=10000)
                progress = self.db.get_crawling_progress(job_id)
                
                completed_count = len([r for r in results if r['status'] == 'COMPLETED'])
                failed_count = len([r for r in results if r['status'] == 'FAILED'])
                
                return {
                    "results": results,
                    "total_count": len(results),
                    "completed_count": completed_count,
                    "failed_count": failed_count,
                    "progress": progress
                }
            else:
                results = self.db.get_crawling_results(job_id, limit)
                progress = self.db.get_crawling_progress(job_id)
                
                return {
                    "results": results,
                    "total_count": progress.get('processed_count', 0),
                    "progress": progress
                }
                
        except Exception as e:
            self.logger.error(f"âŒ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"results": [], "total_count": 0, "error": str(e)}
    
    def get_latest_crawling_results(self) -> Dict[str, Any]:
        """ìµœì‹  í¬ë¡¤ë§ ì‘ì—… ê²°ê³¼ ì¡°íšŒ"""
        try:
            job = self.db.get_latest_crawling_job()
            if not job:
                return {"results": [], "count": 0}
            
            results = self.db.get_crawling_results(job['id'], limit=1000)
            return {
                "results": results,
                "count": len(results),
                "job_info": job
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ìµœì‹  í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"results": [], "count": 0, "error": str(e)}
    
    def stop_crawling(self) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ì¤‘ì§€"""
        if not self.current_job or self.current_job.status != "RUNNING":
            return {"status": "error", "message": "ì¤‘ì§€í•  í¬ë¡¤ë§ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            # í¬ë¡¤ë§ ìƒíƒœ ì—…ë°ì´íŠ¸
            if self.current_job.job_id:
                self.db.update_crawling_job(self.current_job.job_id, {
                    'status': 'STOPPED',
                    'completed_at': datetime.now().isoformat()
                })
            
            self.current_job.status = "STOPPED"
            self.current_job.completed_at = datetime.now().isoformat()
            
            self.logger.info(f"â¹ï¸ í¬ë¡¤ë§ ì¤‘ì§€: Job ID {self.current_job.job_id}")
            
            return {
                "status": "success",
                "message": "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ì§€ ì‹¤íŒ¨: {e}")
            return {"status": "error", "message": str(e)}

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_crawling_service_instance = None

def get_crawling_service() -> CrawlingService:
    """í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _crawling_service_instance
    if _crawling_service_instance is None:
        _crawling_service_instance = CrawlingService()
    return _crawling_service_instance 