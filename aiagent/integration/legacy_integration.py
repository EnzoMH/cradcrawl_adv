#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë ˆê±°ì‹œ ì‹œìŠ¤í…œ í†µí•© ë ˆì´ì–´
ê¸°ì¡´ crawler/ ë° test/ í´ë”ì˜ êµ¬í˜„ì²´ë“¤ì„ AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œê³¼ í†µí•©
"""

import sys
import os
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root))

# ê¸°ì¡´ êµ¬í˜„ì²´ë“¤ import
try:
    from cralwer.url_extractor import HomepageParser
    from cralwer.phone_extractor import search_phone_number, extract_phone_numbers, setup_driver
    from cralwer.fax_extractor import GoogleContactCrawler
    from test.data_processor import DataProcessor
    LEGACY_AVAILABLE = True
    print("âœ… ë ˆê±°ì‹œ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ ë ˆê±°ì‹œ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    LEGACY_AVAILABLE = False

# AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ import
try:
    from aiagent.core.enhanced_agent_system import (
        SearchStrategyAgent, ValidationAgent, ResourceManager, 
        CrawlingResult, DataQualityGrade
    )
    from aiagent.config.gcp_optimization import GCPOptimizer
    AI_AGENT_AVAILABLE = True
    print("âœ… AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    AI_AGENT_AVAILABLE = False

@dataclass
class IntegrationConfig:
    """í†µí•© ì„¤ì •"""
    use_ai_primary: bool = True  # AI ìš°ì„  ì‚¬ìš©
    use_legacy_fallback: bool = True  # ë ˆê±°ì‹œ fallback
    hybrid_validation: bool = True  # í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦
    performance_comparison: bool = True  # ì„±ëŠ¥ ë¹„êµ
    data_quality_threshold: float = 0.7  # ë°ì´í„° í’ˆì§ˆ ì„ê³„ê°’

class LegacyIntegrationManager:
    """ë ˆê±°ì‹œ ì‹œìŠ¤í…œ í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self, config: IntegrationConfig = None):
        self.config = config or IntegrationConfig()
        self.logger = self._setup_logger()
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.ai_agents = {}
        self.legacy_crawlers = {}
        self.data_processor = None
        self.gcp_optimizer = None
        
        self._initialize_systems()
    
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('legacy_integration')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _initialize_systems(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.logger.info("ğŸ”„ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
        
        # AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if AI_AGENT_AVAILABLE:
            try:
                self.ai_agents['search'] = SearchStrategyAgent()
                self.ai_agents['validation'] = ValidationAgent()
                self.ai_agents['resource'] = ResourceManager()
                self.gcp_optimizer = GCPOptimizer()
                self.logger.info("âœ… AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if LEGACY_AVAILABLE:
            try:
                self.legacy_crawlers['homepage'] = HomepageParser(headless=True)
                self.legacy_crawlers['contact'] = GoogleContactCrawler()
                self.data_processor = DataProcessor()
                self.logger.info("âœ… ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def integrated_crawl(self, organization: Dict[str, Any]) -> CrawlingResult:
        """í†µí•© í¬ë¡¤ë§ ìˆ˜í–‰"""
        self.logger.info(f"ğŸ” í†µí•© í¬ë¡¤ë§ ì‹œì‘: {organization.get('name', 'Unknown')}")
        
        # AI ìš°ì„  ì‹œë„
        if self.config.use_ai_primary and AI_AGENT_AVAILABLE:
            try:
                result = self._ai_crawl(organization)
                if self._is_result_satisfactory(result):
                    self.logger.info("âœ… AI í¬ë¡¤ë§ ì„±ê³µ")
                    return result
                else:
                    self.logger.warning("âš ï¸ AI í¬ë¡¤ë§ í’ˆì§ˆ ë¶€ì¡± - ë ˆê±°ì‹œ fallback")
            except Exception as e:
                self.logger.error(f"âŒ AI í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        # ë ˆê±°ì‹œ fallback
        if self.config.use_legacy_fallback and LEGACY_AVAILABLE:
            try:
                result = self._legacy_crawl(organization)
                self.logger.info("âœ… ë ˆê±°ì‹œ í¬ë¡¤ë§ ì™„ë£Œ")
                return result
            except Exception as e:
                self.logger.error(f"âŒ ë ˆê±°ì‹œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        # ìµœì†Œí•œì˜ ê²°ê³¼ ë°˜í™˜
        return CrawlingResult(
            organization_name=organization.get('name', ''),
            address=organization.get('address', ''),
            phone='',
            fax='',
            email='',
            homepage='',
            quality_grade=DataQualityGrade.F,
            confidence_score=0.0,
            processing_time=0.0,
            data_sources=['minimal']
        )
    
    def _ai_crawl(self, organization: Dict[str, Any]) -> CrawlingResult:
        """AI ì—ì´ì „íŠ¸ ê¸°ë°˜ í¬ë¡¤ë§"""
        # ê²€ìƒ‰ ì „ëµ ìƒì„±
        search_agent = self.ai_agents['search']
        search_queries = search_agent.generate_search_queries(
            organization.get('name', ''),
            organization.get('address', ''),
            organization.get('phone', ''),
            organization.get('category', '')
        )
        
        # í¬ë¡¤ë§ ìˆ˜í–‰ (ê°„ë‹¨í•œ êµ¬í˜„)
        result_data = {
            'organization_name': organization.get('name', ''),
            'address': organization.get('address', ''),
            'phone': organization.get('phone', ''),
            'fax': '',
            'email': '',
            'homepage': '',
            'quality_grade': DataQualityGrade.C,
            'confidence_score': 0.8,
            'processing_time': 2.0,
            'data_sources': ['ai_agent']
        }
        
        return CrawlingResult(**result_data)
    
    def _legacy_crawl(self, organization: Dict[str, Any]) -> CrawlingResult:
        """ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ê¸°ë°˜ í¬ë¡¤ë§"""
        org_name = organization.get('name', '')
        
        # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
        phone_numbers = []
        if 'contact' in self.legacy_crawlers:
            try:
                phone_result = self.legacy_crawlers['contact'].search_phone_number(org_name)
                if phone_result:
                    phone_numbers = phone_result.get('phone_numbers', [])
            except Exception as e:
                self.logger.error(f"ì „í™”ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ
        fax_numbers = []
        if 'contact' in self.legacy_crawlers:
            try:
                fax_result = self.legacy_crawlers['contact'].search_fax_number(org_name)
                if fax_result:
                    fax_numbers = fax_result.get('fax_numbers', [])
            except Exception as e:
                self.logger.error(f"íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # í™ˆí˜ì´ì§€ ì¶”ì¶œ
        homepage_info = {}
        if 'homepage' in self.legacy_crawlers:
            try:
                homepage_result = self.legacy_crawlers['homepage'].extract_page_content(
                    f"https://www.google.com/search?q={org_name}"
                )
                if homepage_result:
                    homepage_info = homepage_result
            except Exception as e:
                self.logger.error(f"í™ˆí˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # ë°ì´í„° í’ˆì§ˆ í‰ê°€
        quality_grade = self._evaluate_legacy_quality(
            phone_numbers, fax_numbers, homepage_info
        )
        
        result_data = {
            'organization_name': org_name,
            'address': organization.get('address', ''),
            'phone': phone_numbers[0] if phone_numbers else '',
            'fax': fax_numbers[0] if fax_numbers else '',
            'email': homepage_info.get('email', ''),
            'homepage': homepage_info.get('url', ''),
            'quality_grade': quality_grade,
            'confidence_score': 0.6,
            'processing_time': 5.0,
            'data_sources': ['legacy_crawler']
        }
        
        return CrawlingResult(**result_data)
    
    def _is_result_satisfactory(self, result: CrawlingResult) -> bool:
        """ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ¬ìš´ì§€ í™•ì¸"""
        if not result:
            return False
        
        # í’ˆì§ˆ ë“±ê¸‰ í™•ì¸
        quality_threshold = {
            DataQualityGrade.A: 1.0,
            DataQualityGrade.B: 0.8,
            DataQualityGrade.C: 0.6,
            DataQualityGrade.D: 0.4,
            DataQualityGrade.E: 0.2,
            DataQualityGrade.F: 0.0
        }
        
        return quality_threshold.get(result.quality_grade, 0.0) >= self.config.data_quality_threshold
    
    def _evaluate_legacy_quality(self, phone_numbers: List[str], 
                                fax_numbers: List[str], 
                                homepage_info: Dict) -> DataQualityGrade:
        """ë ˆê±°ì‹œ ê²°ê³¼ì˜ í’ˆì§ˆ í‰ê°€"""
        score = 0
        
        # ì „í™”ë²ˆí˜¸ (30ì )
        if phone_numbers:
            score += 30
        
        # íŒ©ìŠ¤ë²ˆí˜¸ (20ì )
        if fax_numbers:
            score += 20
        
        # í™ˆí˜ì´ì§€ (25ì )
        if homepage_info.get('url'):
            score += 25
        
        # ì´ë©”ì¼ (15ì )
        if homepage_info.get('email'):
            score += 15
        
        # ê¸°íƒ€ ì •ë³´ (10ì )
        if homepage_info.get('content'):
            score += 10
        
        # ë“±ê¸‰ ê²°ì •
        if score >= 90:
            return DataQualityGrade.A
        elif score >= 70:
            return DataQualityGrade.B
        elif score >= 50:
            return DataQualityGrade.C
        elif score >= 30:
            return DataQualityGrade.D
        elif score >= 10:
            return DataQualityGrade.E
        else:
            return DataQualityGrade.F
    
    def hybrid_validation(self, ai_result: CrawlingResult, 
                         legacy_result: CrawlingResult) -> CrawlingResult:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ìˆ˜í–‰"""
        if not self.config.hybrid_validation:
            return ai_result
        
        self.logger.info("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œì‘")
        
        # ìµœê³  í’ˆì§ˆ ê²°ê³¼ ì„ íƒ
        if ai_result.quality_grade.value < legacy_result.quality_grade.value:
            best_result = ai_result
            backup_result = legacy_result
        else:
            best_result = legacy_result
            backup_result = ai_result
        
        # í•„ë“œë³„ ë³´ì™„
        validated_data = {
            'organization_name': best_result.organization_name,
            'address': best_result.address,
            'phone': best_result.phone or backup_result.phone,
            'fax': best_result.fax or backup_result.fax,
            'email': best_result.email or backup_result.email,
            'homepage': best_result.homepage or backup_result.homepage,
            'quality_grade': best_result.quality_grade,
            'confidence_score': max(best_result.confidence_score, backup_result.confidence_score),
            'processing_time': best_result.processing_time + backup_result.processing_time,
            'data_sources': best_result.data_sources + backup_result.data_sources
        }
        
        return CrawlingResult(**validated_data)
    
    def process_batch(self, organizations: List[Dict[str, Any]], 
                     output_file: str = None) -> List[CrawlingResult]:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        self.logger.info(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(organizations)}ê°œ ì¡°ì§")
        
        results = []
        
        for i, org in enumerate(organizations, 1):
            self.logger.info(f"ì²˜ë¦¬ ì¤‘ ({i}/{len(organizations)}): {org.get('name', 'Unknown')}")
            
            try:
                result = self.integrated_crawl(org)
                results.append(result)
                
                # ì§„í–‰ë¥  ì¶œë ¥
                if i % 10 == 0:
                    self.logger.info(f"ì§„í–‰ë¥ : {i}/{len(organizations)} ({i/len(organizations)*100:.1f}%)")
                    
            except Exception as e:
                self.logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {org.get('name', 'Unknown')} - {e}")
                continue
        
        # ê²°ê³¼ ì €ì¥
        if output_file and self.data_processor:
            try:
                self._save_results(results, output_file)
                self.logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            except Exception as e:
                self.logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        self.logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results
    
    def _save_results(self, results: List[CrawlingResult], output_file: str):
        """ê²°ê³¼ ì €ì¥"""
        # CrawlingResultë¥¼ dictë¡œ ë³€í™˜
        results_dict = []
        for result in results:
            result_dict = {
                'organization_name': result.organization_name,
                'address': result.address,
                'phone': result.phone,
                'fax': result.fax,
                'email': result.email,
                'homepage': result.homepage,
                'quality_grade': result.quality_grade.name,
                'confidence_score': result.confidence_score,
                'processing_time': result.processing_time,
                'data_sources': result.data_sources
            }
            results_dict.append(result_dict)
        
        # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ì €ì¥
        if output_file.endswith('.json'):
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results_dict, f, ensure_ascii=False, indent=2)
        elif output_file.endswith(('.xlsx', '.xls')):
            if self.data_processor:
                # ì„ì‹œ JSON íŒŒì¼ë¡œ ì €ì¥ í›„ Excelë¡œ ë³€í™˜
                temp_json = output_file.replace('.xlsx', '_temp.json').replace('.xls', '_temp.json')
                with open(temp_json, 'w', encoding='utf-8') as f:
                    json.dump(results_dict, f, ensure_ascii=False, indent=2)
                
                self.data_processor.json_to_excel(temp_json, output_file)
                os.remove(temp_json)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    
    def performance_comparison(self, organization: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¹„êµ ë¶„ì„"""
        if not self.config.performance_comparison:
            return {}
        
        self.logger.info("ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì‹œì‘")
        
        comparison_results = {
            'organization': organization.get('name', ''),
            'ai_result': None,
            'legacy_result': None,
            'comparison': {}
        }
        
        # AI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        if AI_AGENT_AVAILABLE:
            try:
                import time
                start_time = time.time()
                ai_result = self._ai_crawl(organization)
                ai_time = time.time() - start_time
                
                comparison_results['ai_result'] = {
                    'quality_grade': ai_result.quality_grade.name,
                    'confidence_score': ai_result.confidence_score,
                    'processing_time': ai_time,
                    'data_completeness': self._calculate_completeness(ai_result)
                }
            except Exception as e:
                self.logger.error(f"AI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ë ˆê±°ì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        if LEGACY_AVAILABLE:
            try:
                import time
                start_time = time.time()
                legacy_result = self._legacy_crawl(organization)
                legacy_time = time.time() - start_time
                
                comparison_results['legacy_result'] = {
                    'quality_grade': legacy_result.quality_grade.name,
                    'confidence_score': legacy_result.confidence_score,
                    'processing_time': legacy_time,
                    'data_completeness': self._calculate_completeness(legacy_result)
                }
            except Exception as e:
                self.logger.error(f"ë ˆê±°ì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ë¹„êµ ë¶„ì„
        if comparison_results['ai_result'] and comparison_results['legacy_result']:
            ai_res = comparison_results['ai_result']
            legacy_res = comparison_results['legacy_result']
            
            comparison_results['comparison'] = {
                'speed_winner': 'AI' if ai_res['processing_time'] < legacy_res['processing_time'] else 'Legacy',
                'quality_winner': 'AI' if ai_res['quality_grade'] < legacy_res['quality_grade'] else 'Legacy',
                'completeness_winner': 'AI' if ai_res['data_completeness'] > legacy_res['data_completeness'] else 'Legacy',
                'confidence_winner': 'AI' if ai_res['confidence_score'] > legacy_res['confidence_score'] else 'Legacy'
            }
        
        return comparison_results
    
    def _calculate_completeness(self, result: CrawlingResult) -> float:
        """ë°ì´í„° ì™„ì „ì„± ê³„ì‚°"""
        fields = ['phone', 'fax', 'email', 'homepage']
        filled_fields = sum(1 for field in fields if getattr(result, field, ''))
        return filled_fields / len(fields)
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘")
        
        # ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ ì •ë¦¬
        for name, crawler in self.legacy_crawlers.items():
            try:
                if hasattr(crawler, 'close'):
                    crawler.close()
                elif hasattr(crawler, 'close_driver'):
                    crawler.close_driver()
            except Exception as e:
                self.logger.error(f"í¬ë¡¤ëŸ¬ ì •ë¦¬ ì‹¤íŒ¨ ({name}): {e}")
        
        self.logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ì‚¬ìš© ì˜ˆì œ
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”„ ë ˆê±°ì‹œ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    # ì„¤ì •
    config = IntegrationConfig(
        use_ai_primary=True,
        use_legacy_fallback=True,
        hybrid_validation=True,
        performance_comparison=True
    )
    
    # í†µí•© ê´€ë¦¬ì ì´ˆê¸°í™”
    manager = LegacyIntegrationManager(config)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_organizations = [
        {
            'name': 'ì„œìš¸ì‹œë¦½ì–´ë¦°ì´ì§‘',
            'address': 'ì„œìš¸ì‹œ ê°•ë‚¨êµ¬',
            'category': 'ì–´ë¦°ì´ì§‘'
        },
        {
            'name': 'ë¶€ì‚°ì•„ë™ì„¼í„°',
            'address': 'ë¶€ì‚°ì‹œ í•´ìš´ëŒ€êµ¬',
            'category': 'ì•„ë™ì„¼í„°'
        }
    ]
    
    try:
        # ë°°ì¹˜ ì²˜ë¦¬
        results = manager.process_batch(
            test_organizations,
            output_file='integration_test_results.json'
        )
        
        # ì„±ëŠ¥ ë¹„êµ
        if results:
            comparison = manager.performance_comparison(test_organizations[0])
            print(f"ì„±ëŠ¥ ë¹„êµ ê²°ê³¼: {comparison}")
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        manager.cleanup()

if __name__ == "__main__":
    main() 