#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CenterCrawlingBotê³¼ AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í†µí•© ë ˆì´ì–´
ê¸°ì¡´ ì‹œìŠ¤í…œì˜ ì•ˆì •ì„±ê³¼ ìƒˆë¡œìš´ AI ê¸°ëŠ¥ì„ ê²°í•©

ì£¼ìš” ê¸°ëŠ¥:
1. ê¸°ì¡´ CenterCrawlingBot í´ë˜ìŠ¤ í™•ì¥
2. AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í†µí•©
3. í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹ + AI ê²€ì¦)
4. ì ì§„ì  ì—…ê·¸ë ˆì´ë“œ ì§€ì›
"""

import os
import sys
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import json

# ê¸°ì¡´ ì‹œìŠ¤í…œ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from centercrawling import CenterCrawlingBot
from database.database import ChurchCRMDatabase
from database.models import Organization, CrawlingJob

# AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ import
from aiagent.core.enhanced_agent_system import (
    EnhancedAIAgentSystem, 
    CrawlingResult, 
    SearchStrategyAgent, 
    ValidationAgent,
    ResourceManager,
    DataQualityGrade
)


class IntegratedCrawlingBot(CenterCrawlingBot):
    """í†µí•© í¬ë¡¤ë§ ë´‡ - ê¸°ì¡´ ì‹œìŠ¤í…œ + AI ì—ì´ì „íŠ¸"""
    
    def __init__(self, excel_path: str = None, use_ai: bool = True, send_email: bool = True, 
                 use_database: bool = True, integration_mode: str = "hybrid"):
        """
        ì´ˆê¸°í™”
        
        Args:
            excel_path: ì—‘ì…€ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
            use_ai: AI ê¸°ëŠ¥ ì‚¬ìš© ì—¬ë¶€
            send_email: ì´ë©”ì¼ ì „ì†¡ ì—¬ë¶€
            use_database: ë°ì´í„°ë² ì´ìŠ¤ ì§ì ‘ ì‚¬ìš© ì—¬ë¶€
            integration_mode: í†µí•© ëª¨ë“œ ('hybrid', 'ai_only', 'legacy_only')
        """
        self.use_database = use_database
        self.integration_mode = integration_mode
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        if use_database:
            self.db = ChurchCRMDatabase()
            self.logger = logging.getLogger("IntegratedCrawlingBot")
        
        # AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if use_ai and integration_mode in ['hybrid', 'ai_only']:
            self.ai_agent_system = EnhancedAIAgentSystem()
        else:
            self.ai_agent_system = None
        
        # ê¸°ì¡´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (excel_pathê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        if excel_path and integration_mode in ['hybrid', 'legacy_only']:
            super().__init__(excel_path, use_ai, send_email)
        else:
            # ê¸°ë³¸ ì´ˆê¸°í™”
            self.use_ai = use_ai
            self.send_email = send_email
            self.logger = logging.getLogger("IntegratedCrawlingBot")
            
            # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
            from dotenv import load_dotenv
            load_dotenv()
            
            # AI ëª¨ë¸ ì´ˆê¸°í™”
            if use_ai:
                self._initialize_ai()
            
            # WebDriver ì´ˆê¸°í™”
            self.driver = None
            self._initialize_webdriver()
            
            # ê²°ê³¼ ì €ì¥ìš©
            self.results = []
            self.processed_count = 0
            self.success_count = 0
            self.start_time = datetime.now()
        
        self.logger.info(f"ğŸš€ í†µí•© í¬ë¡¤ë§ ë´‡ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë“œ: {integration_mode})")
    
    def run_database_crawling(self, batch_size: int = 50, max_workers: int = None) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            if not self.use_database:
                raise ValueError("ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“œê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            self.logger.info("ğŸ¯ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘")
            
            # í¬ë¡¤ë§ ì‘ì—… ì‹œì‘
            job_id = None
            if self.ai_agent_system:
                job_id = self.ai_agent_system.start_crawling_job(
                    "í†µí•© í¬ë¡¤ë§ ì‘ì—…", "system"
                )
            
            # í¬ë¡¤ë§ ëŒ€ìƒ ì¡°íšŒ
            organizations = self._get_organizations_to_crawl()
            total_count = len(organizations)
            
            if total_count == 0:
                self.logger.info("ğŸ“‹ í¬ë¡¤ë§í•  ê¸°ê´€ì´ ì—†ìŠµë‹ˆë‹¤.")
                return self._get_crawling_summary()
            
            self.logger.info(f"ğŸ“Š ì´ {total_count}ê°œ ê¸°ê´€ í¬ë¡¤ë§ ì‹œì‘")
            
            # ë°°ì¹˜ ì²˜ë¦¬
            processed_count = 0
            success_count = 0
            
            for i in range(0, total_count, batch_size):
                batch = organizations[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                
                self.logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹œì‘ ({len(batch)}ê°œ)")
                
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_results = self._process_organization_batch(batch)
                
                # ê²°ê³¼ ì§‘ê³„
                processed_count += len(batch)
                success_count += len([r for r in batch_results if r])
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress = (processed_count / total_count) * 100
                self.logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {progress:.1f}% ({processed_count}/{total_count})")
                
                # ì¤‘ê°„ ì €ì¥ ë° í†µê³„ ì—…ë°ì´íŠ¸
                if job_id:
                    self._update_crawling_job_progress(job_id, processed_count, total_count)
                
                # ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
                if self.ai_agent_system:
                    stats = self.ai_agent_system.resource_manager.get_system_stats()
                    if stats['memory_percent'] > 80:
                        self.logger.warning("âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ, ì ì‹œ ëŒ€ê¸°...")
                        time.sleep(10)
            
            # í¬ë¡¤ë§ ì™„ë£Œ
            end_time = datetime.now()
            duration = end_time - self.start_time
            
            # ìµœì¢… í†µê³„
            final_stats = self._get_crawling_summary()
            final_stats.update({
                'total_processed': processed_count,
                'success_count': success_count,
                'duration': str(duration),
                'job_id': job_id
            })
            
            self.logger.info(f"ğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: {duration}")
            
            # ì´ë©”ì¼ ì „ì†¡
            if self.send_email:
                self._send_database_completion_email(final_stats)
            
            return final_stats
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            if self.send_email:
                self._send_error_email(str(e))
            raise
    
    def _get_organizations_to_crawl(self) -> List[Dict[str, Any]]:
        """í¬ë¡¤ë§í•  ê¸°ê´€ ëª©ë¡ ì¡°íšŒ"""
        try:
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¿¼ë¦¬
            query = """
                SELECT id, name, address, phone, fax, homepage, email, mobile,
                       contact_status, priority, last_crawled_at, ai_crawled
                FROM organizations 
                WHERE is_active = true
                AND (
                    ai_crawled = false 
                    OR last_crawled_at IS NULL 
                    OR last_crawled_at < NOW() - INTERVAL '30 days'
                    OR (phone IS NULL OR phone = '')
                    OR (fax IS NULL OR fax = '')
                    OR (homepage IS NULL OR homepage = '')
                    OR (email IS NULL OR email = '')
                )
                ORDER BY 
                    CASE priority 
                        WHEN 'HIGH' THEN 1 
                        WHEN 'MEDIUM' THEN 2 
                        WHEN 'LOW' THEN 3 
                        ELSE 4 
                    END,
                    last_crawled_at ASC NULLS FIRST,
                    id ASC
            """
            
            results = self.db.execute_query(query)
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ë¡¤ë§ ëŒ€ìƒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def _process_organization_batch(self, batch: List[Dict[str, Any]]) -> List[Optional[CrawlingResult]]:
        """ê¸°ê´€ ë°°ì¹˜ ì²˜ë¦¬ - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ"""
        results = []
        
        for org_data in batch:
            try:
                result = self._process_single_organization_hybrid(org_data)
                results.append(result)
                
                if result:
                    self.success_count += 1
                    self.logger.info(f"âœ… ê¸°ê´€ ì²˜ë¦¬ ì„±ê³µ: {org_data['name']} (ë“±ê¸‰: {result.data_quality_grade})")
                else:
                    self.logger.warning(f"âš ï¸ ê¸°ê´€ ì²˜ë¦¬ ì‹¤íŒ¨: {org_data['name']}")
                
                self.processed_count += 1
                
            except Exception as e:
                self.logger.error(f"âŒ ê¸°ê´€ ì²˜ë¦¬ ì˜¤ë¥˜: {org_data['name']} - {e}")
                results.append(None)
                continue
        
        return results
    
    def _process_single_organization_hybrid(self, org_data: Dict[str, Any]) -> Optional[CrawlingResult]:
        """ë‹¨ì¼ ê¸°ê´€ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬"""
        try:
            name = org_data['name']
            address = org_data['address']
            
            if not name or not address:
                return None
            
            # 1ë‹¨ê³„: AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì‚¬ìš© (ìš°ì„ )
            if self.ai_agent_system and self.integration_mode in ['hybrid', 'ai_only']:
                ai_result = self.ai_agent_system.process_single_organization(org_data)
                
                if ai_result and ai_result.validation_score > 0.7:
                    self.logger.info(f"ğŸ¤– AI ì—ì´ì „íŠ¸ ì„±ê³µ: {name} (ì ìˆ˜: {ai_result.validation_score:.2f})")
                    return ai_result
            
            # 2ë‹¨ê³„: ê¸°ì¡´ ì‹œìŠ¤í…œ ì‚¬ìš© (fallback)
            if self.integration_mode in ['hybrid', 'legacy_only']:
                legacy_result = self._process_with_legacy_system(org_data)
                
                if legacy_result:
                    self.logger.info(f"ğŸ”§ ê¸°ì¡´ ì‹œìŠ¤í…œ ì„±ê³µ: {name}")
                    return legacy_result
            
            # 3ë‹¨ê³„: ìµœì†Œí•œì˜ ê²°ê³¼ë¼ë„ ì €ì¥
            minimal_result = CrawlingResult(
                organization_id=org_data['id'],
                name=name,
                address=address,
                phone=org_data.get('phone', ''),
                fax=org_data.get('fax', ''),
                homepage=org_data.get('homepage', ''),
                email=org_data.get('email', ''),
                data_quality_grade=DataQualityGrade.F.value,
                crawling_source="minimal_update"
            )
            
            return minimal_result
            
        except Exception as e:
            self.logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {org_data['name']} - {e}")
            return None
    
    def _process_with_legacy_system(self, org_data: Dict[str, Any]) -> Optional[CrawlingResult]:
        """ê¸°ì¡´ ì‹œìŠ¤í…œìœ¼ë¡œ ì²˜ë¦¬"""
        try:
            name = org_data['name']
            address = org_data['address']
            existing_phone = org_data.get('phone', '')
            
            # ê¸°ì¡´ ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ ë¡œì§ í™œìš©
            found_data = {}
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ê²€ìƒ‰
            if not org_data.get('fax'):
                fax_query = f"{name} íŒ©ìŠ¤ë²ˆí˜¸"
                fax_result = self._search_with_multiple_engines(fax_query, name, 'fax')
                if fax_result:
                    found_data['fax'] = fax_result
            
            # í™ˆí˜ì´ì§€ ê²€ìƒ‰
            if not org_data.get('homepage'):
                homepage_query = f"{name} í™ˆí˜ì´ì§€"
                homepage_result = self._search_for_homepage(homepage_query, name)
                if homepage_result:
                    found_data['homepage'] = homepage_result
            
            # ê²°ê³¼ê°€ ìˆìœ¼ë©´ CrawlingResult ìƒì„±
            if found_data:
                result = CrawlingResult(
                    organization_id=org_data['id'],
                    name=name,
                    address=address,
                    phone=existing_phone,
                    fax=found_data.get('fax', ''),
                    homepage=found_data.get('homepage', ''),
                    crawling_source="legacy_system"
                )
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                self._save_to_database(result)
                
                return result
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ì¡´ ì‹œìŠ¤í…œ ì²˜ë¦¬ ì˜¤ë¥˜: {org_data['name']} - {e}")
            return None
    
    def _save_to_database(self, result: CrawlingResult):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ê²°ê³¼ ì €ì¥"""
        try:
            update_query = """
                UPDATE organizations 
                SET phone = COALESCE(NULLIF(%(phone)s, ''), phone),
                    fax = COALESCE(NULLIF(%(fax)s, ''), fax),
                    homepage = COALESCE(NULLIF(%(homepage)s, ''), homepage),
                    email = COALESCE(NULLIF(%(email)s, ''), email),
                    mobile = COALESCE(NULLIF(%(mobile)s, ''), mobile),
                    ai_crawled = true,
                    last_crawled_at = %(crawled_at)s,
                    crawling_data = %(crawling_data)s,
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = %(organization_id)s
            """
            
            crawling_data = {
                'data_quality_grade': result.data_quality_grade,
                'validation_score': result.validation_score,
                'crawling_source': result.crawling_source,
                'ai_confidence': result.ai_confidence,
                'processed_at': result.crawled_at.isoformat()
            }
            
            params = {
                'phone': result.phone,
                'fax': result.fax,
                'homepage': result.homepage,
                'email': result.email,
                'mobile': result.mobile,
                'crawled_at': result.crawled_at,
                'crawling_data': json.dumps(crawling_data),
                'organization_id': result.organization_id
            }
            
            self.db.execute_update(update_query, params)
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {result.name} - {e}")
    
    def _update_crawling_job_progress(self, job_id: int, processed: int, total: int):
        """í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        try:
            query = """
                UPDATE crawling_jobs 
                SET processed_count = %s, total_count = %s, updated_at = CURRENT_TIMESTAMP
                WHERE id = %s
            """
            self.db.execute_update(query, (processed, total, job_id))
            
        except Exception as e:
            self.logger.error(f"âŒ ì‘ì—… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _get_crawling_summary(self) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ìš”ì•½ í†µê³„"""
        try:
            # ê¸°ë³¸ í†µê³„
            summary = {
                'total_organizations': 0,
                'ai_crawled_count': 0,
                'contact_info_counts': {},
                'quality_distribution': {},
                'recent_crawling_stats': {}
            }
            
            if self.ai_agent_system:
                summary = self.ai_agent_system.get_crawling_statistics()
            
            # ì¶”ê°€ í†µê³„
            recent_stats_query = """
                SELECT 
                    COUNT(*) as recently_crawled,
                    AVG(CAST(crawling_data->>'validation_score' AS FLOAT)) as avg_validation_score
                FROM organizations 
                WHERE last_crawled_at >= NOW() - INTERVAL '1 day'
                AND crawling_data IS NOT NULL
            """
            
            recent_stats = self.db.execute_query(recent_stats_query, fetch_all=False)
            if recent_stats:
                summary['recent_crawling_stats'] = recent_stats
            
            return summary
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ë¡¤ë§ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def _send_database_completion_email(self, stats: Dict[str, Any]):
        """ë°ì´í„°ë² ì´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ ì´ë©”ì¼"""
        try:
            subject = "ğŸ‰ í†µí•© AI í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì™„ë£Œ"
            
            body = f"""
ì•ˆë…•í•˜ì„¸ìš”! ëŒ€í‘œë‹˜!

í†µí•© AI í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

ğŸ“Š **í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:**
- ì´ ì²˜ë¦¬ ê¸°ê´€: {stats.get('total_processed', 0):,}ê°œ
- ì„±ê³µ ì²˜ë¦¬: {stats.get('success_count', 0):,}ê°œ
- ì‹¤í–‰ ì‹œê°„: {stats.get('duration', 'N/A')}

ğŸ“ˆ **ì—°ë½ì²˜ ì •ë³´ í˜„í™©:**
- ì „í™”ë²ˆí˜¸: {stats.get('contact_info', {}).get('phone', 0):,}ê°œ
- íŒ©ìŠ¤ë²ˆí˜¸: {stats.get('contact_info', {}).get('fax', 0):,}ê°œ
- í™ˆí˜ì´ì§€: {stats.get('contact_info', {}).get('homepage', 0):,}ê°œ
- ì´ë©”ì¼: {stats.get('contact_info', {}).get('email', 0):,}ê°œ

ğŸ† **ë°ì´í„° í’ˆì§ˆ ë“±ê¸‰:**
{self._format_quality_grades(stats.get('quality_grades', {}))}

ğŸ¤– **AI ì‹œìŠ¤í…œ ì„±ëŠ¥:**
- AI í¬ë¡¤ë§ ì™„ë£Œ: {stats.get('ai_crawled_count', 0):,}ê°œ
- í‰ê·  ê²€ì¦ ì ìˆ˜: {stats.get('recent_crawling_stats', {}).get('avg_validation_score', 0):.2f}

ğŸ”§ **ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:**
- ìµœëŒ€ ì›Œì»¤ ìˆ˜: {stats.get('system_stats', {}).get('max_workers', 'N/A')}
- ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {stats.get('system_stats', {}).get('memory_percent', 0):.1f}%

í†µí•© AI ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì‘ë™í–ˆìŠµë‹ˆë‹¤!

ê°ì‚¬í•©ë‹ˆë‹¤!
-í†µí•© AI í¬ë¡¤ë§ ì‹œìŠ¤í…œ-
"""
            
            self._send_email(subject, body)
            
        except Exception as e:
            self.logger.error(f"âŒ ì™„ë£Œ ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    def _format_quality_grades(self, grades: Dict[str, int]) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ í¬ë§·íŒ…"""
        if not grades:
            return "- ë°ì´í„° ì—†ìŒ"
        
        formatted = []
        for grade in ['A', 'B', 'C', 'D', 'E', 'F']:
            count = grades.get(grade, 0)
            if count > 0:
                formatted.append(f"- {grade}ë“±ê¸‰: {count:,}ê°œ")
        
        return '\n'.join(formatted) if formatted else "- ë°ì´í„° ì—†ìŒ"
    
    def run_hybrid_crawling(self, excel_path: str = None) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ ì‹¤í–‰ (ì—‘ì…€ + ë°ì´í„°ë² ì´ìŠ¤)"""
        try:
            results = {}
            
            # 1. ì—‘ì…€ ê¸°ë°˜ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹)
            if excel_path and os.path.exists(excel_path):
                self.logger.info("ğŸ“Š ì—‘ì…€ ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘")
                self.excel_path = excel_path
                self._load_data()
                
                # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í¬ë¡¤ë§
                super().run_extraction()
                
                # ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                self._import_excel_results_to_database()
                
                results['excel_crawling'] = {
                    'processed_count': self.processed_count,
                    'success_count': self.success_count
                }
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ AI í¬ë¡¤ë§
            if self.use_database:
                self.logger.info("ğŸ¤– ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ AI í¬ë¡¤ë§ ì‹œì‘")
                db_results = self.run_database_crawling()
                results['database_crawling'] = db_results
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise
    
    def _import_excel_results_to_database(self):
        """ì—‘ì…€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not hasattr(self, 'df') or self.df is None:
                return
            
            self.logger.info("ğŸ“¥ ì—‘ì…€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ê°€ì ¸ì˜¤ê¸° ì‹œì‘")
            
            for _, row in self.df.iterrows():
                try:
                    # ê¸°ì¡´ ê¸°ê´€ í™•ì¸
                    existing_query = """
                        SELECT id FROM organizations 
                        WHERE name = %s AND address = %s
                    """
                    existing = self.db.execute_query(
                        existing_query, (row['name'], row['address']), fetch_all=False
                    )
                    
                    if existing:
                        # ì—…ë°ì´íŠ¸
                        update_query = """
                            UPDATE organizations 
                            SET phone = COALESCE(NULLIF(%s, ''), phone),
                                fax = COALESCE(NULLIF(%s, ''), fax),
                                homepage = COALESCE(NULLIF(%s, ''), homepage),
                                updated_at = CURRENT_TIMESTAMP
                            WHERE id = %s
                        """
                        self.db.execute_update(update_query, (
                            row.get('phone', ''), 
                            row.get('fax', ''), 
                            row.get('homepage', ''),
                            existing['id']
                        ))
                    else:
                        # ìƒˆë¡œ ì‚½ì…
                        insert_query = """
                            INSERT INTO organizations (name, address, phone, fax, homepage, created_by)
                            VALUES (%s, %s, %s, %s, %s, %s)
                        """
                        self.db.execute_update(insert_query, (
                            row['name'],
                            row['address'],
                            row.get('phone', ''),
                            row.get('fax', ''),
                            row.get('homepage', ''),
                            'excel_import'
                        ))
                
                except Exception as e:
                    self.logger.error(f"âŒ í–‰ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {row.get('name', 'Unknown')} - {e}")
                    continue
            
            self.logger.info("âœ… ì—‘ì…€ ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì—‘ì…€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        try:
            status = {
                'integration_mode': self.integration_mode,
                'use_database': self.use_database,
                'ai_agent_available': self.ai_agent_system is not None,
                'webdriver_available': self.driver is not None,
                'database_connection': False,
                'crawling_active': getattr(self, 'is_running', False)
            }
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
            if self.use_database:
                try:
                    self.db.execute_query("SELECT 1", fetch_all=False)
                    status['database_connection'] = True
                except:
                    status['database_connection'] = False
            
            # AI ì—ì´ì „íŠ¸ ìƒíƒœ
            if self.ai_agent_system:
                status['ai_system_stats'] = self.ai_agent_system.resource_manager.get_system_stats()
            
            # í¬ë¡¤ë§ í†µê³„
            if self.use_database:
                status['crawling_stats'] = self._get_crawling_summary()
            
            return status
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì •ë¦¬
            if self.ai_agent_system:
                self.ai_agent_system.stop_crawling()
            
            # ê¸°ì¡´ ì‹œìŠ¤í…œ ì •ë¦¬
            if hasattr(self, 'driver') and self.driver:
                self.driver.quit()
            
            self.logger.info("ğŸ§¹ í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ì •ë¦¬ ì˜¤ë¥˜: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass


# ==================== ì‚¬ìš© ì˜ˆì œ ====================

def main():
    """ë©”ì¸ í•¨ìˆ˜ - í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    try:
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # í†µí•© í¬ë¡¤ë§ ë´‡ ì´ˆê¸°í™”
        bot = IntegratedCrawlingBot(
            use_ai=True,
            send_email=True,
            use_database=True,
            integration_mode="hybrid"
        )
        
        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        status = bot.get_system_status()
        print(f"ğŸ” ì‹œìŠ¤í…œ ìƒíƒœ: {json.dumps(status, ensure_ascii=False, indent=2)}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ í¬ë¡¤ë§ ì‹¤í–‰
        results = bot.run_database_crawling(batch_size=10)
        print(f"ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼: {json.dumps(results, ensure_ascii=False, indent=2)}")
        
        # ì‹œìŠ¤í…œ ì •ë¦¬
        bot.cleanup()
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 