#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Agentic Workflow í†µí•© CRM ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ v4.0
JSON (êµíšŒ) + Excel (í•™ì›) + Excel (êµíšŒ) ë°ì´í„° AI ê¸°ë°˜ ì§€ëŠ¥ì  í†µí•©
ì£¼ì†Œ ê¸°ë°˜ ì¤‘ë³µ ì œê±° + AI Agents í˜‘ë ¥ ë¶„ì„
"""

import json
import os
import sys
import sqlite3
import hashlib
import secrets
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from contextlib import contextmanager
from pathlib import Path

import logging
import sys

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ai_workflow.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

# AI ì „ìš© ë¡œê±°
ai_logger = logging.getLogger('AI_WORKFLOW')

# Gemini AI ì„¤ì • (ìˆ˜ì •ëœ ë¶€ë¶„)
try:
    import google.generativeai as genai
    
    # 1. settings.pyì—ì„œ ë¨¼ì € ì‹œë„
    try:
        from settings import GEMINI_API_KEY
        ai_logger.info("ğŸ”‘ settings.pyì—ì„œ API í‚¤ ë¡œë“œ")
    except ImportError:
        # 2. .env íŒŒì¼ì—ì„œ ì§ì ‘ ì½ê¸°
        GEMINI_API_KEY = None
        env_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env')
        
        if os.path.exists(env_path):
            try:
                with open(env_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line.startswith('GEMINI_API_KEY'):
                            # GEMINI_API_KEY = "AI....DY" í˜•íƒœ íŒŒì‹±
                            if '=' in line:
                                key, value = line.split('=', 1)
                                value = value.strip()
                                # ë”°ì˜´í‘œ ì œê±°
                                if value.startswith('"') and value.endswith('"'):
                                    value = value[1:-1]
                                elif value.startswith("'") and value.endswith("'"):
                                    value = value[1:-1]
                                GEMINI_API_KEY = value
                                ai_logger.info(f"ğŸ”‘ .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ: {env_path}")
                                break
            except Exception as e:
                ai_logger.error(f"âŒ .env íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        # 3. í™˜ê²½ë³€ìˆ˜ì—ì„œ ìµœì¢… ì‹œë„
        if not GEMINI_API_KEY:
            GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
            if GEMINI_API_KEY:
                ai_logger.info("ğŸ”‘ í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ")
    
    if GEMINI_API_KEY:
        genai.configure(api_key=GEMINI_API_KEY)
        print(f"ğŸ”‘ Gemini API í‚¤ ë¡œë“œ ì„±ê³µ: {GEMINI_API_KEY[:10]}...{GEMINI_API_KEY[-4:]}")
        ai_logger.info(f"âœ… Gemini API ì„¤ì • ì™„ë£Œ")
        AI_AVAILABLE = True
    else:
        print("âŒ GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ“‹ í™•ì¸ ê²½ë¡œ:")
        print(f"  1. settings.py")
        print(f"  2. .env íŒŒì¼: {env_path}")
        print(f"  3. í™˜ê²½ë³€ìˆ˜: GEMINI_API_KEY")
        AI_AVAILABLE = False
        
except ImportError as e:
    print(f"âš ï¸ Gemini AI ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    ai_logger.error(f"Gemini AI ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    AI_AVAILABLE = False

# ==================== ì§ì ‘ Enum ì •ì˜ ====================

class UserRole:
    ADMIN1 = "ëŒ€í‘œ"
    ADMIN2 = "íŒ€ì¥"  
    SALES = "ì˜ì—…"
    DEVELOPER = "ê°œë°œì"

class OrganizationType:
    CHURCH = "êµíšŒ"
    ACADEMY = "í•™ì›"
    PUBLIC = "ê³µê³µê¸°ê´€"
    SCHOOL = "í•™êµ"

# ==================== AI Agent í´ë˜ìŠ¤ë“¤ ====================

class AIAgent:
    """ê¸°ë³¸ AI Agent í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, role: str):
        self.name = name
        self.role = role
        self.model = None
        
        if AI_AVAILABLE:
            try:
                self.model = genai.GenerativeModel("gemini-1.5-flash")
                print(f"ğŸ¤– {self.name} Agent ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {self.name} Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def analyze(self, prompt: str) -> str:
        """AI ë¶„ì„ ìˆ˜í–‰"""
        if not self.model:
            return f"AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({self.name})"
        
        # í”„ë¡¬í”„íŠ¸ ë¡œê¹…
        ai_logger.info(f"ğŸ¤– [{self.name}] AI í”„ë¡¬í”„íŠ¸ ì „ì†¡:")
        ai_logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
        ai_logger.debug(f"ğŸ“‹ ì „ì²´ í”„ë¡¬í”„íŠ¸:\n{'-'*50}\n{prompt}\n{'-'*50}")
        
        try:
            full_prompt = f"ë‹¹ì‹ ì€ {self.role}ì…ë‹ˆë‹¤.\n\n{prompt}"
            
            # AI í˜¸ì¶œ ì‹œì‘ ë¡œê¹…
            ai_logger.info(f"â³ [{self.name}] Gemini API í˜¸ì¶œ ì¤‘...")
            
            response = self.model.generate_content(full_prompt)
            result = response.text.strip()
            
            # ì‘ë‹µ ë¡œê¹…
            ai_logger.info(f"âœ… [{self.name}] AI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            ai_logger.info(f"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(result)}ì")
            ai_logger.debug(f"ğŸ¯ AI ì‘ë‹µ ë‚´ìš©:\n{'-'*50}\n{result}\n{'-'*50}")
            
            return result
            
        except Exception as e:
            ai_logger.error(f"âŒ [{self.name}] AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

class DataAnalystAgent(AIAgent):
    """ë°ì´í„° ë¶„ì„ ì „ë¬¸ Agent"""
    
    def __init__(self):
        super().__init__("DataAnalyst", "ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ë° ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€")
    
    def analyze_data_source(self, source_info: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ì†ŒìŠ¤ ë¶„ì„"""
        ai_logger.info(f"ğŸ” [{self.name}] ë°ì´í„° ì†ŒìŠ¤ ë¶„ì„ ì‹œì‘: {source_info['path']}")
        prompt = f"""
ë‹¤ìŒ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

**íŒŒì¼ ì •ë³´:**
- ê²½ë¡œ: {source_info['path']}
- íƒ€ì…: {source_info['type']}
- ë°ì´í„° ìˆ˜: {source_info['count']:,}ê°œ
- ì»¬ëŸ¼: {source_info['columns'][:10]}... (ì´ {len(source_info['columns'])}ê°œ)

**ìƒ˜í”Œ ë°ì´í„°:**
{json.dumps(source_info['sample_data'][:2], ensure_ascii=False, indent=2)}

**ë¶„ì„ ìš”ì²­:**
1. ë°ì´í„° í’ˆì§ˆ í‰ê°€ (ìƒ/ì¤‘/í•˜)
2. í•µì‹¬ í•„ë“œ ì‹ë³„ (ê¸°ê´€ëª…, ì£¼ì†Œ, ì—°ë½ì²˜ ë“±)
3. í†µí•© ì‹œ ê³ ë ¤ì‚¬í•­
4. ê¶Œì¥ ë§¤í•‘ ì „ëµ

ê°„ê²°í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        analysis_text = self.analyze(prompt)
        
        ai_logger.info(f"âœ… [{self.name}] ë°ì´í„° ì†ŒìŠ¤ ë¶„ì„ ì™„ë£Œ")
        
        return {
            "agent": self.name,
            "source_path": source_info['path'],
            "analysis": analysis_text,
            "timestamp": datetime.now().isoformat()
        }

class IntegrationAgent(AIAgent):
    """í†µí•© ì„¤ê³„ ì „ë¬¸ Agent"""
    
    def __init__(self):
        super().__init__("Integration", "ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ë° ìŠ¤í‚¤ë§ˆ ì„¤ê³„ ì „ë¬¸ê°€")
    
    def design_integration_strategy(self, analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í†µí•© ì „ëµ ì„¤ê³„"""
        
        # DataFrameì„ ì œê±°í•œ AIìš© ë¶„ì„ ê²°ê³¼ ìƒì„±
        ai_analyses = []
        for analysis in analyses:
            ai_analysis = {
                "agent": analysis["agent"],
                "source_path": analysis["source_path"],
                "source_name": analysis["source_name"],
                "analysis": analysis["analysis"],
                "timestamp": analysis["timestamp"]
            }
            
            # data_infoì—ì„œ raw_data ì œê±°í•˜ê³  ë©”íƒ€ë°ì´í„°ë§Œ í¬í•¨
            if "data_info" in analysis:
                data_info = analysis["data_info"].copy()
                data_info.pop("raw_data", None)  # DataFrame ì œê±°
                ai_analysis["data_info"] = data_info
            
            ai_analyses.append(ai_analysis)
        
        prompt = f"""
    ë‹¤ìŒ ë°ì´í„° ì†ŒìŠ¤ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í†µí•© ì „ëµì„ ì„¤ê³„í•´ì£¼ì„¸ìš”:

    **ë°ì´í„° ì†ŒìŠ¤ ìš”ì•½:**
    """
        
        for i, analysis in enumerate(ai_analyses, 1):
            data_info = analysis.get("data_info", {})
            prompt += f"""
    {i}. {analysis["source_name"]}
    - ê²½ë¡œ: {analysis["source_path"]}
    - ë°ì´í„° ìˆ˜: {data_info.get("count", "N/A"):,}ê°œ
    - ì»¬ëŸ¼ ìˆ˜: {len(data_info.get("columns", [])):,}ê°œ
    - AI ë¶„ì„: {analysis["analysis"][:200]}...
    """

        prompt += """

    **í†µí•© ì „ëµ ì„¤ê³„ ìš”ì²­:**

    1. **ì»¬ëŸ¼ ë§¤í•‘ ì „ëµ**
    ê° ì†ŒìŠ¤ì˜ ì»¬ëŸ¼ì„ í†µí•© í…Œì´ë¸”ë¡œ ì–´ë–»ê²Œ ë§¤í•‘í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”.

    2. **ì¤‘ë³µ ì œê±° ê·œì¹™**
    ì£¼ì†Œ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ë¡œì§ì„ ì–´ë–»ê²Œ ì ìš©í• ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.

    3. **ë°ì´í„° ë³€í™˜ ê·œì¹™**
    ê° ì†ŒìŠ¤ë³„ë¡œ í•„ìš”í•œ ë°ì´í„° ì •ì œ ë° ë³€í™˜ ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

    4. **í†µí•© ìˆœì„œ ë° ìš°ì„ ìˆœìœ„**
    ì–´ë–¤ ìˆœì„œë¡œ ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  ì¶©ëŒ ì‹œ ìš°ì„ ìˆœìœ„ëŠ” ì–´ë–»ê²Œ í• ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.

    **ë‹µë³€ í˜•ì‹:**
    ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ êµ¬ì¡°í™”í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    """
        
        strategy_text = self.analyze(prompt)
        
        return {
            "agent": self.name,
            "strategy": strategy_text,
            "timestamp": datetime.now().isoformat()
        }

class ReviewerAgent(AIAgent):
    """ê²€í†  ë° ê²€ì¦ ì „ë¬¸ Agent"""
    
    def __init__(self):
        super().__init__("Reviewer", "ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ê²€í†  ë° í’ˆì§ˆ ë³´ì¦ ì „ë¬¸ê°€")
    
    def review_integration_plan(self, strategy: Dict[str, Any]) -> Dict[str, Any]:
        """í†µí•© ê³„íš ê²€í† """
        prompt = f"""
    ë‹¤ìŒ í†µí•© ì „ëµì„ ê²€í† í•˜ê³  ìŠ¹ì¸ ì—¬ë¶€ë¥¼ ê²°ì •í•´ì£¼ì„¸ìš”:

    **í†µí•© ì „ëµ:**
    {strategy.get("strategy", "ì „ëµ ì •ë³´ ì—†ìŒ")}

    **ê²€í†  ê´€ì :**
    1. ì „ëµì˜ ì ì ˆì„± - ë°ì´í„° ì†ŒìŠ¤ë³„ íŠ¹ì„±ì„ ì˜ ê³ ë ¤í–ˆëŠ”ê°€?
    2. ë°ì´í„° ë¬´ê²°ì„± - ì¤‘ë³µ ì œê±° ë° ë°ì´í„° í’ˆì§ˆ ë³´ì¥ ë°©ì•ˆì€ ì ì ˆí•œê°€?
    3. ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­ - ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ì— ì í•©í•œê°€?
    4. êµ¬í˜„ ê°€ëŠ¥ì„± - ì‹¤ì œ êµ¬í˜„ì´ ê°€ëŠ¥í•œ í˜„ì‹¤ì ì¸ ë°©ì•ˆì¸ê°€?

    **ê²€í†  ê²°ê³¼:**
    - ì „ì²´ í‰ê°€: ìŠ¹ì¸/ë³´ë¥˜/ê±°ë¶€ ì¤‘ í•˜ë‚˜ë¡œ ëª…ì‹œ
    - ì ìˆ˜: 10ì  ë§Œì  ì¤‘ ëª‡ ì 
    - ì£¼ìš” ì¥ì : 
    - ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„:
    - ìµœì¢… ê²°ë¡ :

    êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ìœ¼ë¡œ ê²€í† í•´ì£¼ì„¸ìš”.
    """
        
        review_text = self.analyze(prompt)
        
        # ìŠ¹ì¸ ìƒíƒœ íŒŒì•… (í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ)
        approval_status = "ê²€í† ì¤‘"
        if "ìŠ¹ì¸" in review_text and ("ë³´ë¥˜" not in review_text and "ê±°ë¶€" not in review_text):
            approval_status = "ìŠ¹ì¸"
        elif "ë³´ë¥˜" in review_text:
            approval_status = "ë³´ë¥˜"
        elif "ê±°ë¶€" in review_text:
            approval_status = "ê±°ë¶€"
        
        return {
            "agent": self.name,
            "review": review_text,
            "timestamp": datetime.now().isoformat(),
            "approval": approval_status
        }

# ==================== í†µí•© CRM ë°ì´í„°ë² ì´ìŠ¤ í´ë˜ìŠ¤ ====================

class EnhancedCRMDatabase:
    """AI ê¸°ë°˜ í†µí•© CRM ë°ì´í„°ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, db_path: str = "churches_crm.db"):
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("PRAGMA foreign_keys=ON")
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            self._create_tables(conn)
    
    def _create_tables(self, conn):
        """í…Œì´ë¸” ìƒì„± (í™•ì¥ëœ êµ¬ì¡°)"""
        # Users í…Œì´ë¸”
        conn.execute('''
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            username TEXT UNIQUE NOT NULL,
            password_hash TEXT NOT NULL,
            full_name TEXT NOT NULL,
            email TEXT,
            phone TEXT,
            role TEXT NOT NULL,
            team TEXT,
            is_active BOOLEAN DEFAULT 1,
            last_login DATETIME,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        # Organizations í…Œì´ë¸” (3ê°œ ì†ŒìŠ¤ í†µí•© ì§€ì›)
        conn.execute('''
        CREATE TABLE IF NOT EXISTS organizations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            type TEXT DEFAULT 'UNKNOWN',
            category TEXT DEFAULT 'ê¸°íƒ€',
            subcategory TEXT,  -- êµìŠµê³¼ì •ëª…, êµë‹¨ ë“±
            
            -- ì—°ë½ì²˜ ì •ë³´
            homepage TEXT,
            phone TEXT,
            fax TEXT,
            email TEXT,
            mobile TEXT,
            postal_code TEXT,
            address TEXT,
            
            -- ìƒì„¸ ì •ë³´
            organization_size TEXT,
            founding_year INTEGER,
            member_count INTEGER,
            denomination TEXT,
            
            -- CRM ì •ë³´
            contact_status TEXT DEFAULT 'NEW',
            priority TEXT DEFAULT 'MEDIUM',
            assigned_to TEXT,
            lead_source TEXT DEFAULT 'DATABASE',
            estimated_value INTEGER DEFAULT 0,
            
            -- ì˜ì—… ë…¸íŠ¸
            sales_notes TEXT,
            internal_notes TEXT,
            last_contact_date DATE,
            next_follow_up_date DATE,
            
            -- ë©”íƒ€ë°ì´í„°
            data_source TEXT,  -- JSON/EXCEL_ACADEMY/EXCEL_CHURCH
            ai_analysis TEXT,  -- AI ë¶„ì„ ê²°ê³¼
            crawling_data TEXT,  -- í¬ë¡¤ë§ ì›ë³¸ ë°ì´í„°
            
            -- ì‹œìŠ¤í…œ í•„ë“œ
            created_by TEXT,
            updated_by TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            is_active BOOLEAN DEFAULT 1
        )
        ''')
        
        # ì¸ë±ìŠ¤ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_org_name ON organizations(name)",
            "CREATE INDEX IF NOT EXISTS idx_org_type ON organizations(type)",
            "CREATE INDEX IF NOT EXISTS idx_org_address ON organizations(address)",
            "CREATE INDEX IF NOT EXISTS idx_org_phone ON organizations(phone)",
            "CREATE INDEX IF NOT EXISTS idx_org_source ON organizations(data_source)",
            "CREATE INDEX IF NOT EXISTS idx_org_status ON organizations(contact_status)"
        ]
        
        for index in indexes:
            conn.execute(index)
        
        conn.commit()
    
    def _hash_password(self, password: str) -> str:
        """ë¹„ë°€ë²ˆí˜¸ í•´ì‹œí™”"""
        salt = secrets.token_hex(16)
        password_hash = hashlib.pbkdf2_hmac('sha256', 
                                          password.encode('utf-8'), 
                                          salt.encode('utf-8'), 
                                          100000)
        return f"{salt}:{password_hash.hex()}"
    
    def create_user(self, user_data: Dict[str, Any]) -> int:
        """ì‚¬ìš©ì ìƒì„±"""
        with sqlite3.connect(self.db_path) as conn:
            password_hash = self._hash_password(user_data['password'])
            cursor = conn.execute('''
            INSERT INTO users (username, password_hash, full_name, email, phone, role, team)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                user_data['username'],
                password_hash,
                user_data['full_name'],
                user_data.get('email', ''),
                user_data.get('phone', ''),
                user_data['role'],
                user_data.get('team', '')
            ))
            return cursor.lastrowid
    
    def create_organization(self, org_data: Dict[str, Any]) -> int:
        """ê¸°ê´€ ì •ë³´ ìƒì„± (í™•ì¥ëœ êµ¬ì¡°)"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
            INSERT INTO organizations (
                name, type, category, subcategory, homepage, phone, fax, email, mobile,
                postal_code, address, organization_size, founding_year, member_count,
                denomination, contact_status, priority, assigned_to, lead_source,
                estimated_value, sales_notes, internal_notes, data_source,
                ai_analysis, crawling_data, created_by
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                org_data.get('name', ''),
                org_data.get('type', 'UNKNOWN'),
                org_data.get('category', 'ê¸°íƒ€'),
                org_data.get('subcategory', ''),
                org_data.get('homepage', ''),
                org_data.get('phone', ''),
                org_data.get('fax', ''),
                org_data.get('email', ''),
                org_data.get('mobile', ''),
                org_data.get('postal_code', ''),
                org_data.get('address', ''),
                org_data.get('organization_size', ''),
                org_data.get('founding_year'),
                org_data.get('member_count'),
                org_data.get('denomination', ''),
                org_data.get('contact_status', 'NEW'),
                org_data.get('priority', 'MEDIUM'),
                org_data.get('assigned_to', ''),
                org_data.get('lead_source', 'DATABASE'),
                org_data.get('estimated_value', 0),
                org_data.get('sales_notes', ''),
                org_data.get('internal_notes', ''),
                org_data.get('data_source', 'UNKNOWN'),
                org_data.get('ai_analysis', ''),
                json.dumps(org_data.get('crawling_data', {})),
                org_data.get('created_by', 'MIGRATION')
            ))
            return cursor.lastrowid
    
    def get_dashboard_stats(self) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œ í†µê³„ (ì†ŒìŠ¤ë³„ ë¶„í¬ í¬í•¨)"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            
            total_orgs = conn.execute("SELECT COUNT(*) FROM organizations WHERE is_active = 1").fetchone()[0]
            
            # íƒ€ì…ë³„ ë¶„í¬
            type_counts = {}
            for row in conn.execute("SELECT type, COUNT(*) FROM organizations WHERE is_active = 1 GROUP BY type"):
                type_counts[row[0]] = row[1]
            
            # ì†ŒìŠ¤ë³„ ë¶„í¬
            source_counts = {}
            for row in conn.execute("SELECT data_source, COUNT(*) FROM organizations WHERE is_active = 1 GROUP BY data_source"):
                source_counts[row[0]] = row[1]
            
            return {
                "total_organizations": total_orgs,
                "type_counts": type_counts,
                "source_counts": source_counts
            }

# ==================== AI ê¸°ë°˜ í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë˜ìŠ¤ ====================

class AIAgenticDataMigrator:
    """AI Agentic Workflow ê¸°ë°˜ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    def __init__(self, db_path: str = "churches_crm.db"):
        self.db = EnhancedCRMDatabase(db_path)
        
        # AI Agents ì´ˆê¸°í™”
        self.data_analyst = DataAnalystAgent()
        self.integration_agent = IntegrationAgent()
        self.reviewer_agent = ReviewerAgent()
        
        # í†µê³„
        self.stats = {
            "total_processed": 0,
            "successfully_migrated": 0,
            "failed": 0,
            "duplicates_skipped": 0,
            "sources": {},
            "errors": []
        }
    
        # AI ë¶„ì„ ê²°ê³¼
        self.analyses = []
        self.integration_strategy = None
        self.review_result = None
        
        print("ğŸ¤– AI Agentic Data Migrator ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_json_data(self, file_path: str) -> Optional[Dict[str, Any]]:
        """JSON ë°ì´í„° ë¡œë“œ ë° ë¶„ì„"""
        try:
            if not os.path.exists(file_path):
                return None
            
            print(f"ğŸ“‚ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, dict) and 'churches' in data:
                items = data['churches']
            elif isinstance(data, list):
                items = data
            else:
                items = [data]
            
            columns = list(items[0].keys()) if items and isinstance(items[0], dict) else []
            sample_data = items[:3] if items else []
            
            return {
                "path": file_path,
                "type": "JSON",
                "count": len(items),
                "columns": columns,
                "sample_data": sample_data,
                "raw_data": items
            }
            
        except Exception as e:
            print(f"âŒ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def load_excel_data(self, file_path: str, header_row: int = 0) -> Optional[Dict[str, Any]]:
        """Excel ë°ì´í„° ë¡œë“œ ë° ë¶„ì„"""
        try:
            if not os.path.exists(file_path):
                return None
            
            print(f"ğŸ“‚ Excel íŒŒì¼ ë¡œë“œ ì¤‘: {file_path} (í—¤ë”: {header_row+1}í–‰)")
            
            df = pd.read_excel(file_path, sheet_name=0, header=header_row)
            df = df.dropna(how='all')
            df.columns = [str(col).strip() for col in df.columns]
            
            sample_data = []
            for i, row in df.head(3).iterrows():
                sample_data.append(row.to_dict())
            
            return {
                "path": file_path,
                "type": "Excel",
                "count": len(df),
                "columns": df.columns.tolist(),
                "sample_data": sample_data,
                "raw_data": df
            }
            
        except Exception as e:
            print(f"âŒ Excel ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def execute_ai_analysis_phase(self, data_sources: List[Dict[str, Any]]):
        """AI ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰"""
        print("\nğŸ¤– Phase 1: AI Agents ë°ì´í„° ë¶„ì„")
        print("-" * 50)
        
        for i, source in enumerate(data_sources, 1):
            print(f"\nğŸ” {i}. {source['name']} ë¶„ì„ ì¤‘...")
            
            # ë°ì´í„° ë¡œë“œ
            if source['type'] == 'JSON':
                data_info = self.load_json_data(source['path'])
            elif source['type'] == 'Excel':
                header_row = source.get('header_row', 0)
                data_info = self.load_excel_data(source['path'], header_row)
            else:
                print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…: {source['type']}")
                continue
            
            if not data_info:
                print(f"âŒ {source['name']} ë¡œë“œ ì‹¤íŒ¨")
                continue
            
            # AI ë¶„ì„ ìˆ˜í–‰
            if AI_AVAILABLE:
                analysis = self.data_analyst.analyze_data_source(data_info)
                print(f"ğŸ¤– DataAnalystê°€ {source['name']} ë¶„ì„ ì™„ë£Œ")
            else:
                analysis = {
                    "agent": "Manual",
                    "source_path": data_info['path'],
                    "analysis": "AI ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¡œ ìˆ˜ë™ ë¶„ì„",
                    "timestamp": datetime.now().isoformat()
                }
                print(f"ğŸ”§ {source['name']} ìˆ˜ë™ ë¶„ì„ ì™„ë£Œ")
            
            analysis['source_name'] = source['name']
            analysis['data_info'] = data_info
            self.analyses.append(analysis)
            
            print(f"âœ… {source['name']}: {data_info['count']:,}ê°œ, {len(data_info['columns'])}ê°œ ì»¬ëŸ¼")
    
    def execute_integration_design_phase(self):
        """í†µí•© ì„¤ê³„ ë‹¨ê³„ ì‹¤í–‰"""
        print("\nğŸ—ï¸ Phase 2: í†µí•© ì „ëµ ì„¤ê³„")
        print("-" * 50)
        
        if AI_AVAILABLE:
            print("ğŸ¤– Integration Agentê°€ í†µí•© ì „ëµ ì„¤ê³„ ì¤‘...")
            self.integration_strategy = self.integration_agent.design_integration_strategy(self.analyses)
            print("âœ… í†µí•© ì „ëµ ì„¤ê³„ ì™„ë£Œ")
        else:
            print("ğŸ”§ ìˆ˜ë™ í†µí•© ì „ëµ ì‚¬ìš©")
            self.integration_strategy = {
                "agent": "Manual",
                "strategy": "AI ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¡œ ê¸°ë³¸ ì „ëµ ì‚¬ìš©",
                "timestamp": datetime.now().isoformat()
            }

    def execute_review_phase(self):
        """ê²€í†  ë‹¨ê³„ ì‹¤í–‰"""
        print("\nğŸ” Phase 3: ì„¤ê³„ ê²€í† ")
        print("-" * 50)
        
        if AI_AVAILABLE:
            print("ğŸ¤– Reviewer Agentê°€ ì„¤ê³„ ê²€í†  ì¤‘...")
            self.review_result = self.reviewer_agent.review_integration_plan(self.integration_strategy)
            approval = self.review_result.get('approval', 'ê²€í† ì¤‘')
            print(f"âœ… ê²€í†  ì™„ë£Œ - ìƒíƒœ: {approval}")
        else:
            print("ğŸ”§ ìˆ˜ë™ ê²€í†  ìŠ¹ì¸")
            self.review_result = {
                "agent": "Manual",
                "review": "AI ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¡œ ìë™ ìŠ¹ì¸",
                "approval": "ìŠ¹ì¸",
                "timestamp": datetime.now().isoformat()
            }
    
    def create_column_mapping(self, data_info: Dict[str, Any], source_type: str) -> Dict[str, str]:
        """ì»¬ëŸ¼ ë§¤í•‘ ìƒì„±"""
        columns = data_info['columns']
        mapping = {}
        
        # ê¸°ë³¸ ë§¤í•‘ ê·œì¹™
        for col in columns:
            col_lower = col.lower().strip()
            
            # ê¸°ê´€ëª… ë§¤í•‘
            if any(keyword in col_lower for keyword in ['í•™ì›ëª…', 'ê¸°ê´€ëª…', 'ìƒí˜¸ëª…', 'name', 'êµíšŒëª…']):
                mapping['name'] = col
            # ì£¼ì†Œ ë§¤í•‘
            elif any(keyword in col_lower for keyword in ['ë„ë¡œëª…ì£¼ì†Œ', 'ì£¼ì†Œ', 'address', 'ì†Œì¬ì§€']):
                mapping['address'] = col
            # ì „í™”ë²ˆí˜¸ ë§¤í•‘
            elif any(keyword in col_lower for keyword in ['ì „í™”ë²ˆí˜¸', 'phone', 'ì—°ë½ì²˜']) and 'íŒ©ìŠ¤' not in col_lower:
                mapping['phone'] = col
            # íŒ©ìŠ¤ ë§¤í•‘
            elif any(keyword in col_lower for keyword in ['íŒ©ìŠ¤', 'fax']):
                mapping['fax'] = col
            # ì´ë©”ì¼ ë§¤í•‘
            elif any(keyword in col_lower for keyword in ['ì´ë©”ì¼', 'email', 'e-mail']):
                mapping['email'] = col
            # í™ˆí˜ì´ì§€ ë§¤í•‘
            elif any(keyword in col_lower for keyword in ['í™ˆí˜ì´ì§€', 'homepage', 'website', 'url']):
                mapping['homepage'] = col
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            elif any(keyword in col_lower for keyword in ['ë¶„ì•¼ëª…', 'êµìŠµê³¼ì •', 'category', 'ì—…ì¢…']):
                mapping['category'] = col
            # ì„œë¸Œì¹´í…Œê³ ë¦¬ ë§¤í•‘
            elif any(keyword in col_lower for keyword in ['êµìŠµê³¼ì •ëª…', 'ê³„ì—´', 'êµë‹¨']):
                mapping['subcategory'] = col
        
        return mapping
    
    def transform_json_data(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """JSON ë°ì´í„° ë³€í™˜ (êµíšŒ)"""
        crawling_fields = ['homepage_parsed', 'page_title', 'ai_summary', 'parsing_timestamp']
        crawling_data = {k: v for k, v in item.items() if k in crawling_fields}
        
        return {
            'name': item.get('name', '').strip(),
            'type': OrganizationType.CHURCH,
            'category': item.get('category', 'ì¢…êµì‹œì„¤'),
            'homepage': item.get('homepage', '').strip(),
            'phone': item.get('phone', '').strip(),
            'fax': item.get('fax', '').strip(),
            'email': item.get('email', '').strip(),
            'mobile': item.get('mobile', '').strip(),
            'postal_code': item.get('postal_code', '').strip(),
            'address': item.get('address', '').strip(),
            'contact_status': 'NEW',
            'priority': 'MEDIUM',
            'lead_source': 'DATABASE',
            'data_source': 'JSON_CHURCH',
            'crawling_data': crawling_data,
            'created_by': 'AI_MIGRATION'
        }
    
    def transform_excel_data(self, row: pd.Series, mapping: Dict[str, str], source_type: str) -> Dict[str, Any]:
        """Excel ë°ì´í„° ë³€í™˜"""
        def safe_get(field: str) -> str:
            col = mapping.get(field)
            if col and col in row.index:
                value = row[col]
                if pd.isna(value):
                    return ""
                return str(value).strip()
            return ""
        
        org_type = OrganizationType.ACADEMY if 'ACADEMY' in source_type else OrganizationType.CHURCH
        
        return {
            'name': safe_get('name'),
            'type': org_type,
            'category': safe_get('category') or ('í•™ì›' if org_type == OrganizationType.ACADEMY else 'ì¢…êµì‹œì„¤'),
            'subcategory': safe_get('subcategory'),
            'phone': safe_get('phone'),
            'fax': safe_get('fax'),
            'email': safe_get('email'),
            'homepage': safe_get('homepage'),
            'address': safe_get('address'),
            'contact_status': 'NEW',
            'priority': 'MEDIUM',
            'lead_source': 'DATABASE',
            'data_source': source_type,
            'created_by': 'AI_MIGRATION'
        }
    
    def is_duplicate_by_address(self, org_data: Dict[str, Any]) -> bool:
        """ì£¼ì†Œ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬"""
        try:
            with sqlite3.connect(self.db.db_path) as conn:
                address = org_data.get('address', '').strip()
                name = org_data.get('name', '').strip()
                
                if not name:
                    return False
                
                if address:
                    # ì£¼ì†Œì™€ ìƒí˜¸ëª…ì´ ëª¨ë‘ ê°™ì€ ê²½ìš°
                    existing = conn.execute('''
                    SELECT COUNT(*) FROM organizations 
                    WHERE address = ? AND name = ? AND is_active = 1
                    ''', (address, name)).fetchone()[0]
                else:
                    # ì£¼ì†Œê°€ ì—†ìœ¼ë©´ ìƒí˜¸ëª… + ì „í™”ë²ˆí˜¸ë¡œ ì²´í¬
                    phone = org_data.get('phone', '').strip()
                existing = conn.execute('''
                SELECT COUNT(*) FROM organizations 
                    WHERE name = ? AND phone = ? AND is_active = 1
                    ''', (name, phone)).fetchone()[0]
                
                return existing > 0
                
        except Exception as e:
            print(f"âš ï¸  ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨: {e}")
            return False
    
    def migrate_all_sources(self, batch_size: int = 1000) -> bool:
        """ëª¨ë“  ì†ŒìŠ¤ í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜"""
        ai_logger.info("ğŸš€ í†µí•© ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        ai_logger.info(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size:,}ê°œ")
        
        total_success = 0
        
        for analysis in self.analyses:
            source_name = analysis['source_name']
            data_info = analysis['data_info']
            
            ai_logger.info(f"ğŸ“‚ {source_name} ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘ - {data_info['count']:,}ê°œ ë°ì´í„°")
            
            print(f"\nğŸ“Š {source_name} ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘...")
            
            if data_info['type'] == 'JSON':
                success = self._migrate_json_data(data_info, batch_size)
            else:  # Excel
                source_type = 'EXCEL_ACADEMY' if 'í•™ì›' in source_name else 'EXCEL_CHURCH'
                success = self._migrate_excel_data(data_info, source_type, batch_size)
            
            total_success += success
            self.stats['sources'][source_name] = success
            
            ai_logger.info(f"âœ… {source_name} ì™„ë£Œ: {success:,}ê°œ ì¶”ê°€")
        
        ai_logger.info(f"ğŸ‰ ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {total_success:,}ê°œ ì¶”ê°€")
        
        return total_success > 0
    
    def _migrate_json_data(self, data_info: Dict[str, Any], batch_size: int) -> int:
        """JSON ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        items = data_info['raw_data']
        success_count = 0
        
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            
            for item in batch:
                self.stats["total_processed"] += 1
                
                try:
                    org_data = self.transform_json_data(item)
                    
                    if not org_data.get('name'):
                        self.stats["failed"] += 1
                        continue
                    
                    if self.is_duplicate_by_address(org_data):
                        self.stats["duplicates_skipped"] += 1
                        continue
                    
                    org_id = self.db.create_organization(org_data)
                    
                    if org_id:
                        self.stats["successfully_migrated"] += 1
                        success_count += 1
                    else:
                        self.stats["failed"] += 1
                        
                except Exception as e:
                    self.stats["failed"] += 1
                    error_msg = f"JSON '{item.get('name', 'Unknown')}' ì‹¤íŒ¨: {str(e)[:100]}"
                    self.stats["errors"].append(error_msg)
        
        return success_count
    
    def _migrate_excel_data(self, data_info: Dict[str, Any], source_type: str, batch_size: int) -> int:
        """Excel ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        df = data_info['raw_data']
        mapping = self.create_column_mapping(data_info, source_type)
        success_count = 0
        
        print(f"ğŸ”— ì»¬ëŸ¼ ë§¤í•‘: {mapping}")
        
        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i + batch_size]
            
            for _, row in batch.iterrows():
                self.stats["total_processed"] += 1
                
                try:
                    org_data = self.transform_excel_data(row, mapping, source_type)
                    
                    if not org_data.get('name'):
                        self.stats["failed"] += 1
                        continue
                    
                    if self.is_duplicate_by_address(org_data):
                        self.stats["duplicates_skipped"] += 1
                        continue
                    
                    org_id = self.db.create_organization(org_data)
                    
                    if org_id:
                        self.stats["successfully_migrated"] += 1
                        success_count += 1
                    else:
                        self.stats["failed"] += 1
                        
                except Exception as e:
                    self.stats["failed"] += 1
                    error_msg = f"Excel '{row.get(mapping.get('name', ''), 'Unknown')}' ì‹¤íŒ¨: {str(e)[:100]}"
                    self.stats["errors"].append(error_msg)
        
        return success_count
    
    def create_default_users(self):
        """ê¸°ë³¸ ì‚¬ìš©ì ê³„ì • ìƒì„±"""
        print("\nğŸ‘¥ ê¸°ë³¸ ì‚¬ìš©ì ê³„ì • ìƒì„± ì¤‘...")
        
        default_users = [
            {"username": "admin", "password": "admin123", "full_name": "ì‹œìŠ¤í…œ ê´€ë¦¬ì", 
             "email": "admin@company.com", "role": UserRole.ADMIN1},
            {"username": "manager", "password": "manager123", "full_name": "ì˜ì—… íŒ€ì¥", 
             "email": "manager@company.com", "role": UserRole.ADMIN2},
            {"username": "sales1", "password": "sales123", "full_name": "ì˜ì—…íŒ€ì›1", 
             "email": "sales1@company.com", "role": UserRole.SALES, "team": "AíŒ€"}
        ]
        
        for user_data in default_users:
            try:
                self.db.create_user(user_data)
                print(f"âœ… ì‚¬ìš©ì ìƒì„±: {user_data['username']}")
            except Exception as e:
                if "UNIQUE" in str(e):
                    print(f"âš ï¸  ì‚¬ìš©ì ì´ë¯¸ ì¡´ì¬: {user_data['username']}")
    
    def print_final_summary(self):
        """ìµœì¢… ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*70)
        print("ğŸ‰ AI Agentic Workflow í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
        print("="*70)
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        print("ğŸ“‹ ë°ì´í„° ì†ŒìŠ¤ë³„ ê²°ê³¼:")
        for source, count in self.stats["sources"].items():
            print(f"  - {source}: {count:,}ê°œ")
        
        print(f"\nğŸ“ˆ ì´ ì²˜ë¦¬: {self.stats['total_processed']:,}ê°œ")
        print(f"âœ… ì„±ê³µ ë§ˆì´ê·¸ë ˆì´ì…˜: {self.stats['successfully_migrated']:,}ê°œ")
        print(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {self.stats['duplicates_skipped']:,}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {self.stats['failed']:,}ê°œ")
        
        if self.stats['total_processed'] > 0:
            success_rate = (self.stats['successfully_migrated'] / self.stats['total_processed']) * 100
            print(f"ğŸ“Š ì„±ê³µë¥ : {success_rate:.1f}%")
        
        # ìµœì¢… DB í†µê³„
        try:
            final_stats = self.db.get_dashboard_stats()
            print(f"\nğŸ¯ ìµœì¢… ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
            print(f"  - ì´ ê¸°ê´€ ìˆ˜: {final_stats['total_organizations']:,}ê°œ")
            print(f"  - íƒ€ì…ë³„ ë¶„í¬:")
            for org_type, count in final_stats['type_counts'].items():
                print(f"    * {org_type}: {count:,}ê°œ")
            print(f"  - ì†ŒìŠ¤ë³„ ë¶„í¬:")
            for source, count in final_stats['source_counts'].items():
                print(f"    * {source}: {count:,}ê°œ")
        except Exception as e:
            print(f"âš ï¸  ìµœì¢… í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # AI ë¶„ì„ ê²°ê³¼ ìš”ì•½
        if AI_AVAILABLE and self.review_result:
            approval = self.review_result.get('approval', 'í™•ì¸ë¶ˆê°€')
            print(f"\nğŸ¤– AI ê²€í†  ê²°ê³¼: {approval}")
        
        print("="*70)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ AI Agentic Workflow í†µí•© CRM ë§ˆì´ê·¸ë ˆì´ì…˜ v4.0")
    print("ğŸ¯ 3ê°œ ë°ì´í„° ì†ŒìŠ¤ AI ê¸°ë°˜ ì§€ëŠ¥ì  í†µí•©")
    print("="*70)
    
    # ë°ì´í„° ì†ŒìŠ¤ ì •ì˜
    data_sources = [
        {
            "name": "êµíšŒ JSON ë°ì´í„°",
            "type": "JSON",
            "path": "data/json/merged_church_data_20250618_174032.json"
        },
        {
            "name": "í•™ì› Excel ë°ì´í„°",
            "type": "Excel",
            "path": "í•™ì›êµìŠµì†Œì •ë³´_2024ë…„10ì›”31ì¼ê¸°ì¤€20250617ìˆ˜ì •.xlsx",
            "header_row": 0
        },
        {
            "name": "êµíšŒ Excel ë°ì´í„°",
            "type": "Excel",
            "path": "data/excel/êµíšŒ_ì›ë³¸_ìˆ˜ì •01.xlsx",
            "header_row": 2  # 3í–‰ì´ í—¤ë”
        }
    ]
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    available_sources = []
    for source in data_sources:
        if os.path.exists(source['path']):
            available_sources.append(source)
            print(f"âœ… {source['name']}: {source['path']}")
        else:
            alt_path = f"../{source['path']}"
            if os.path.exists(alt_path):
                source['path'] = alt_path
                available_sources.append(source)
                print(f"âœ… {source['name']}: {alt_path}")
            else:
                print(f"âŒ {source['name']}: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    if not available_sources:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì†ŒìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    print(f"\nğŸ“Š ì²˜ë¦¬í•  ë°ì´í„° ì†ŒìŠ¤: {len(available_sources)}ê°œ")
    
    # AI ê¸°ëŠ¥ ìƒíƒœ í™•ì¸
    if AI_AVAILABLE:
        print("ğŸ¤– AI Agentic Workflow í™œì„±í™”")
    else:
        print("ğŸ”§ AI ê¸°ëŠ¥ ë¹„í™œì„±í™” - ê¸°ë³¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ëª¨ë“œ")
    
    # ì‹¤í–‰ í™•ì¸
    response = input("\nğŸš€ AI Agentic Workflow ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if response not in ['y', 'yes']:
        print("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return False
    
    # AI Agentic Workflow ì‹¤í–‰
    migrator = AIAgenticDataMigrator("churches_crm.db")
    
    try:
        start_time = datetime.now()
        print(f"\nâ° ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 1. ê¸°ë³¸ ì‚¬ìš©ì ìƒì„±
        migrator.create_default_users()
        
        # 2. AI ë¶„ì„ ë‹¨ê³„
        migrator.execute_ai_analysis_phase(available_sources)
        
        # 3. í†µí•© ì„¤ê³„ ë‹¨ê³„
        migrator.execute_integration_design_phase()
        
        # 4. ê²€í†  ë‹¨ê³„
        migrator.execute_review_phase()
        
        # 5. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
        success = migrator.migrate_all_sources(batch_size=1000)
    
        end_time = datetime.now()
        duration = end_time - start_time
    
        # 6. ìµœì¢… ê²°ê³¼ ìš”ì•½
        migrator.print_final_summary()
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {duration}")
    
        if success:
            print("\nğŸ‰ AI Agentic Workflow ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤: {migrator.db.db_path}")
            return True
        else:
            print("\nâŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return False
        
    except Exception as e:
        print(f"\nâŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        sys.exit(1)