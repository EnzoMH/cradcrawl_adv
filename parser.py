#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì›¹í˜ì´ì§€ íŒŒì‹± ëª¨ë“ˆ
í™ˆí˜ì´ì§€ì—ì„œ ì—°ë½ì²˜ ì •ë³´, í…ìŠ¤íŠ¸ ë‚´ìš© ë“±ì„ ì¶”ì¶œí•˜ëŠ” íŒŒì‹± í•¨ìˆ˜ë“¤
"""

import re
import logging
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import requests
import urllib3

# settings.pyì—ì„œ ìƒìˆ˜ë“¤ ì„í¬íŠ¸ (ìˆ˜ì •)
from settings import (
    PHONE_EXTRACTION_PATTERNS,
    FAX_EXTRACTION_PATTERNS,
    EMAIL_EXTRACTION_PATTERNS,
    ADDRESS_EXTRACTION_PATTERNS,
    WEBSITE_EXTRACTION_PATTERNS,
    KOREAN_AREA_CODES,
    AREA_CODE_LENGTH_RULES,
    LOGGER_NAMES,
    format_phone_number,
    extract_phone_area_code,
    is_valid_area_code
)

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class WebPageParser:
    def __init__(self):
        self.logger = logging.getLogger(LOGGER_NAMES["parser"])
        
        # settings.pyì—ì„œ ê°€ì ¸ì˜¨ íŒ¨í„´ë“¤ ì‚¬ìš©
        self.phone_patterns = PHONE_EXTRACTION_PATTERNS
        self.fax_patterns = FAX_EXTRACTION_PATTERNS
        self.email_patterns = EMAIL_EXTRACTION_PATTERNS
        self.address_patterns = ADDRESS_EXTRACTION_PATTERNS
        self.website_patterns = WEBSITE_EXTRACTION_PATTERNS
        
    def format_phone_number_safe(self, number_str):
        """ì „í™”ë²ˆí˜¸ í¬ë§·íŒ… (ì¶©ëŒ ë°©ì§€ìš©)"""
        return self.format_phone_number(number_str)
        
    # í…ìŠ¤íŠ¸ì—ì„œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
    def extract_contact_info(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        contact_info = {
            "phones": [],
            "faxes": [],
            "emails": [],
            "addresses": [],
            "websites": []
        }
        
        try:
            # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in self.phone_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    formatted_phone = self.format_phone_number_safe(match)
                    if formatted_phone and formatted_phone not in contact_info["phones"]:
                        contact_info["phones"].append(formatted_phone)
            
            # íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ
            for pattern in self.fax_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    formatted_fax = self.format_phone_number_safe(match)
                    if formatted_fax and formatted_fax not in contact_info["faxes"]:
                        contact_info["faxes"].append(formatted_fax)
            
            # ì´ë©”ì¼ ì¶”ì¶œ
            for pattern in self.email_patterns:
                email_matches = re.findall(pattern, text, re.IGNORECASE)
                for email in email_matches:
                    if email not in contact_info["emails"]:
                        contact_info["emails"].append(email)
            
            # ì£¼ì†Œ ì¶”ì¶œ
            for pattern in self.address_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    cleaned_address = self.clean_address(match)
                    if cleaned_address and cleaned_address not in contact_info["addresses"]:
                        contact_info["addresses"].append(cleaned_address)
            
            # ì›¹ì‚¬ì´íŠ¸ ì¶”ì¶œ
            for pattern in self.website_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    cleaned_website = self.clean_website(match)
                    if cleaned_website and cleaned_website not in contact_info["websites"]:
                        contact_info["websites"].append(cleaned_website)
        
        except Exception as e:
            self.logger.error(f"ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return contact_info
    
    # ì „í™”ë²ˆí˜¸ í¬ë§·íŒ…
    def format_phone_number(self, number_str):
        """ì „í™”ë²ˆí˜¸ í¬ë§·íŒ… (settings.py í•¨ìˆ˜ í™œìš©)"""
        if not number_str:
            return None
        
        # ìˆ«ìë§Œ ì¶”ì¶œ
        number = re.sub(r'[^\d]', '', number_str)
        
        # ê¸¸ì´ ê²€ì¦ (9-11ìë¦¬)
        if len(number) < 9 or len(number) > 11:
            return None
        
        # í•œêµ­ ì „í™”ë²ˆí˜¸ ì²´ê³„ì— ë§ëŠ”ì§€ í™•ì¸ (settings í•¨ìˆ˜ ì‚¬ìš©)
        area_code = extract_phone_area_code(number)
        if not area_code or not is_valid_area_code(area_code):
            return None
        
        # settings.pyì˜ í¬ë§·íŒ… í•¨ìˆ˜ ì‚¬ìš©
        return format_phone_number(number, area_code)
    
    # í•œêµ­ ì „í™”ë²ˆí˜¸ ì²´ê³„ ê²€ì¦
    def is_valid_korean_phone_number(self, number):
        """í•œêµ­ ì „í™”ë²ˆí˜¸ ì²´ê³„ ê²€ì¦ (settings.py ë°ì´í„° í™œìš©)"""
        # settings.pyì˜ ì§€ì—­ë²ˆí˜¸ ë°ì´í„° ì‚¬ìš©
        for area_code in KOREAN_AREA_CODES.keys():
            if number.startswith(area_code):
                return True
        
        return False
    
    # ì£¼ì†Œ ì •ë¦¬
    def clean_address(self, address):
        """ì£¼ì†Œ ì •ë¦¬"""
        if not address:
            return None
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        cleaned = re.sub(r'\s+', ' ', address.strip())
        
        # ë„ˆë¬´ ì§§ì€ ì£¼ì†Œ ì œì™¸
        if len(cleaned) < 10:
            return None
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        cleaned = re.sub(r'[^\w\s\-.,()ê°€-í£]', '', cleaned)
        
        return cleaned
    
    # ì›¹ì‚¬ì´íŠ¸ URL ì •ë¦¬
    def clean_website(self, website):
        """ì›¹ì‚¬ì´íŠ¸ URL ì •ë¦¬"""
        if not website:
            return None
        
        # ê³µë°± ì œê±°
        cleaned = website.strip()
        
        # http/https ì¶”ê°€
        if not cleaned.startswith(('http://', 'https://')):
            if cleaned.startswith('www.'):
                cleaned = 'http://' + cleaned
            elif '.' in cleaned:
                cleaned = 'http://' + cleaned
        
        # ìœ íš¨í•œ URL í˜•ì‹ì¸ì§€ í™•ì¸
        if not re.match(r'https?://[^\s]+\.[^\s]+', cleaned):
            return None
        
        return cleaned
    
    # HTMLì—ì„œ footer ë‚´ìš© ì¶”ì¶œ
    def extract_footer_content(self, soup):
        """HTMLì—ì„œ footer ë‚´ìš© ì¶”ì¶œ"""
        footer_content = []
        
        try:
            # footer íƒœê·¸ ì°¾ê¸°
            footer_elements = soup.find_all(['footer', 'div'], 
                                          class_=re.compile(r'footer|bottom|contact|info', re.I))
            
            for footer in footer_elements:
                text = footer.get_text().strip()
                if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ë§Œ
                    footer_content.append(text)
            
            # footerê°€ ì—†ìœ¼ë©´ í˜ì´ì§€ í•˜ë‹¨ë¶€ ì¶”ì¶œ
            if not footer_content:
                all_text = soup.get_text()
                # ë§ˆì§€ë§‰ 1000ì ì •ë„ë¥¼ í•˜ë‹¨ë¶€ë¡œ ê°„ì£¼
                if len(all_text) > 1000:
                    footer_content.append(all_text[-1000:])
        
        except Exception as e:
            self.logger.error(f"Footer ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return "\n".join(footer_content)
    
    # HTMLì—ì„œ ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
    def extract_meta_info(self, soup):
        """HTMLì—ì„œ ë©”íƒ€ ì •ë³´ ì¶”ì¶œ"""
        meta_info = {
            "title": "",
            "description": "",
            "keywords": "",
            "author": ""
        }
        
        try:
            # ì œëª© ì¶”ì¶œ
            title_tag = soup.find('title')
            if title_tag:
                meta_info["title"] = title_tag.get_text().strip()
            
            # ë©”íƒ€ íƒœê·¸ë“¤ ì¶”ì¶œ
            meta_tags = soup.find_all('meta')
            for meta in meta_tags:
                name = meta.get('name', '').lower()
                content = meta.get('content', '')
                
                if name == 'description':
                    meta_info["description"] = content
                elif name == 'keywords':
                    meta_info["keywords"] = content
                elif name == 'author':
                    meta_info["author"] = content
        
        except Exception as e:
            self.logger.error(f"ë©”íƒ€ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return meta_info

    # í™ˆí˜ì´ì§€ ì „ì²´ íŒŒì‹±
    def parse_homepage(self, url, html_content=None):
        """í™ˆí˜ì´ì§€ ì „ì²´ íŒŒì‹±"""
        result = {
            "url": url,
            "meta_info": {},
            "contact_info": {},
            "footer_content": "",
            "all_text": "",
            "status": "success",
            "error": None
        }
        
        try:
            # HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
            if html_content is None:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                }
                response = requests.get(url, headers=headers, timeout=10, verify=False)
                response.encoding = response.apparent_encoding
                html_content = response.text
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
            result["meta_info"] = self.extract_meta_info(soup)
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            all_text = soup.get_text()
            result["all_text"] = all_text
            
            # ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
            result["contact_info"] = self.extract_contact_info(all_text)
            
            # Footer ë‚´ìš© ì¶”ì¶œ
            result["footer_content"] = self.extract_footer_content(soup)
            
            self.logger.info(f"í™ˆí˜ì´ì§€ íŒŒì‹± ì™„ë£Œ: {url}")
        
        except Exception as e:
            result["status"] = "error"
            result["error"] = str(e)
            self.logger.error(f"í™ˆí˜ì´ì§€ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {url}, ì˜¤ë¥˜: {e}")
        
        return result
    
    # êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
    def extract_structured_data(self, soup):
        """êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ (JSON-LD, ë§ˆì´í¬ë¡œë°ì´í„° ë“±)"""
        structured_data = {}
        
        try:
            # JSON-LD ìŠ¤í¬ë¦½íŠ¸ ì°¾ê¸°
            json_ld_scripts = soup.find_all('script', type='application/ld+json')
            for script in json_ld_scripts:
                try:
                    import json
                    data = json.loads(script.string)
                    if isinstance(data, dict):
                        # ì¡°ì§ ì •ë³´ ì¶”ì¶œ
                        if data.get('@type') in ['Organization', 'LocalBusiness']:
                            if 'telephone' in data:
                                structured_data['phone'] = data['telephone']
                            if 'faxNumber' in data:
                                structured_data['fax'] = data['faxNumber']
                            if 'address' in data:
                                structured_data['address'] = data['address']
                            if 'url' in data:
                                structured_data['website'] = data['url']
                except:
                    continue
        
        except Exception as e:
            self.logger.error(f"êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return structured_data

def test_parser():
    """íŒŒì„œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    parser = WebPageParser()
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_text = """
    íš¨ì„±ì˜ê´‘êµíšŒ
    ì£¼ì†Œ: ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123
    ì „í™”: 02-1234-5678
    íŒ©ìŠ¤: 02-1234-5679
    ì´ë©”ì¼: info@church.co.kr
    í™ˆí˜ì´ì§€: www.church.co.kr
    """
    
    print("=" * 50)
    print("ğŸ“‹ ì›¹í˜ì´ì§€ íŒŒì„œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    contact_info = parser.extract_contact_info(test_text)
    
    print("ğŸ” ì¶”ì¶œëœ ì—°ë½ì²˜ ì •ë³´:")
    print(f"  ğŸ“ ì „í™”ë²ˆí˜¸: {contact_info['phones']}")
    print(f"  ğŸ“  íŒ©ìŠ¤ë²ˆí˜¸: {contact_info['faxes']}")
    print(f"  ğŸ“§ ì´ë©”ì¼: {contact_info['emails']}")
    print(f"  ğŸ  ì£¼ì†Œ: {contact_info['addresses']}")
    print(f"  ğŸŒ ì›¹ì‚¬ì´íŠ¸: {contact_info['websites']}")
    print("=" * 50)

if __name__ == "__main__":
    test_parser()